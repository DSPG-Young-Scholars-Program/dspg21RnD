{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval - Artificial Intelligence \n",
    "This notebook retrieves abstracts relevant to AI and then uses topic modeling to analyze the chosen abstracts.  Three info retrieval techniques are used: Literal Term Matching, TF-IDF, and Latent Semantic Indexing.  These are linear algebra techniques.  \n",
    "We use the Scikit-Learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in entire dataframe\n",
    "\n",
    "df = pd.read_pickle(\"/home/scb8kw/git/dspg21RnD/data/dspg21RnD/smaller-final-dataset.pkl\")\n",
    "\n",
    "\n",
    "df.reset_index(inplace = True)\n",
    "#df.rename(columns={'index':'original index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>original index</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>FY</th>\n",
       "      <th>PROJECT_TERMS</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>AGENCY</th>\n",
       "      <th>IC_CENTER</th>\n",
       "      <th>...</th>\n",
       "      <th>BUDGET_START_DATE</th>\n",
       "      <th>BUDGET_END_DATE</th>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <th>FY.y</th>\n",
       "      <th>FY_TOTAL_COST</th>\n",
       "      <th>FY_TOTAL_COST_SUB_PROJECTS</th>\n",
       "      <th>ORG_COUNT</th>\n",
       "      <th>PI_COUNT</th>\n",
       "      <th>nchar</th>\n",
       "      <th>final_frqwds_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17608</td>\n",
       "      <td>152242</td>\n",
       "      <td>The multiprotein complex y-secretase proteolyt...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Active Sites; Affect; Alzheimer's Disease; Amy...</td>\n",
       "      <td>STRUCTURE OF SIGNAL PEPTIDE PEPTIDASE</td>\n",
       "      <td>HHS</td>\n",
       "      <td>NIH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.866</td>\n",
       "      <td>2008</td>\n",
       "      <td>3483.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1402</td>\n",
       "      <td>[multiprotein, y_secretase, proteolytically_cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>111864</td>\n",
       "      <td>190316</td>\n",
       "      <td>DESCRIPTION (provided by applicant):   The Kis...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Affect; Animal Model; Axon; Behavior; Behavior...</td>\n",
       "      <td>ROLE OF KISS1 NEURONS IN THE SEASONAL AND CIRC...</td>\n",
       "      <td>HHS</td>\n",
       "      <td>NIH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.865</td>\n",
       "      <td>2008</td>\n",
       "      <td>39175.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2553</td>\n",
       "      <td>[kissl, gene, encode, peptide, kisspeptin, bin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22052</td>\n",
       "      <td>154213</td>\n",
       "      <td>DESCRIPTION (provided by applicant): The objec...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Agreement; Antibodies; base; Binding; Biochemi...</td>\n",
       "      <td>CARBONIC ANHYDRASE AS A MODEL TO UNDERSTAND DI...</td>\n",
       "      <td>HHS</td>\n",
       "      <td>NIH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.859</td>\n",
       "      <td>2008</td>\n",
       "      <td>49646.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1414</td>\n",
       "      <td>[biophysical, basis, thermodynamics_kinetic, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>35004</td>\n",
       "      <td>159362</td>\n",
       "      <td>Obesity is the cause of many adverse pregnancy...</td>\n",
       "      <td>2008</td>\n",
       "      <td>African; Analysis of Variance; Asians; Birth; ...</td>\n",
       "      <td>OBESITY ON VAGAL TONE AND HBA1C DURING PREGNANCY</td>\n",
       "      <td>HHS</td>\n",
       "      <td>NIH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.361</td>\n",
       "      <td>2008</td>\n",
       "      <td>20406.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1545</td>\n",
       "      <td>[obesity, adverse_pregnancyoutcome, great, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>371628</td>\n",
       "      <td>594482</td>\n",
       "      <td>Local potato advisory groups have expressed in...</td>\n",
       "      <td>2010</td>\n",
       "      <td>cost; Health; interest; Manure; Parasitic nema...</td>\n",
       "      <td>PLANT-PARASITIC NEMATODE MANAGEMENT AS A COMPO...</td>\n",
       "      <td>USDA</td>\n",
       "      <td>NIFA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.203</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>271</td>\n",
       "      <td>[local, potato, advisory, express, interest, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  original index PROJECT_ID  \\\n",
       "0      0           17608     152242   \n",
       "1      1          111864     190316   \n",
       "2      2           22052     154213   \n",
       "3      3           35004     159362   \n",
       "4      4          371628     594482   \n",
       "\n",
       "                                            ABSTRACT    FY  \\\n",
       "0  The multiprotein complex y-secretase proteolyt...  2008   \n",
       "1  DESCRIPTION (provided by applicant):   The Kis...  2008   \n",
       "2  DESCRIPTION (provided by applicant): The objec...  2008   \n",
       "3  Obesity is the cause of many adverse pregnancy...  2008   \n",
       "4  Local potato advisory groups have expressed in...  2010   \n",
       "\n",
       "                                       PROJECT_TERMS  \\\n",
       "0  Active Sites; Affect; Alzheimer's Disease; Amy...   \n",
       "1  Affect; Animal Model; Axon; Behavior; Behavior...   \n",
       "2  Agreement; Antibodies; base; Binding; Biochemi...   \n",
       "3  African; Analysis of Variance; Asians; Birth; ...   \n",
       "4  cost; Health; interest; Manure; Parasitic nema...   \n",
       "\n",
       "                                       PROJECT_TITLE DEPARTMENT AGENCY  \\\n",
       "0              STRUCTURE OF SIGNAL PEPTIDE PEPTIDASE        HHS    NIH   \n",
       "1  ROLE OF KISS1 NEURONS IN THE SEASONAL AND CIRC...        HHS    NIH   \n",
       "2  CARBONIC ANHYDRASE AS A MODEL TO UNDERSTAND DI...        HHS    NIH   \n",
       "3   OBESITY ON VAGAL TONE AND HBA1C DURING PREGNANCY        HHS    NIH   \n",
       "4  PLANT-PARASITIC NEMATODE MANAGEMENT AS A COMPO...       USDA   NIFA   \n",
       "\n",
       "  IC_CENTER  ... BUDGET_START_DATE BUDGET_END_DATE CFDA_CODE  FY.y  \\\n",
       "0       NaN  ...               NaN             NaN    93.866  2008   \n",
       "1       NaN  ...               NaN             NaN    93.865  2008   \n",
       "2       NaN  ...               NaN             NaN    93.859  2008   \n",
       "3       NaN  ...               NaN             NaN    93.361  2008   \n",
       "4       NaN  ...               NaN             NaN    10.203  2010   \n",
       "\n",
       "  FY_TOTAL_COST FY_TOTAL_COST_SUB_PROJECTS ORG_COUNT PI_COUNT nchar  \\\n",
       "0        3483.0                        NaN         1        1  1402   \n",
       "1       39175.0                        NaN         1        1  2553   \n",
       "2       49646.0                        NaN         1        1  1414   \n",
       "3       20406.0                        NaN         1        1  1545   \n",
       "4           NaN                        NaN         1        1   271   \n",
       "\n",
       "                                final_frqwds_removed  \n",
       "0  [multiprotein, y_secretase, proteolytically_cl...  \n",
       "1  [kissl, gene, encode, peptide, kisspeptin, bin...  \n",
       "2  [biophysical, basis, thermodynamics_kinetic, m...  \n",
       "3  [obesity, adverse_pregnancyoutcome, great, hea...  \n",
       "4  [local, potato, advisory, express, interest, m...  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needed for doc-term matrix creation in Scikit-Learn is one string per document (not a list of strings).  \n",
    "# Original data 'ABSTRACT' is already in this form, but not the tokens in \"final_frqwds_removed\"\n",
    "\n",
    "tokens = df[\"final_frqwds_removed\"]\n",
    "\n",
    "docs = [] # docs will contain the processed tokens in string form (1 string per abstract)\n",
    "\n",
    "for abstract in tokens:\n",
    "    docs.append(\" \".join(abstract))\n",
    "    \n",
    "docs = pd.Series(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed for all info retrieval approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query vector \n",
    "\n",
    "def create_query(words, terms):\n",
    "    \n",
    "    # words: search query words\n",
    "    # terms: terms in corpus\n",
    "    \n",
    "    q = np.zeros(len(terms))  # number of terms\n",
    "\n",
    "    idx = []\n",
    "    for word in query_words:\n",
    "        idx.append(terms.index(word))\n",
    "\n",
    "    q[idx] = 1\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_abstracts(docs, scores, top_n):\n",
    "    \n",
    "    '''\n",
    "    docs: Series that contains abstract\n",
    "    scores: scores of abstracts\n",
    "    top_n: return the top_n abstracts given by idx, if top_n = -1 return all abstracts\n",
    "    '''\n",
    "    # sort scores in descending order\n",
    "    scores_sorted_idx = np.argsort(scores)[::-1]\n",
    "    \n",
    "    if top_n == -1:\n",
    "        n = sum(scores > 0)\n",
    "        ix = scores_sorted_idx[:n]\n",
    "    else:\n",
    "        ix = scores_sorted_idx[:top_n]\n",
    "    \n",
    "    print(ix[0:10])\n",
    "    \n",
    "    return ix, docs[ix]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_df(abstracts, scores):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"abstracts\"] = abstracts\n",
    "    df[\"scores\"] = scores\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literal Term Matching - Frequency Count Document-Term Matrix\n",
    "\n",
    "This will return all abstracts in the corpus with exact word matches to the query.  \n",
    "\n",
    "Results will be return in sorted order of how high the query scores with each abstract. A high score means more occurences of the query words in the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_stopwords():\\n      \\n    \"\"\" creates list of stopwords. stop words include the general English list and any additional we see sneaking \\n    through.  \"\"\"\\n    \\n    spacy_stop_words = STOP_WORDS\\n\\n    # more stop words that do not add meaning to topics\\n    additional_stopwords = {\\'addition\\', \\'specifically\\', \\'similar\\',\\'including\\', \\'particular\\', \\n                            \\'furthermore\\',\\'include\\', \\'includes\\',\\'overall\\', \\'finally\\', \\'specific\\', \\n                            \\'additional\\'} \\n           \\n    sw = spacy_stop_words.union(additional_stopwords)\\n    \\n    return sw\\n    \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note - we are now using the spaCy stopwords list instead of nltk.  It is more comprehensive.\n",
    "# ONLY USE THIS FUNCTION if using raw text to search\n",
    "\n",
    "'''\n",
    "def create_stopwords():\n",
    "      \n",
    "    \"\"\" creates list of stopwords. stop words include the general English list and any additional we see sneaking \n",
    "    through.  \"\"\"\n",
    "    \n",
    "    spacy_stop_words = STOP_WORDS\n",
    "\n",
    "    # more stop words that do not add meaning to topics\n",
    "    additional_stopwords = {'addition', 'specifically', 'similar','including', 'particular', \n",
    "                            'furthermore','include', 'includes','overall', 'finally', 'specific', \n",
    "                            'additional'} \n",
    "           \n",
    "    sw = spacy_stop_words.union(additional_stopwords)\n",
    "    \n",
    "    return sw\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrix based on count frequencies\n",
    "\n",
    "#stop_words = create_stopwords()\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, min_df=1)\n",
    "doc_term_matrix = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690814, 1277822)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Query Words - list the search terms\n",
    "\n",
    "A query is just a list of words to search for in the corpus.  We will use the same query for all three info retrieval techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'artificial_intelligence' in terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncount = 0\\nidx = []\\n\\nfor ix, abstract in enumerate(df['ABSTRACT']):\\n    if 'artificial intelligence' in abstract.lower(): \\n        count = count + 1\\n        idx.append(ix)\\n    elif 'artificialintelligence' in abstract.lower(): \\n        count = count + 1\\n        idx.append(ix)\\n    elif 'artificially intelligent' in abstract.lower(): \\n        count = count + 1\\n        idx.append(ix)\\n    #else: \\n        # do nothing\\n\\nprint(count)\\n\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLY USE FOR RAW TEXT STRING MATCHING\n",
    "\n",
    "# tokens with - or strange symbol between words\n",
    "\n",
    "'''\n",
    "count = 0\n",
    "idx = []\n",
    "\n",
    "for ix, abstract in enumerate(df['ABSTRACT']):\n",
    "    if 'artificial intelligence' in abstract.lower(): \n",
    "        count = count + 1\n",
    "        idx.append(ix)\n",
    "    elif 'artificialintelligence' in abstract.lower(): \n",
    "        count = count + 1\n",
    "        idx.append(ix)\n",
    "    elif 'artificially intelligent' in abstract.lower(): \n",
    "        count = count + 1\n",
    "        idx.append(ix)\n",
    "    #else: \n",
    "        # do nothing\n",
    "\n",
    "print(count)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE QUERY WORDS HERE \n",
    "\n",
    "# these words were found by analyzing all tokens with some form of 'artificial intelligence'\n",
    "query_words = ['artificial_intelligence', 'artificial_intelligence_ai', \n",
    "               'artificial_intelligence_machine_learning', 'artificialintelligence', 'artificially_intelligent',\n",
    "               'artificial_intelligence_aaai', 'artificial_intelligence_ijcai', 'artificialintelligence_ai',\n",
    "               'artificialintelligent'\n",
    "              ] \n",
    "\n",
    "#'ai', 'artificial', 'intelligence' \n",
    "              \n",
    "q = create_query(query_words, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the score for each document against the query. Docs with more occurences of the query words \n",
    "# will score higher\n",
    "\n",
    "f_scores = doc_term_matrix.dot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(f_scores >0)  # how many abstracts include at least one of the query words\n",
    "\n",
    "# some are being left off from raw counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 5., 5., 4., 4., 4., 4., 4., 4., 3.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "f_scores_sorted = np.sort(f_scores)[::-1]\n",
    "f_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[493912  83585  10300  19512 293012 689629 637044 689591 429296 292485]\n"
     ]
    }
   ],
   "source": [
    "f_idx, f_top_abstracts = return_top_abstracts(docs, f_scores, -1)  # CHANGE NUMBER OF TOP DOCS RETURNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493912    unique interdisciplinary team computer scienti...\n",
       "83585     eager award education collaboration kindle_mat...\n",
       "10300     graduate student attend workshop organize conj...\n",
       "19512     aaai artificial_intelligence interactive digit...\n",
       "293012    grant participation undergraduate student hold...\n",
       "                                ...                        \n",
       "363997    investigation topic temporal relation extracti...\n",
       "32191     broader commercial small_business_innovation s...\n",
       "357956    undergraduate reu site advance student technol...\n",
       "145096    yale spore skin cancer skin cancer basal cell ...\n",
       "122566    alignment rfa da 19_008 seek great_lakes node ...\n",
       "Length: 971, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unique interdisciplinary team computer scientist scientist ornithologist manager programmer network machine_learning human observational capacity explore synergy mechanical computation human computation human computer learning network broad scale citizen science network wide applicability variety domain network active learning feedback_loop machine human dramatically continually effectiveness network human computer learning network leverage contribution broad recruitment human observer artificial_intelligence algorithm total computational power far exceed sum individual highly successful ebird citizen science testbed human computer learning network bird engage global network volunteer submit million bird observation annually central database fundamental challenge citizen science error identification classification object quantify difference individual observer spatial bias prevalent citizen science challenge build advance artificial_intelligence opportunity generation account enormous complexity preliminary observer classification extend multi_label machine learn classification algorithm better ecological interpretation accurate prediction active learning algorithm construct sampling path optimize volunteer survey maximize spatial coverage incentivize participation crowdsourcing participant observation feedback artificial_intelligence broad scale citizen science recruit extensive network volunteer act intelligent trainable sensor environment gather observation artificial_intelligence dramatically observational volunteer filtering input observer expertise judgment aggregate historical guide observer immediate feedback observation accuracy customization observation worksheet artificial_intelligence advance expertise observer simultaneously artificial_intelligence decision benefit citizen science broader emerge world ubiquitous computing human machine partnership increasingly common'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_top_abstracts.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A unique interdisciplinary team of computer scientists, information scientists, ornithologists, project managers, and programmers will develop a novel network between machine learning methods and human observational capacity to explore the synergies between mechanical computation and human computation. This is called a Human/Computer Learning Network, and while the focus is to improve data quality in broad-scale citizen-science projects, the network has the potential for wide applicability in a variety of complex problem domains. The core of this network is an active learning feedback loop between machines and humans that dramatically improves the quality of both, and thereby continually improves the effectiveness of the network as a whole. The Human/Computer Learning Network will leverage the contributions of broad recruitment of human observers and process their contributed data with artificial intelligence algorithms leading to a total computational power far exceeding the sum of their individual parts. This work will use the highly successful eBird citizen-science project as a testbed to develop the Human/Computer Learning Network. eBird engages a global network of volunteers who submit tens of millions of bird observations annually to a central database.This research addresses three fundamental data quality challenges in citizen-science. These are: 1) reducing errors in identification or classification of objects; 2) identifying and quantifying the differences between individual observers; 3) reducing the spatial bias prevalent in many citizen-science projects. To address these challenges, the project will build on advances in artificial intelligence that now provide the opportunity to study systems through the generation of models that can account for enormous complexity. Preliminary work on observer classification will be extended by developing new multi-label machine learning classification algorithms that provide better ecological interpretations and more accurate predictions. In addition, the research will develop new active learning algorithms by constructing sampling paths that will optimize volunteer survey efforts to maximize overall spatial coverage, and incentivize participation via crowdsourcing techniques. Finally, it will study how participants can improve the quality of their observations based on the feedback and information provided by the artificial intelligence. Broad-scale citizen-science projects can recruit extensive networks of volunteers, who act as intelligent and trainable sensors in the environment to gather observations. Artificial intelligence processes can dramatically improve the quality of the observational data that volunteers can provide by filtering inputs based on observers' expertise, a judgment that is based on aggregated historical data. By guiding the observers with immediate feedback on observation accuracy and customization of observation worksheets, the artificial intelligence processes contribute to advancing expertise of the observers, while simultaneously improving the quality of the training data on which the artificial intelligence processes make their decisions. The results of the project will have significant benefit for all citizen science and broader impact in an emerging world of ubiquitous computing in which human-machine partnerships will become increasingly common.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ABSTRACT'][493912]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Document-Term Matrix\n",
    "\n",
    "This approach is similar to Literal Term Matching using frequency counts in the document-term matrix.  However, instead of using frequency counts, the entries of the document-term matrix are weighted using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find doc-term matrix using TF-IDF weighting\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(lowercase=True, min_df=1)\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_terms = tf_idf_vectorizer.get_feature_names()  # these terms are the same as the terms created from the \n",
    "                                                      # frequency count document-term matrix, so we do not need to\n",
    "                                                      # recreate the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_terms == terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the score for each document against the query. Docs with more occurences of the query words \n",
    "# will score higher\n",
    "\n",
    "tf_idf_scores = tf_idf.dot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tf_idf_scores >0)   # how many abstracts include at least one of the query words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67110913, 0.50600405, 0.44719966, 0.42104089, 0.40140886,\n",
       "       0.39908403, 0.37190372, 0.37129172, 0.37005479, 0.3416502 ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "tf_idf_scores_sorted = np.sort(tf_idf_scores)[::-1]\n",
    "tf_idf_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[235101 293012  10300 209895 493912 366791 581817 232572 108705 531638]\n"
     ]
    }
   ],
   "source": [
    "tfidf_idx, tfidf_top_abstracts = return_top_abstracts(docs, tf_idf_scores, -1)  # CHANGE NUMBER OF TOP DOCS RETURNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235101    phd student artificial_intelligence opportunit...\n",
       "293012    grant participation undergraduate student hold...\n",
       "10300     graduate student attend workshop organize conj...\n",
       "209895    imperative environmental societal sustainabili...\n",
       "493912    unique interdisciplinary team computer scienti...\n",
       "                                ...                        \n",
       "168231    animal human means better human barrier progre...\n",
       "688857    generate humanity past exceed create human his...\n",
       "114280    elderly patient dementia present massive care ...\n",
       "638620    scene ie recognize object human action event i...\n",
       "237025    background people multiple_sclerosis_ms chroni...\n",
       "Length: 971, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phd student artificial_intelligence opportunity share interact senior learn sub ai mentor publication career opportunity accomplish partially travel cost phd student attend international joint conference artificial_intelligence_ijcai premier international conference artificial_intelligence conference 2017 hold melbourne_australia attract international crowd academics industry worker entrepreneur funding agency leader'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_top_abstracts.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This proposal will support US-based Ph.D. students working in artificial intelligence the opportunity to share their knowledge and interact with each other and more senior researchers, to learn about different sub-fields within AI, and to be mentored in research, publication, and career opportunities. This goal will be accomplished by partially supporting the travel costs for US-based Ph.D. students to attend the International Joint Conference on Artificial Intelligence (IJCAI), which is one of the premier international conferences on research in artificial intelligence. The conference, which in 2017 will be held in Melbourne, Australia attracts an international crowd that includes academics, industry workers, entrepreneurs, and funding agency leaders.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ABSTRACT'][235101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing (LSI) Approach\n",
    "\n",
    "LSI Uses the TF-IDF matrix.  LSI is a tecnique that utilizes a truncated Singular Value Decomposition of the document-term matrix.  Basically, LSI still returns relevant documents to the query; however some of the documents returned may not include the exact search terms!  LSI is finding the latent or hidden relationships in the terms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-449e7c5cc077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                       \u001b[0;31m# MATRIX, BUT IS ALSO MORE EXPENSIVE AND MAY NOT LEAD TO THE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                       \u001b[0;31m# BEST INFO RETRIEVAL RESULTS.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mUSigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mVtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/HT_Bert/lib/python3.8/site-packages/sklearn/decomposition/_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 raise ValueError(\"n_components must be < n_features;\"\n\u001b[1;32m    181\u001b[0m                                  \" got %d >= %d\" % (k, n_features))\n\u001b[0;32m--> 182\u001b[0;31m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[0m\u001b[1;32m    183\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                                           random_state=random_state)\n",
      "\u001b[0;32m~/.conda/envs/HT_Bert/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/HT_Bert/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;31m# compute the SVD on the thin matrix: (k + p) wide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/HT_Bert/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/HT_Bert/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;32m~/.conda/envs/HT_Bert/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__rmatmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    564\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    565\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__rmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;31m####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/HT_Bert/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__rmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;31m#######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/HT_Bert/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/HT_Bert/lib/python3.8/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvecs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         fn(M, N, n_vecs, self.indptr, self.indices, self.data,\n\u001b[0;32m--> 492\u001b[0;31m            other.ravel(), result.ravel())\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Find the Truncated SVD of the TF-IDF matrix\n",
    "\n",
    "lsa = TruncatedSVD(n_components=500, random_state=1)  # CHANGE THE NUMBER OF COMPONENTS - NOTE: MORE COMPONENTS \n",
    "                                                      # GIVES YOU A MORE ACCURATE APPROXIMATION OF THE DOC-TERM \n",
    "                                                      # MATRIX, BUT IS ALSO MORE EXPENSIVE AND MAY NOT LEAD TO THE \n",
    "                                                      # BEST INFO RETRIEVAL RESULTS.\n",
    "USigma = lsa.fit_transform(tf_idf)\n",
    "Vtrans = lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform query to be in same space as documents\n",
    "\n",
    "q = q.reshape(1,-1)\n",
    "qhat = lsa.transform(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qhat.shape)\n",
    "print(USigma.shape)\n",
    "print(Vtrans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_scores = pairwise_distances(qhat, USigma, metric='cosine', n_jobs=7)  # CHANGE N_JOBS TO BE NUMBER OF CORES - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lsa_scores[0] > 0)  # how many abstracts scored above 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "lsa_scores_sorted = np.sort(lsa_scores[0])[::-1]\n",
    "lsa_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_idx, lsa_top_abstracts = return_top_abstracts(docs, lsa_scores[0], 50)  # CHANGE NUMBER OF TOP DOCS RETURNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_top_abstracts.iloc[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ABSTRACT'][344482]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't think LSI is giving us AI results - I spot checked 5-10 of these.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pandemics corpus\n",
    "\n",
    "We use the results of our three information retrieval techniques to create a new, smaller corpus that only contains abstracts relevant to the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ix = np.concatenate([f_idx, tfidf_idx]) #lsa_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_idx = np.unique(docs_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create case-study corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_corpus = df.loc[docs_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save or read corpus\n",
    "\n",
    "#ai_corpus.to_pickle(\"./ai_corpus.pkl\")\n",
    "#ai_corpus = pd.read_pickle(\"ai_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_docs = ai_corpus[\"final_frqwds_removed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needed for LDA, NMF (all from Scikit-Learn) is one string per document (not a list of strings)\n",
    "\n",
    "text = []\n",
    "\n",
    "for token_list in lim_docs:\n",
    "    text.append(\" \".join(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lim_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with relevant AI abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function slightly modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF document-term matrix for the AI corpus \n",
    "\n",
    "# TRY DIFFERENT PARAMETERS IN THE TF-IDF DOC-TERM MATRIX SET-UP\n",
    "nmf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=3, lowercase=True) #, max_features=int(len(lim_docs)/2))\n",
    "\n",
    "# by default TfidfVectorizer has l2 normalization for rows: \n",
    "# from Scikit Learn documentation: Each output row will have unit norm, either: * â€˜l2â€™: Sum of squares of vector \n",
    "# elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied.\n",
    "\n",
    "nmf_tf_idf = nmf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_terms = nmf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_terms[2830:2850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling with NMF\n",
    "\n",
    "nmf_model = NMF(n_components=30, random_state=1)  # TRY DIFFERENT NUMBERS OF TOPICS\n",
    "W = nmf_model.fit_transform(nmf_tf_idf)\n",
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topics(nmf_model, nmf_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### properties of AI corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project terms - worth looking into??\n",
    "\n",
    "ai_corpus['PROJECT_TERMS'].iloc[500] # does not have artificial intelligence as key term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_corpus['ABSTRACT'].iloc[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of abstracts by department\n",
    "\n",
    "y = ai_corpus[\"DEPARTMENT\"].value_counts()\n",
    "y.plot(kind = 'bar')\n",
    "plt.ylabel('Number of Abstracts')\n",
    "plt.title('Abstract Count by Department');\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of abstracts by project start year\n",
    "\n",
    "# extract year from project start date\n",
    "\n",
    "def getYear(a):   \n",
    "    a = str(a)\n",
    "    if a.find(\"/\"):\n",
    "        splitdate = a.split(\"/\")\n",
    "        if len(splitdate) == 3:\n",
    "            a = splitdate[2]\n",
    "        else:\n",
    "            a = splitdate[0]\n",
    "    year = str(a)\n",
    "    return year\n",
    "\n",
    "ai_corpus['START_YEAR'] = ai_corpus['PROJECT_START_DATE'].apply(getYear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ai_corpus[\"START_YEAR\"].value_counts().sort_index()\n",
    "y.plot(kind = 'bar')\n",
    "plt.ylabel('Number of Abstracts')\n",
    "plt.title('Abstract Count by Project Start Year');\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dollars by year\n",
    "\n",
    "dollars_by_year = ai_corpus.groupby(by = ['START_YEAR']).sum()\n",
    "dollars_by_year[\"FY_TOTAL_COST\"] = dollars_by_year[\"FY_TOTAL_COST\"]/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollars_by_year.plot.bar(y = \"FY_TOTAL_COST\")\n",
    "plt.ylabel('Dollars (millions)')\n",
    "plt.title('Dollars Spent by Project Start Year');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dollars by year & department\n",
    "\n",
    "dollars_by_deptNyear = ai_corpus.groupby(by = ['START_YEAR','DEPARTMENT']).sum()\n",
    "dollars_by_deptNyear[\"FY_TOTAL_COST\"] = dollars_by_deptNyear[\"FY_TOTAL_COST\"]/1000000\n",
    "\n",
    "dollars_by_deptNyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollars_by_deptNyear.plot.bar(y = \"FY_TOTAL_COST\", figsize=(15,5))\n",
    "plt.ylabel('Dollars (millions)')\n",
    "plt.title('Dollars Spent by Project Start Year and Department');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollars_by_deptNyear = dollars_by_deptNyear.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(ax=ax, x=\"START_YEAR\", y=\"FY_TOTAL_COST\", hue=\"DEPARTMENT\", data=dollars_by_deptNyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step - look at documents containing machine learning applications (such as topic 21)\n",
    "\n",
    "topic_docs = W[:, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(topic_docs > 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = max(topic_docs)\n",
    "max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lim_docs.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(topic_docs == max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(topic_docs > 0.3)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lim_docs.iloc[745]) # psychosis topic with AI component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_corpus[\"ABSTRACT\"].iloc[759]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-HT_Bert]",
   "language": "python",
   "name": "conda-env-.conda-HT_Bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
