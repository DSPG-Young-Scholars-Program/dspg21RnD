{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape AI Wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Crystal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: June 21, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "\n",
    "\n",
    "# web scrapping\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import bs4 as bs\n",
    "import urllib\n",
    "\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping all the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Not sure what's the difference between 'find_all' and 'select' functions from Beautiful Soup package. But it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
    "source = urllib.request.urlopen(url).read()\n",
    "\n",
    "#parse the data and create beautifulsoup object\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "# scrape all the text from wiki\n",
    "texts = \"\"\n",
    "#for paragraph in soup.find_all('p'):\n",
    "#    texts += paragraph.text\n",
    "    \n",
    "for paragraph in soup.select('p'):\n",
    "    texts += paragraph.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text and save as a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a text is empty\n",
    "def is_empty(texts):\n",
    "    if (texts =='\\n') or (texts=='\\r'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#clen wiki text\n",
    "def clean_wiki_text(texts):\n",
    "    if is_empty(texts):\n",
    "        return None\n",
    "    else:\n",
    "        texts = re.sub(r'\\[[0-9]*\\]', ' ', texts) #removes numbers\n",
    "       # texts = re.sub(r'\\s+',' ',texts) #remove long blank space\n",
    "       # texts = texts.lower() #lowercase text\n",
    "       # texts = re.sub(r'\\d', ' ', texts) \n",
    "       # texts = re.sub(r'\\s+', ' ', texts)\n",
    "        return(texts)\n",
    "\n",
    "#wiki_texts_clean = clean_wiki_text(texts)\n",
    "#print(wiki_texts_clean[1:1000])\n",
    "#stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example \n",
    "t = \"AI is ....[29]\"\n",
    "re.sub(r'\\[[0-9]*\\]', ' ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save cleaned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/ai_wiki_text\"+\".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#    for i in soup.select('p'):\n",
    " #       text_i = i.text\n",
    " #       text_i = clean_wiki_text(text_i)\n",
    " #       # write each paragraph to the file\n",
    " #       f.write(str(text_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping table of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reasoning, problem solving', 'Knowledge representation', 'Planning', 'Learning', 'Natural language processing', 'Perception', 'Motion and manipulation', 'Social intelligence', 'General intelligence', 'Cybernetics and brain simulation', 'Symbolic', 'Sub-symbolic', 'Statistical', 'Integrating the approaches', 'The limits of artificial general intelligence', 'Ethical machines', 'Machine consciousness, sentience and mind', 'Superintelligence', 'Risks of narrow AI', 'Risks of general AI', 'AI textbooks', 'History of AI', 'Other sources', '\\nPersonal tools\\n', '\\nNamespaces\\n', '\\nVariants\\n', '\\nViews\\n', '\\nMore\\n', '\\nSearch\\n', '\\nNavigation\\n', '\\nContribute\\n', '\\nTools\\n', '\\nPrint/export\\n', '\\nIn other projects\\n', '\\nLanguages\\n']\n"
     ]
    }
   ],
   "source": [
    "# nested items listed in the table of contents\n",
    "topics = []\n",
    "for topic in soup.find_all(\"h3\"):\n",
    "    topics.append(topic.text)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping all the links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Exclude url links that contain:\n",
    "    - \".png\"\n",
    "    - \".jpg\"\n",
    "    - \".svg\"\n",
    "    - \".gif\"\n",
    "    - \"Template\"\n",
    "    - \"(identifier)\", might be a link to a journal article\n",
    "\n",
    "\n",
    "2. Exclude duplicate links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584, 2)\n"
     ]
    }
   ],
   "source": [
    "url_ls = []\n",
    "title_ls = []\n",
    "\n",
    "condition = url.startswith(\"/wiki/\") & (not url.endswith(\".jpg\"))  & (not url.endswith(\".png\"))  & (not \"Template\" in url) & (not url.endswith(\".gif\"))  & (not \"(identifier)\" in url)\n",
    "\n",
    "for link in soup.find_all(\"a\"):\n",
    "    url = link.get(\"href\", \"\")\n",
    "    if url.startswith(\"/wiki/\") & (not url.endswith(\".jpg\"))  & (not url.endswith(\".png\"))  & (not \"Template\" in url) & (not url.endswith(\".gif\"))  & (not \"(identifier)\" in url):\n",
    "       # print(url)\n",
    "        url_ls.append(url)\n",
    "        title_ls.append(link.text.strip())\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'url':url_ls, 'title':title_ls})\n",
    "df = df.drop_duplicates(subset = 'url', keep = 'first')\n",
    "\n",
    "print(df.shape)\n",
    "df.to_csv(r'/home/zz3hs/git/dspg21RnD/data/dspg21RnD/wiki_ai_links.csv', index = False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: \n",
    "1. Remove unrelated items\n",
    "    - people's names\n",
    "2. What to do with the topics listed under \"Articles related to Artificial intelligence\"?\n",
    "3. What to do with acronames such as \"k-NN\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping topics within the itemized list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: suppose to grab three sections with class sidebar-content hlist, but only grabbing the first one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. AI, ML/data mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial general intelligence', 'Planning', 'Computer vision', 'General game playing', 'Knowledge reasoning', 'Machine learning', 'Natural language processing', 'Robotics']\n"
     ]
    }
   ],
   "source": [
    "# getting the headding too\n",
    "div = soup.find('div', {\"class\": \"sidebar-list-content mw-collapsible-content\"})\n",
    "topics = []\n",
    "for topic in div.find_all(\"a\"):\n",
    "    topics.append(topic.text)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Major goals', 'Artificial general intelligence', 'Planning', 'Computer vision', 'General game playing', 'Knowledge reasoning', 'Machine learning', 'Natural language processing', 'Robotics']\n"
     ]
    }
   ],
   "source": [
    "# Problem: suppose to grab all sections with class sidebar-content, but only grabbing the first one\n",
    "# getting the items only (not heading)\n",
    "td = soup.find('td', {\"class\": \"sidebar-content\"})\n",
    "topics = []\n",
    "for topic in td.find_all(\"a\"):\n",
    "    topics.append(topic.text)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Evolutionary algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial development', 'Artificial life', 'Cellular evolutionary algorithm', 'Cultural algorithm', 'Differential evolution', 'Effective fitness', 'Evolutionary computation', 'Evolution strategy', 'Gaussian adaptation', 'Evolutionary multimodal optimization', 'Particle swarm optimization', 'Memetic algorithm', 'Natural evolution strategy', 'Neuroevolution', 'Promoter based genetic algorithm', 'Spiral optimization algorithm', 'Self-modifying code', 'Polymorphic code']\n"
     ]
    }
   ],
   "source": [
    "# part of a series on evolutionary algorithm\n",
    "td = soup.find('td', {\"class\": \"sidebar-content hlist\"})\n",
    "topics = []\n",
    "for topic in td.find_all(\"a\"):\n",
    "    topics.append(topic.text)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-crystal]",
   "language": "python",
   "name": "conda-env-.conda-crystal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
