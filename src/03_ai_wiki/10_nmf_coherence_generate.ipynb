{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use BII, 20 cores, 3 hours, 100 Ram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:Input a dataframe that are ai related abstracts, need variables: final_frqwds_removed\n",
    "#abstracts = pd.read_csv(r'/home/zz3hs/git/dspg21RnD/data/dspg21RnD/bert_ai_abstracts.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ['effectiveness', 'computerize', 'instructiona...\n",
       "1        ['direct', 'identification', 'basic', 'apply',...\n",
       "2        ['generation', 'economist', 'harrod', 'domar',...\n",
       "3        ['developer', 'create', 'prototype', 'playing_...\n",
       "4        ['team', 'prototype', 'gowrite', 'platform', '...\n",
       "                               ...                        \n",
       "22553    ['public', 'health', 'child', 'mental', 'healt...\n",
       "22554    ['purpose', 'advance', 'jacket_gpu_engine', 'm...\n",
       "22555    ['american_indian', 'science', 'engineering', ...\n",
       "22556    ['astep', 'observational', 'technology', 'enab...\n",
       "22557    ['advance', 'genome', 'interactive', 'explorat...\n",
       "Name: final_frqwds_removed, Length: 22558, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[\"final_frqwds_removed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option(\"display.max_colwidth\", -1)\n",
    "#abstracts[abstracts[\"cosine_similarity_score\"] >0.55].ABSTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstracts[abstracts[\"cosine_similarity_score\"]  <0.41].ABSTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Model to find the optimal number of topics for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = abstracts[\"final_frqwds_removed\"]\n",
    "\n",
    "clean_docs = []\n",
    "for doc in docs:\n",
    "    doc = ast.literal_eval(doc)\n",
    "    clean_docs.append(doc)\n",
    "    \n",
    "text = []\n",
    "for abstract in clean_docs:\n",
    "    text.append(\" \".join(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'effectiveness'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_docs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'effectiveness computerize instructional adjunct standard integrate brain injury rehabilitation member satisfaction utilize computerize inform participant instructional selection maximize benefit'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=3)\n",
    "tf_idf = tfidf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22558, 23678)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVars(docs):\n",
    "\n",
    "    # Create the variables needed for NMF from df[final_frqwds_removed]: dictionary (id2word), corpus\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "    if len(docs) <= 100000:\n",
    "        id2word.filter_extremes(no_below=3, no_above=1.0,  keep_n = 100000)\n",
    "        print(\"Use keep_n = 100,000 defalt.\")\n",
    "    else:\n",
    "        id2word.filter_extremes(no_below=3, no_above=1.0,  keep_n = len(docs))\n",
    "        print(\"Number of documents exceed the dafalt number of 100,000. Use the keep_n = number of document.\")\n",
    "\n",
    "    # Create Corpus (Term Document Frequency)\n",
    "\n",
    "    #Creates a count for each unique word appearing in the document, where the word_id is substituted for the word\n",
    "    corpus = [id2word.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    return id2word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use keep_n = 100,000 defalt.\n"
     ]
    }
   ],
   "source": [
    "#TODO: run the following code to generate id2word and corpus\n",
    "#id2word, corpus = createVars(clean_docs)\n",
    "#TODO: RENAME the file, run the code to save the output\n",
    "docs  = clean_docs\n",
    "#pickle.dump([corpus, id2word, docs], open('../../data/dspg21RnD/coherence_vars_XXX.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Coherence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Read in your coherence data (change the name of the file)\n",
    "f = open('../../data/dspg21RnD/coherence_vars_bert.sav', 'rb')\n",
    "[corpus, id2word, docs] = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22558"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['effectiveness',\n",
       " 'computerize',\n",
       " 'instructional',\n",
       " 'adjunct',\n",
       " 'standard',\n",
       " 'integrate',\n",
       " 'brain',\n",
       " 'injury',\n",
       " 'rehabilitation',\n",
       " 'member',\n",
       " 'satisfaction',\n",
       " 'utilize',\n",
       " 'computerize',\n",
       " 'inform',\n",
       " 'participant',\n",
       " 'instructional',\n",
       " 'selection',\n",
       " 'maximize',\n",
       " 'benefit']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for doc in docs:\n",
    "    text.append(\" \".join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'effectiveness computerize instructional adjunct standard integrate brain injury rehabilitation member satisfaction utilize computerize inform participant instructional selection maximize benefit'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function slightly modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)\n",
    "            \n",
    "# Function to format topics as a \"list of list of strings\".\n",
    "# Needed for topic coherence function in Gensim\n",
    "\n",
    "# function modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def list_topics(model, vectorizer, top_n=10):\n",
    "\n",
    "    #input. top_n: how many words to list per topic.  If -1, then list all words.\n",
    "       \n",
    "    topic_words = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "            \n",
    "        if top_n == -1:   \n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1]])\n",
    "        else:\n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document-term matrix - TFIDF \n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=3)\n",
    "tf_idf = tfidf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "\n",
    "def nmf_metrics(doc_term_matrix, n_topics, vectorizer, corpus, id2word, docs, rand_start):\n",
    "    \"\"\"\n",
    "    Compute c_v topic coherence for various number of topics\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tf_idf\n",
    "    n_topics : list of number of topics\n",
    "    Returns:\n",
    "    -------\n",
    "    coherence_values : c_v topic coherence values corresponding to the NMF model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    coherence_values = []\n",
    "    \n",
    "    i = rand_start\n",
    "    for num_topics in n_topics:\n",
    "\n",
    "        # create model\n",
    "        t1 = time.time()\n",
    "        nmf_model = NMF(n_components=num_topics, random_state = i)\n",
    "        nmf_model.fit_transform(doc_term_matrix)\n",
    "        t2 = time.time()\n",
    "        print(f\"  Model time: {t2-t1}\")\n",
    "        \n",
    "        # create list of topics\n",
    "        topics = list_topics(nmf_model, vectorizer, top_n=10)\n",
    "        \n",
    "        # calculate coherence\n",
    "        t1 = time.time()\n",
    "        \n",
    "        #TODO:manually adjust number of processes\n",
    "        cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=id2word, texts=docs, \n",
    "                            coherence='c_v', #model for calculating coherence score\n",
    "                            processes=19 #for smaller corpus, pronesses= number of cores - 5\n",
    "                           ) #window_size=500 ) \n",
    "        coherence_values.append(cm.get_coherence())\n",
    "        t2 = time.time()\n",
    "        print(f\"  Coherence time: {t2-t1}\")\n",
    "        \n",
    "        # output completion message\n",
    "        i = i+1\n",
    "        print('Number of topics =', num_topics, \"complete.\")\n",
    "\n",
    "    return coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  Model time: 3.132255792617798\n",
      "  Coherence time: 2.2182235717773438\n",
      "Number of topics = 5 complete.\n",
      "  Model time: 6.946704864501953\n",
      "  Coherence time: 3.4823157787323\n",
      "Number of topics = 10 complete.\n",
      "  Model time: 10.436957359313965\n",
      "  Coherence time: 4.005496978759766\n",
      "Number of topics = 15 complete.\n",
      "  Model time: 10.771311283111572\n",
      "  Coherence time: 4.4601075649261475\n",
      "Number of topics = 20 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zz3hs/.conda/envs/crystal_bert/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model time: 30.37483859062195\n",
      "  Coherence time: 5.9043214321136475\n",
      "Number of topics = 25 complete.\n",
      "  Model time: 33.9063618183136\n",
      "  Coherence time: 6.94676661491394\n",
      "Number of topics = 30 complete.\n"
     ]
    }
   ],
   "source": [
    "# code copied from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "# minor alterations made\n",
    "n_topics = list(range(5,51,5)) + [75,100,125] #takes about 4-5 hrs to run\n",
    "\n",
    "num_runs = 10\n",
    "\n",
    "col_names = [f\"iteration {i}\" for i in range(num_runs)]\n",
    "nmf_c = pd.DataFrame(index = n_topics, columns = col_names)\n",
    "\n",
    "for i in range(num_runs):\n",
    "    print(f\"Iteration {i}\")\n",
    "    \n",
    "    # run models\n",
    "    c = nmf_metrics(doc_term_matrix=tf_idf, n_topics=n_topics, vectorizer=tfidf_vectorizer, \n",
    "                         corpus=corpus, id2word=id2word, docs=docs, rand_start = (i)*len(n_topics))\n",
    "    \n",
    "    # save results\n",
    "    nmf_c[f\"iteration {i}\"] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save results, change file name\n",
    "nmf_c.to_pickle(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/nmf_bert_10.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-crystal_bert]",
   "language": "python",
   "name": "conda-env-.conda-crystal_bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
