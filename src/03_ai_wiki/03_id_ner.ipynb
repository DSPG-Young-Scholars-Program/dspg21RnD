{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition on AI Wiki text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arthor: Crystal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major package used: Stanza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e426b40561cf4d3a91ca10bc9732602d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-27 17:38:42 INFO: Downloading default packages for language: en (English)...\n",
      "2021-06-27 17:38:43 INFO: File exists: /home/zz3hs/stanza_resources/en/default.zip.\n",
      "2021-06-27 17:38:48 INFO: Finished downloading models and saved to /home/zz3hs/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "\n",
    "\n",
    "# web scrapping\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import bs4 as bs\n",
    "import urllib\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#name entity recognition\n",
    "import stanza\n",
    "stanza.download('en') # download English model\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import warnings\n",
    "#warnings.simplefilter('always')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in named entity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'/home/zz3hs/git/dspg21RnD/data/dspg21RnD/ai_wiki_text_entity.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>end_char</th>\n",
       "      <th>start_char</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>Tesler</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1862.0</td>\n",
       "      <td>1857.0</td>\n",
       "      <td>Alpha</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2656.0</td>\n",
       "      <td>2653.0</td>\n",
       "      <td>AGI</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>390</td>\n",
       "      <td>62078.0</td>\n",
       "      <td>62058.0</td>\n",
       "      <td>Karel Čapek's R.U.R.</td>\n",
       "      <td>WORK_OF_ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>391</td>\n",
       "      <td>62133.0</td>\n",
       "      <td>62090.0</td>\n",
       "      <td>A.I. Artificial Intelligence and Ex Machina</td>\n",
       "      <td>WORK_OF_ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>392</td>\n",
       "      <td>62191.0</td>\n",
       "      <td>62138.0</td>\n",
       "      <td>well as the novel Do Androids Dream of Electri...</td>\n",
       "      <td>WORK_OF_ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>393</td>\n",
       "      <td>62211.0</td>\n",
       "      <td>62197.0</td>\n",
       "      <td>Philip K. Dick</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>394</td>\n",
       "      <td>62217.0</td>\n",
       "      <td>62213.0</td>\n",
       "      <td>Dick</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  end_char  start_char  \\\n",
       "0             0    1046.0      1040.0   \n",
       "1             1    1642.0      1638.0   \n",
       "2             2    1862.0      1857.0   \n",
       "3             3    1906.0      1902.0   \n",
       "4             4    2656.0      2653.0   \n",
       "..          ...       ...         ...   \n",
       "390         390   62078.0     62058.0   \n",
       "391         391   62133.0     62090.0   \n",
       "392         392   62191.0     62138.0   \n",
       "393         393   62211.0     62197.0   \n",
       "394         394   62217.0     62213.0   \n",
       "\n",
       "                                                  text         type  \n",
       "0                                               Tesler       PERSON  \n",
       "1                                                 1955         DATE  \n",
       "2                                                Alpha       PERSON  \n",
       "3                                                 2015         DATE  \n",
       "4                                                  AGI          ORG  \n",
       "..                                                 ...          ...  \n",
       "390                               Karel Čapek's R.U.R.  WORK_OF_ART  \n",
       "391        A.I. Artificial Intelligence and Ex Machina  WORK_OF_ART  \n",
       "392  well as the novel Do Androids Dream of Electri...  WORK_OF_ART  \n",
       "393                                     Philip K. Dick       PERSON  \n",
       "394                                               Dick       PERSON  \n",
       "\n",
       "[395 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#de-duplicate text\n",
    "df_unique = df.drop_duplicates(subset=\"text\")\n",
    "len(df_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ORG            69\n",
       "PERSON         58\n",
       "DATE           58\n",
       "WORK_OF_ART    29\n",
       "CARDINAL       17\n",
       "GPE            13\n",
       "PRODUCT        10\n",
       "LOC             6\n",
       "ORDINAL         5\n",
       "PERCENT         5\n",
       "NORP            4\n",
       "MONEY           3\n",
       "TIME            2\n",
       "LANGUAGE        1\n",
       "QUANTITY        1\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique[\"type\"].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Named entities that we are not considering:\n",
    "- Cardinal, just numbers, which wer are not concerned about that could be linked to another Wiki page\n",
    "- Product, such as Skype, kINECT, Xobx 360\n",
    "- Percent\n",
    "- Ordinal\n",
    "- Money\n",
    "- Time\n",
    "- Quantity\n",
    "- Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGI',\n",
       " 'AI',\n",
       " \"Alan Turing's\",\n",
       " 'Dartmouth College',\n",
       " 'Attendees Allen Newell',\n",
       " 'CMU)',\n",
       " 'Herbert Simon',\n",
       " 'MIT)',\n",
       " 'Arthur Samuel',\n",
       " 'IBM',\n",
       " 'the Department of Defense',\n",
       " 'Congress',\n",
       " 'ANN',\n",
       " 'Lee Sedol',\n",
       " 'AlphaZero',\n",
       " 'MuZero',\n",
       " 'Bloomberg',\n",
       " 'Microsoft',\n",
       " 'Facebook',\n",
       " 'DeepMind',\n",
       " 'SVM',\n",
       " 'Occam',\n",
       " 'Moravec',\n",
       " 'Cyc',\n",
       " 'Japanese Fifth Generation Computer Systems',\n",
       " \"W. Grey Walter's\",\n",
       " 'the Johns Hopkins Beast',\n",
       " 'the Teleological Society',\n",
       " 'Princeton University',\n",
       " 'the Ratio Club',\n",
       " 'Carnegie Mellon University',\n",
       " 'Stanford',\n",
       " 'MIT',\n",
       " 'Allen Newell',\n",
       " 'Simon and Newell',\n",
       " 'Stanford (SAIL',\n",
       " 'Seymour Papert',\n",
       " 'CMU',\n",
       " 'Edward Feigenbaum)',\n",
       " 'Grey',\n",
       " 'GOFAI',\n",
       " 'Google Search',\n",
       " 'Siri',\n",
       " 'Deepfakes',\n",
       " 'ZDNet',\n",
       " 'Computer Power and Human Reason',\n",
       " 'AMA',\n",
       " 'AMAs',\n",
       " \"California's Institute\",\n",
       " 'Strong AI',\n",
       " 'Aldous Huxley',\n",
       " 'European Union',\n",
       " 'Economist',\n",
       " 'OECD',\n",
       " 'Ford',\n",
       " 'United Nations',\n",
       " 'the Future of Life Institute',\n",
       " 'ProPublica',\n",
       " 'COMPAS',\n",
       " 'SpaceX',\n",
       " 'Peter Thiel',\n",
       " 'Amazon Web Services',\n",
       " 'Musk',\n",
       " 'OpenAI',\n",
       " 'Oracle',\n",
       " 'the European Union',\n",
       " \"Arthur C. Clarke's\",\n",
       " \"Stanley Kubrick's\",\n",
       " 'Multivac']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_ls = df_unique[df_unique[\"type\"] == \"ORG\"].text.tolist()\n",
    "org_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1955',\n",
       " '2015',\n",
       " 'the twenty-first century',\n",
       " '1943',\n",
       " '1956',\n",
       " '1954',\n",
       " '1959',\n",
       " 'the middle of the 1960s',\n",
       " 'twenty years',\n",
       " '1974',\n",
       " 'next few years',\n",
       " 'the early 1980s',\n",
       " '1985',\n",
       " '1987',\n",
       " 'the 1980s',\n",
       " '1989',\n",
       " 'the late 1990s',\n",
       " 'early 21st century',\n",
       " '11 May 1997',\n",
       " '2011',\n",
       " '2012',\n",
       " 'March 2016',\n",
       " 'the 2017',\n",
       " 'two years',\n",
       " 'year',\n",
       " '2017',\n",
       " '2016',\n",
       " '2020',\n",
       " '10,000 days',\n",
       " 'the late 1980s',\n",
       " '1990s',\n",
       " '2019',\n",
       " '1988',\n",
       " 'millions of years',\n",
       " '1984',\n",
       " '1982–1992',\n",
       " '2010s',\n",
       " 'the 1940s and 1950s',\n",
       " '1960',\n",
       " '1980s',\n",
       " '1960s',\n",
       " 'the 1960s and the 1970s',\n",
       " '1970',\n",
       " '1950s',\n",
       " 'election year',\n",
       " '2005',\n",
       " 'the next few hundred years',\n",
       " '2010',\n",
       " 'the year 2029',\n",
       " '1863',\n",
       " '1998',\n",
       " 'February 2020',\n",
       " 'January 2015',\n",
       " '2001',\n",
       " '1968',\n",
       " '1999',\n",
       " '1951',\n",
       " '1986']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_ls = df_unique[df_unique[\"type\"] == \"DATE\"].text.tolist()\n",
    "date_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tesler',\n",
       " 'Alpha',\n",
       " 'Karel Čapek',\n",
       " 'Turing',\n",
       " 'John McCarthy',\n",
       " 'Norbert Wiener',\n",
       " 'Marvin Minsky',\n",
       " 'James Lighthill',\n",
       " 'A. Mead',\n",
       " 'Mohammed Ismail',\n",
       " 'Moore',\n",
       " 'Garry Kasparov',\n",
       " 'Watson',\n",
       " 'Brad Rutter',\n",
       " 'Ken Jennings',\n",
       " 'Ke Jie',\n",
       " \"Deep Blue's\",\n",
       " 'Murray Campbell',\n",
       " 'Jack Clark',\n",
       " 'Clark',\n",
       " 'John Haugeland',\n",
       " 'Roger Schank',\n",
       " 'David Rumelhart',\n",
       " 'Markov',\n",
       " 'Joseph Weizenbaum',\n",
       " 'Weizenbaum',\n",
       " 'Wendell Wallach',\n",
       " 'Wallach',\n",
       " 'Charles T. Rubin',\n",
       " 'David Chalmers',\n",
       " 'Jerry Fodor',\n",
       " 'Hilary Putnam',\n",
       " 'John Searle',\n",
       " 'Searle',\n",
       " 'Vernor Vinge',\n",
       " 'Ray Kurzweil',\n",
       " 'Kevin Warwick',\n",
       " 'Robert Ettinger',\n",
       " \"Samuel Butler's\",\n",
       " 'George Dyson',\n",
       " 'Pricewaterhouse',\n",
       " 'Michael Osborne',\n",
       " 'Carl Benedikt Frey',\n",
       " 'Martin Ford',\n",
       " \"Andrew Yang's\",\n",
       " 'Irakli Beridze',\n",
       " 'Stephen Hawking',\n",
       " 'Bill Gates',\n",
       " 'Yuval Noah Harari',\n",
       " 'Elon Musk',\n",
       " 'Hawking',\n",
       " 'Mark Hurd',\n",
       " 'Mark Zuckerberg',\n",
       " 'Isaac Asimov',\n",
       " 'Asimov',\n",
       " 'George Lucas',\n",
       " 'Philip K. Dick',\n",
       " 'Dick']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_ls = df_unique[df_unique[\"type\"] == \"PERSON\"].text.tolist()\n",
    "person_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: WORK_OF_ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Mary Shelley's Frankenstein\",\n",
       " 'Turing-complete \"',\n",
       " 'Analog VLSI Implementation',\n",
       " 'Future of Go Summit',\n",
       " 'Moral Machines  For Wallach',\n",
       " 'part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as \"Does Humanity Want Computers Making Moral Decisions\"',\n",
       " 'Symposium on Machine Ethics',\n",
       " 'Ethics',\n",
       " 'the AAAI Fall 2005 Symposium on Machine Ethics',\n",
       " 'I think',\n",
       " 'Plug & Pray',\n",
       " 'Star Trek Next Generation, with the character of Commander Data',\n",
       " 'Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\"',\n",
       " 'Darwin among the Machines\"',\n",
       " 'book of the same name',\n",
       " 'Nick Bostrom',\n",
       " 'Human Compatible, AI researcher Stuart J. Russell',\n",
       " 'A Space Odyssey',\n",
       " 'The Terminator',\n",
       " 'The Matrix',\n",
       " 'Gort from The Day the Earth Stood Still',\n",
       " 'Bishop from Aliens',\n",
       " 'the Three Laws of Robotics',\n",
       " 'Ghost in the Shell',\n",
       " \"Hajime Sorayama's Sexy Robots\",\n",
       " 'Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids',\n",
       " \"Karel Čapek's R.U.R.\",\n",
       " 'A.I. Artificial Intelligence and Ex Machina',\n",
       " 'well as the novel Do Androids Dream of Electric Sheep']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_of_art_ls = df_unique[df_unique[\"type\"] == \"WORK_OF_ART\"].text.tolist()\n",
    "work_of_art_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U.S.',\n",
       " 'US',\n",
       " 'Japan',\n",
       " 'U.S',\n",
       " 'China',\n",
       " 'Denver',\n",
       " 'England',\n",
       " 'Edinburgh',\n",
       " 'the United States',\n",
       " 'Russia',\n",
       " 'the United Kingdom',\n",
       " 'Republic',\n",
       " 'Bostrom']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpe_ls = df_unique[df_unique[\"type\"] == \"GPE\"].text.tolist()\n",
    "gpe_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['East', 'San Francisco', 'West', 'Europe', 'Rodney Brooks', 'Earth']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_ls = df_unique[df_unique[\"type\"] == \"LOC\"].text.tolist()\n",
    "loc_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: NORP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['British', 'Bayesian', 'Americans', 'Chinese']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norp_ls = df_unique[df_unique[\"type\"] == \"NORP\"].text.tolist()\n",
    "norp_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all the named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = org_ls + date_ls + person_ls + work_of_art_ls + gpe_ls + loc_ls  + norp_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-crystal]",
   "language": "python",
   "name": "conda-env-.conda-crystal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
