{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Crystal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, it takes 9.663 seconds to run 100 abstracts. We are working with 690,815 abstract, which will take about 18.6 hours to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "#nltk.download() #input: punkt\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "\n",
    "import statistics #calculate mean and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690814"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2') # this model provides quick model with high quality\n",
    "#embedder = SentenceTransformer('paraphrase-mpnet-base-v2') # this model provides the best quality\n",
    "\n",
    "abstracts = pd.read_pickle(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/FR-cleaned-2021FEB24.pkl\")\n",
    "len(abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = abstracts.iloc[590853:690814]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_embeddings(dir):\n",
    "    with open(dir) as f:\n",
    "        ai_text = f.read()\n",
    "    ai_corpus = tokenize.sent_tokenize(ai_text) #sentence tokenization\n",
    "    ai_embeddings = embedder.encode(ai_corpus, show_progress_bar=True) # embeddings\n",
    "    return ai_embeddings\n",
    "\n",
    "\n",
    "# k: number of similar sentences from AI corpus\n",
    "# abstract: abstract from FEDERAL RePORTER\n",
    "# print_result: if TRUE, print out the similar sentenses from AI corpus to each sentence in the abstract\n",
    "def get_score(k, abstract, print_result = False):\n",
    "    queries = tokenize.sent_tokenize(abstract) \n",
    "\n",
    "    # init a result list for scores\n",
    "    result = []\n",
    "    \n",
    "    # Find the closest k sentences of the AI corpus for each query sentence (ML) based on cosine similarity\n",
    "    top_k = min(k, len(ai_embeddings))\n",
    "    \n",
    "    for query in queries: #compare each sentence in the abstract to the ai corpus\n",
    "        query_embedding = embedder.encode(query, show_progress_bar=False) \n",
    "        \n",
    "        # We use cosine-similarity and torch.topk to find the highest k scores\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, ai_embeddings)[0]\n",
    "        \n",
    "        top_results = torch.topk(cos_scores, k=top_k)   #get the top k scores\n",
    "        result.append(top_results.values.tolist()) #unlist the top result list\n",
    "        if print_result:\n",
    "            print(\"\\n\\n======================\\n\\n\")\n",
    "            print(\"Query:\", query)\n",
    "            print(\"Results:\", top_results)\n",
    "            print(\"\\nTop k=5 most similar sentences in corpus:\")\n",
    "            for score, idx in zip(top_results[0], top_results[1]):\n",
    "                print(ai_corpus[idx], \"(Score: {:.4f})\".format(score))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time for calculating embeddings: 2021-07-12 09:04:56.323026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96311daeb1ec4c0680fc960b99a1eec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "print(\"Start time for calculating embeddings:\", start_time)\n",
    "\n",
    "ai_embeddings = get_corpus_embeddings(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/ai_wiki_text.txt\")\n",
    "\n",
    "abstracts = abstracts.assign(score= abstracts[\"ABSTRACT\"].apply(lambda x: get_score(5,x,False)))\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Finished calculating \", len(abstracts), \"of\", \"embedding score at\", end_time)\n",
    "print(\"It took\", end_time-start_time, \"to run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(\"Start time for calculating average score:\", start_time)\n",
    "\n",
    "\n",
    "sentence_score= []\n",
    "for abstract in abstracts[\"score\"]:\n",
    "    sentence_score.append([statistics.mean(i) for i in abstract])\n",
    "\n",
    "abstracts[\"sentence_score\"]=sentence_score\n",
    "abstracts\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Finished calculating \", len(abstracts), \"of\", \"average score at\", end_time)\n",
    "print(\"It took\", end_time-start_time, \"to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts.to_csv(r'/home/zz3hs/git/dspg21RnD/data/dspg21RnD/abstracts_embedding_score_4.csv', index = False)   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "abstract1 = pd.read_csv(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/abstracts_embedding_score_1.csv\")\n",
    "abstract2 = pd.read_csv(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/abstracts_embedding_score_2.csv\")\n",
    "abstract3 = pd.read_csv(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/abstracts_embedding_score_3.csv\")\n",
    "abstract4 = pd.read_csv(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/abstracts_embedding_score_4.csv\")\n",
    "frames = [abstract1, abstract2, abstract3, abstract4]\n",
    "\n",
    "abstracts_score = pd.concat(frames)\n",
    "abstracts_score\n",
    "\n",
    "abstracts_score = abstracts_score[[\"index\", \"original index\", \"PROJECT_ID\",\"score\",\"sentence_score\"]]\n",
    "abstracts_score.to_csv(r'/home/zz3hs/git/dspg21RnD/data/dspg21RnD/abstracts_embedding_score_all.csv', index = False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-crystal_bert]",
   "language": "python",
   "name": "conda-env-.conda-crystal_bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
