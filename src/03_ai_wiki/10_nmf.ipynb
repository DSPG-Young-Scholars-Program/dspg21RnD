{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:Input a dataframe that are ai related abstracts, need variables: final_frqwds_removed\n",
    "abstracts = pd.read_csv(r'/home/zz3hs/git/dspg21RnD/data/dspg21RnD/bert_ai_abstracts.csv')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Model to find the optimal number of topics for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVars(docs):\n",
    "\n",
    "    # Create the variables needed for NMF from df[final_frqwds_removed]: dictionary (id2word), corpus\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "    if len(docs) <= 100000:\n",
    "        id2word.filter_extremes(no_below=3, no_above=1.0,  keep_n = 100000)\n",
    "        print(\"Use keep_n = 100,000 defalt.\")\n",
    "    else:\n",
    "        id2word.filter_extremes(no_below=3, no_above=1.0,  keep_n = len(docs))\n",
    "        print(\"Number of documents exceed the dafalt number of 100,000. Use the keep_n = number of document.\")\n",
    "\n",
    "    # Create Corpus (Term Document Frequency)\n",
    "\n",
    "    #Creates a count for each unique word appearing in the document, where the word_id is substituted for the word\n",
    "    corpus = [id2word.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    return id2word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = abstracts[\"final_frqwds_removed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [multiprotein, y_secretase, proteolytically_cl...\n",
       "1         [kissl, gene, encode, peptide, kisspeptin, bin...\n",
       "2         [biophysical, basis, thermodynamics_kinetic, m...\n",
       "3         [obesity, adverse_pregnancyoutcome, great, hea...\n",
       "4         [local, potato, advisory, express, interest, m...\n",
       "                                ...                        \n",
       "690809    [pathophysiology, schizophrenia, advance, thed...\n",
       "690810    [alzheimer, ad, amyotrophic_lateral_sclerosis_...\n",
       "690811    [highest, mortality, acute, care, encounter, r...\n",
       "690812    [paradigm, kidney, largely, stagnant, decade, ...\n",
       "690813    [division, intramural, population, health, dip...\n",
       "Name: final_frqwds_removed, Length: 690814, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: run the following code to generate id2word and corpus\n",
    "#id2word, corpus = createVars(docs)\n",
    "\n",
    "#TODO: RENAME the file, run the code to save the output\n",
    "#pickle.dump([corpus, id2word, docs], open('../../data/dspg21RnD/coherence_vars_XXXXXX.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Coherence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Read in your coherence data (change the name of the file)\n",
    "f = open('../../data/dspg21RnD/coherence_vars.sav', 'rb')\n",
    "[corpus, id2word, docs] = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [multiprotein, y_secretase, proteolytically_cl...\n",
       "1         [kissl, gene, encode, peptide, kisspeptin, bin...\n",
       "2         [biophysical, basis, thermodynamics_kinetic, m...\n",
       "3         [obesity, adverse_pregnancyoutcome, great, hea...\n",
       "4         [local, potato, advisory, express, interest, m...\n",
       "                                ...                        \n",
       "690809    [pathophysiology, schizophrenia, advance, thed...\n",
       "690810    [alzheimer, ad, amyotrophic_lateral_sclerosis_...\n",
       "690811    [highest, mortality, acute, care, encounter, r...\n",
       "690812    [paradigm, kidney, largely, stagnant, decade, ...\n",
       "690813    [division, intramural, population, health, dip...\n",
       "Name: final_frqwds_removed, Length: 690814, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for abstract in docs:\n",
    "    text.append(\" \".join(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiprotein y_secretase proteolytically_cleave intramembrane region amyloid_precursorprotein_app turn plaque alzheimer ad patient catalyticcomponent y_secretase intramembrane_aspartyl_protease iap presenilin mutation inpresenilin directly link familial onset ad member iap family signalpeptide peptidase_spp proteolyze remnant peptide beencleave peptidase biochemistry individual spp onlybegin elucidate homologue kingdom life presenilin spp exhibitsignificant sequence similarity strongly share structural catalytic feature amolecular tractable spp likely drug presenilin y_secretase express solve crystal anextremophilic bacterial spp ortholog transition analog inhibitor substratemimic drug candidate screen silico intramembraneprotease insight biochemistry intramembrane_proteolysis enable ad drug screen'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function slightly modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)\n",
    "            \n",
    "# Function to format topics as a \"list of list of strings\".\n",
    "# Needed for topic coherence function in Gensim\n",
    "\n",
    "# function modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def list_topics(model, vectorizer, top_n=10):\n",
    "\n",
    "    #input. top_n: how many words to list per topic.  If -1, then list all words.\n",
    "       \n",
    "    topic_words = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "            \n",
    "        if top_n == -1:   \n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1]])\n",
    "        else:\n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document-term matrix - TFIDF \n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=3)\n",
    "tf_idf = tfidf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "\n",
    "def nmf_metrics(doc_term_matrix, n_topics, vectorizer, corpus, id2word, docs, rand_start):\n",
    "    \"\"\"\n",
    "    Compute c_v topic coherence for various number of topics\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tf_idf\n",
    "    n_topics : list of number of topics\n",
    "    Returns:\n",
    "    -------\n",
    "    coherence_values : c_v topic coherence values corresponding to the NMF model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    coherence_values = []\n",
    "    \n",
    "    i = rand_start\n",
    "    for num_topics in n_topics:\n",
    "\n",
    "        # create model\n",
    "        t1 = time.time()\n",
    "        nmf_model = NMF(n_components=num_topics, random_state = i)\n",
    "        nmf_model.fit_transform(doc_term_matrix)\n",
    "        t2 = time.time()\n",
    "        print(f\"  Model time: {t2-t1}\")\n",
    "        \n",
    "        # create list of topics\n",
    "        topics = list_topics(nmf_model, vectorizer, top_n=10)\n",
    "        \n",
    "        # calculate coherence\n",
    "        t1 = time.time()\n",
    "        \n",
    "        #TODO:manually adjust number of processes\n",
    "        cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=id2word, texts=docs, \n",
    "                            coherence='c_v', #model for calculating coherence score\n",
    "                            processes=12 #for smaller corpus, pronesses= number of cores - 1 \n",
    "                           ) #window_size=500 ) \n",
    "        coherence_values.append(cm.get_coherence())\n",
    "        t2 = time.time()\n",
    "        print(f\"  Coherence time: {t2-t1}\")\n",
    "        \n",
    "        # output completion message\n",
    "        i = i+1\n",
    "        print('Number of topics =', num_topics, \"complete.\")\n",
    "\n",
    "    return coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    }
   ],
   "source": [
    "# code copied from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "# minor alterations made\n",
    "\n",
    "n_topics = list(range(5,101,5)) #from 5 to 100, increment by 5\n",
    "num_runs = 2\n",
    "\n",
    "col_names = [f\"iteration {i}\" for i in range(num_runs)]\n",
    "nmf_c = pd.DataFrame(index = n_topics, columns = col_names)\n",
    "\n",
    "for i in range(num_runs):\n",
    "    \n",
    "    print(f\"Iteration {i}\")\n",
    "    \n",
    "    # run models\n",
    "    c = nmf_metrics(doc_term_matrix=tf_idf, n_topics=n_topics, vectorizer=tfidf_vectorizer, \n",
    "                         corpus=corpus, id2word=id2word, docs=docs, rand_start = (i)*len(n_topics))\n",
    "    \n",
    "    # save results\n",
    "    nmf_c[f\"iteration {i}\"] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "\n",
    "nmf_c.to_pickle(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/nmf_bert.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_docs = docs\n",
    "len(lim_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needed for LDA, NMF (all from Scikit-Learn) is one string per document (not a list of strings)\n",
    "\n",
    "text = []\n",
    "\n",
    "for token_list in lim_docs:\n",
    "    text.append(\" \".join(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF document-term matrix for the AI corpus \n",
    "\n",
    "# TRY DIFFERENT PARAMETERS IN THE TF-IDF DOC-TERM MATRIX SET-UP\n",
    "nmf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=3, lowercase=True) #, max_features=int(len(lim_docs)/2))\n",
    "\n",
    "# by default TfidfVectorizer has l2 normalization for rows: \n",
    "# from Scikit Learn documentation: Each output row will have unit norm, either: * ‘l2’: Sum of squares of vector \n",
    "# elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied.\n",
    "\n",
    "nmf_tf_idf = nmf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_terms = nmf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_terms[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling with NMF\n",
    "\n",
    "nmf_model = NMF(n_components=30, random_state=1)  # TRY DIFFERENT NUMBERS OF TOPICS\n",
    "W = nmf_model.fit_transform(nmf_tf_idf)\n",
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topics(nmf_model, nmf_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot and cold figure "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-crystal_bert]",
   "language": "python",
   "name": "conda-env-.conda-crystal_bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
