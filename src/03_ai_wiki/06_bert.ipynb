{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Sentence Bert Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Crystal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "#nltk.download() #input: punkt\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "#embeddings\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#embedder = SentenceTransformer(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702f9f778b1c4429af5809823736ac44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_corpus_embeddings(dir):\n",
    "    with open(dir) as f:\n",
    "        ai_text = f.read()\n",
    "    ai_corpus = tokenize.sent_tokenize(ai_text) #sentence tokenization\n",
    "    ai_embeddings = embedder.encode(ai_corpus, show_progress_bar=True) # embeddings\n",
    "    return ai_embeddings\n",
    "\n",
    "def get_ai_corpus(dir):\n",
    "    with open(dir) as f:\n",
    "        ai_text = f.read()\n",
    "    ai_corpus = tokenize.sent_tokenize(ai_text) #sentence tokenization\n",
    "    return ai_corpus\n",
    "\n",
    "ai_corpus = get_ai_corpus(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/ai_wiki_text.txt\")\n",
    "ai_embeddings = get_corpus_embeddings(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/ai_wiki_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NoneNoneArtificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k: number of similar sentences from AI corpus\n",
    "# abstract: abstract from FEDERAL RePORTER\n",
    "# print_result: if TRUE, print out the similar sentenses from AI corpus to each sentence in the abstract\n",
    "def get_score(k, abstract, print_result = False):\n",
    "    queries = tokenize.sent_tokenize(abstract) \n",
    "\n",
    "    # init a result list for scores\n",
    "    result = []\n",
    "    \n",
    "    # Find the closest k sentences of the AI corpus for each query sentence (ML) based on cosine similarity\n",
    "    top_k = k\n",
    "    \n",
    "    for query in queries: #compare each sentence in the abstract to the ai corpus\n",
    "        query_embedding = embedder.encode(query, show_progress_bar=False) \n",
    "        \n",
    "        # We use cosine-similarity and torch.topk to find the highest k scores\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, ai_embeddings)[0]\n",
    "        \n",
    "        top_results = torch.topk(cos_scores, k=top_k)   #get the top k scores\n",
    "        result.append(top_results.values.tolist()) #unlist the top result list\n",
    "        if print_result:\n",
    "            print(\"\\n\\n======================\\n\\n\")\n",
    "            print(\"Query:\", query)\n",
    "            print(\"Results:\", top_results)\n",
    "            print(\"\\nTop k=5 most similar sentences in corpus:\")\n",
    "            for score, idx in zip(top_results[0], top_results[1]):\n",
    "                print(ai_corpus[idx], \"(Score: {:.4f})\".format(score))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1. Abstract on \"FROM ALPHAGO TO POWER SYSTEM ARTIFICIAL INTELLIGENCE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = pd.read_pickle(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/smaller-final-dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original index</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>FY</th>\n",
       "      <th>ORG_COUNT</th>\n",
       "      <th>PI_COUNT</th>\n",
       "      <th>nchar</th>\n",
       "      <th>final_frqwds_removed</th>\n",
       "      <th>PROJECT_TERMS</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>...</th>\n",
       "      <th>ORGANIZATION_CITY</th>\n",
       "      <th>ORGANIZATION_STATE</th>\n",
       "      <th>ORGANIZATION_ZIP</th>\n",
       "      <th>ORGANIZATION_COUNTRY</th>\n",
       "      <th>BUDGET_START_DATE</th>\n",
       "      <th>BUDGET_END_DATE</th>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <th>FY.y</th>\n",
       "      <th>FY_TOTAL_COST</th>\n",
       "      <th>FY_TOTAL_COST_SUB_PROJECTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>531810</th>\n",
       "      <td>1062091</td>\n",
       "      <td>1088974</td>\n",
       "      <td>The game of Go is an ancient board game which ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2379</td>\n",
       "      <td>[game, ancient, board, game, consider, far, bo...</td>\n",
       "      <td>Address; Algorithms; Area; Artificial Intelli...</td>\n",
       "      <td>FROM ALPHAGO TO POWER SYSTEM ARTIFICIAL INTELL...</td>\n",
       "      <td>...</td>\n",
       "      <td>KNOXVILLE</td>\n",
       "      <td>TN</td>\n",
       "      <td>37996-0003</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.041</td>\n",
       "      <td>2018</td>\n",
       "      <td>330000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        original index PROJECT_ID  \\\n",
       "531810         1062091    1088974   \n",
       "\n",
       "                                                 ABSTRACT    FY  ORG_COUNT  \\\n",
       "531810  The game of Go is an ancient board game which ...  2018          1   \n",
       "\n",
       "        PI_COUNT  nchar                               final_frqwds_removed  \\\n",
       "531810         1   2379  [game, ancient, board, game, consider, far, bo...   \n",
       "\n",
       "                                            PROJECT_TERMS  \\\n",
       "531810   Address; Algorithms; Area; Artificial Intelli...   \n",
       "\n",
       "                                            PROJECT_TITLE  ...  \\\n",
       "531810  FROM ALPHAGO TO POWER SYSTEM ARTIFICIAL INTELL...  ...   \n",
       "\n",
       "       ORGANIZATION_CITY ORGANIZATION_STATE ORGANIZATION_ZIP  \\\n",
       "531810         KNOXVILLE                 TN       37996-0003   \n",
       "\n",
       "       ORGANIZATION_COUNTRY BUDGET_START_DATE BUDGET_END_DATE CFDA_CODE  FY.y  \\\n",
       "531810        UNITED STATES               NaN             NaN    47.041  2018   \n",
       "\n",
       "       FY_TOTAL_COST FY_TOTAL_COST_SUB_PROJECTS  \n",
       "531810      330000.0                        NaN  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[abstracts[\"PROJECT_TITLE\"] ==\"FROM ALPHAGO TO POWER SYSTEM ARTIFICIAL INTELLIGENCE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed in ML Wiki article as a query.\n",
    "For each sentence in the ML Wiki article, we identify the top 5 most similar/closest sentences from AI Wiki article based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project SummaryGeneration of the eye, more so than many organs, requires precise control of its shape for optimal function.Obtaining knowledge of how the eye and lens is constructed during embryonic development is thereforeimportant to help describe the nature of ocular abnormalities that lead to major structural defects or moresubtle changes that alter vision. An example of a morphogenetic event required for the generation of organs isepithelial invagination. This process drives the inward bending of epithelia of several early organ systemsincluding that of the lens placode during early ocular development. Although several mechanisms have beenproposed to drive this process, such as apical constriction or local placodal growth, none have been foundsufficient to account for epithelial bending. We have recently observed that placodal cells change shape,move, and generate cytoskeletal structures in a planar polarized manner that produces a net flow of cellstoward the central placode. One of the hallmarks of planar-polarized cell movements such as these is theformation and resolution of cellular rosettes, an organized process of cell rearrangement that requires spatialrestriction of junctional proteins that contract and shorten junctions and proteins that lengthen and stabilizecellular junctions. We have identified planar-polarized localization of proteins responsible for junctionalcontraction (Shroom3 and p120-catenin) and stabilization (Par3 and cdc42). These results led us to ourcentral hypothesis that invagination is driven by a combination of epithelial cell movements and anisotropic cellshape changes organized by radial planar polarized protein localization, junction contraction, and junctionelongation. We will test this central hypothesis with three aims utilizing live-fluorescent microscopy ofgenetically altered mouse embryos. In aim 1 we will characterize the role of anisotropic junctional contractionand analyze the consequences of combined deficiency of Shroom3 and p120 catenin. The goal of aim 2 is tocharacterize the role of Par3 in junction elongation during rosette resolution and invagination. Aim 3 willinvestigate whether anisotropic cell geometry and movement results from the mutual antagonism betweenproteins that induce junctional contraction and junctional elongation. Once completed, the experiments in thisproposal will define the cell behaviors that drive the mechanisms of lens placode invagination.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_related_p = abstracts[\"ABSTRACT\"][531850]\n",
    "ai_related_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Project SummaryGeneration of the eye, more so than many organs, requires precise control of its shape for optimal function.Obtaining knowledge of how the eye and lens is constructed during embryonic development is thereforeimportant to help describe the nature of ocular abnormalities that lead to major structural defects or moresubtle changes that alter vision.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.5004, 0.4038, 0.3924, 0.3729, 0.3711]),\n",
      "indices=tensor([164, 129, 120, 365, 138]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "Computer vision is the ability to analyze visual input. (Score: 0.5004)\n",
      "Knowledge representation  and knowledge engineering  are central to classical AI research. (Score: 0.4038)\n",
      "The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. (Score: 0.3924)\n",
      "Regulation of AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem. (Score: 0.3729)\n",
      "They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make choices that maximize the utility (or \"value\") of available choices. (Score: 0.3711)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: An example of a morphogenetic event required for the generation of organs isepithelial invagination.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2486, 0.2235, 0.2153, 0.2105, 0.2090]),\n",
      "indices=tensor([ 80, 359, 148, 272,  72]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "[a] Alternatively, an evolutionary system can induce goals by using a \"fitness function\" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food. (Score: 0.2486)\n",
      "I think there is potentially a dangerous outcome there.\" (Score: 0.2235)\n",
      "Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. (Score: 0.2153)\n",
      "This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth. (Score: 0.2105)\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.2090)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: This process drives the inward bending of epithelia of several early organ systemsincluding that of the lens placode during early ocular development.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2891, 0.2469, 0.2448, 0.2446, 0.2386]),\n",
      "indices=tensor([164,  72, 230, 357, 304]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "Computer vision is the ability to analyze visual input. (Score: 0.2891)\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.2469)\n",
      "Interest in neural networks and \"connectionism\" was revived by David Rumelhart and others in the middle of the 1980s. (Score: 0.2448)\n",
      "The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. (Score: 0.2446)\n",
      "Superintelligence may also refer to the form or degree of intelligence possessed by such an agent. (Score: 0.2386)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Although several mechanisms have beenproposed to drive this process, such as apical constriction or local placodal growth, none have been foundsufficient to account for epithelial bending.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2975, 0.2810, 0.2725, 0.2557, 0.2397]),\n",
      "indices=tensor([269, 170,  89,  96, 148]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "He argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" (Score: 0.2975)\n",
      "Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object. (Score: 0.2810)\n",
      "In practice, it is seldom possible to consider every possibility, because of the phenomenon of \"combinatorial explosion\", where the time needed to solve a problem grows exponentially. (Score: 0.2725)\n",
      "These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. (Score: 0.2557)\n",
      "Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. (Score: 0.2397)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: We have recently observed that placodal cells change shape,move, and generate cytoskeletal structures in a planar polarized manner that produces a net flow of cellstoward the central placode.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2783, 0.2474, 0.2442, 0.2372, 0.2253]),\n",
      "indices=tensor([ 72, 161,  31, 181, 312]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.2783)\n",
      "By 2019, transformer-based deep learning architectures could generate coherent text. (Score: 0.2474)\n",
      "Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. (Score: 0.2442)\n",
      "Historically, projects such as the Cyc knowledge base (1984–) and the massive Japanese Fifth Generation Computer Systems initiative (1982–1992) attempted to cover the breadth of human cognition. (Score: 0.2372)\n",
      "Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. (Score: 0.2253)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: One of the hallmarks of planar-polarized cell movements such as these is theformation and resolution of cellular rosettes, an organized process of cell rearrangement that requires spatialrestriction of junctional proteins that contract and shorten junctions and proteins that lengthen and stabilizecellular junctions.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3484, 0.3033, 0.2814, 0.2781, 0.2775]),\n",
      "indices=tensor([169,  72, 170, 156, 145]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "Motion planning is the process of breaking down a movement task into \"primitives\" such as individual joint movements. (Score: 0.3484)\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.3033)\n",
      "Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object. (Score: 0.2814)\n",
      "Many current approaches use word co-occurrence frequencies to construct syntactic representations of text. (Score: 0.2781)\n",
      "[e] \n",
      "Unsupervised learning is the ability to find patterns in a stream of input, without requiring a human to label the inputs first. (Score: 0.2775)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: We have identified planar-polarized localization of proteins responsible for junctionalcontraction (Shroom3 and p120-catenin) and stabilization (Par3 and cdc42).\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3695, 0.2502, 0.2178, 0.2154, 0.2136]),\n",
      "indices=tensor([ 72, 161, 335,  31, 365]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.3695)\n",
      "By 2019, transformer-based deep learning architectures could generate coherent text. (Score: 0.2502)\n",
      "In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies. (Score: 0.2178)\n",
      "Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. (Score: 0.2154)\n",
      "Regulation of AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem. (Score: 0.2136)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: These results led us to ourcentral hypothesis that invagination is driven by a combination of epithelial cell movements and anisotropic cellshape changes organized by radial planar polarized protein localization, junction contraction, and junctionelongation.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2555, 0.2382, 0.2256, 0.2229, 0.2106]),\n",
      "indices=tensor([119, 148, 313,  72, 117]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "The functional model refers to the correlating data to its computed counterpart. (Score: 0.2555)\n",
      "Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. (Score: 0.2382)\n",
      "This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger. (Score: 0.2256)\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.2229)\n",
      "This gives rise to two classes of models: structuralist and functionalist. (Score: 0.2106)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: We will test this central hypothesis with three aims utilizing live-fluorescent microscopy ofgenetically altered mouse embryos.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3990, 0.3811, 0.3556, 0.3424, 0.3423]),\n",
      "indices=tensor([ 72,  31, 189, 302, 195]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.3990)\n",
      "Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. (Score: 0.3811)\n",
      "Finally, a few \"emergent\" approaches look to simulating human intelligence extremely closely, and believe that anthropomorphic features like an artificial brain or simulated child development may someday reach a critical point where general intelligence emerges. (Score: 0.3556)\n",
      "Are there limits to how intelligent machines—or human-machine hybrids—can be? (Score: 0.3424)\n",
      "[f] A few of the most long-standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurobiology? (Score: 0.3423)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: In aim 1 we will characterize the role of anisotropic junctional contractionand analyze the consequences of combined deficiency of Shroom3 and p120 catenin.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3161, 0.2777, 0.2543, 0.2396, 0.2381]),\n",
      "indices=tensor([ 72, 117, 119,  96, 171]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.3161)\n",
      "This gives rise to two classes of models: structuralist and functionalist. (Score: 0.2777)\n",
      "The functional model refers to the correlating data to its computed counterpart. (Score: 0.2543)\n",
      "These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. (Score: 0.2396)\n",
      "Moravec's paradox generalizes that low-level sensorimotor skills that humans take for granted are, counterintuitively, difficult to program into a robot; the paradox is named after Hans Moravec, who stated in 1988 that \"it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility\". (Score: 0.2381)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: The goal of aim 2 is tocharacterize the role of Par3 in junction elongation during rosette resolution and invagination.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2408, 0.2359, 0.2327, 0.2233, 0.2178]),\n",
      "indices=tensor([357, 148,  72, 345, 119]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. (Score: 0.2408)\n",
      "Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. (Score: 0.2359)\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.2327)\n",
      "If this AI's goals do not fully reflect humanity's—one example is an AI told to compute as many digits of pi as possible—it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. (Score: 0.2233)\n",
      "The functional model refers to the correlating data to its computed counterpart. (Score: 0.2178)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Aim 3 willinvestigate whether anisotropic cell geometry and movement results from the mutual antagonism betweenproteins that induce junctional contraction and junctional elongation.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3262, 0.3232, 0.3089, 0.2997, 0.2932]),\n",
      "indices=tensor([ 96,  72, 313,  95,  80]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. (Score: 0.3262)\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.3232)\n",
      "This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger. (Score: 0.3089)\n",
      "A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the artificial neural network approach uses artificial \"neurons\" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to \"reinforce\" connections that seemed to be useful. (Score: 0.2997)\n",
      "[a] Alternatively, an evolutionary system can induce goals by using a \"fitness function\" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food. (Score: 0.2932)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Once completed, the experiments in thisproposal will define the cell behaviors that drive the mechanisms of lens placode invagination.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3006, 0.2745, 0.2611, 0.2563, 0.2518]),\n",
      "indices=tensor([ 72, 290, 164,  96, 148]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "DeepMind's AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein. (Score: 0.3006)\n",
      ")[k] Everyone knows subjective experience exists, because they do it every day (e.g., all sighted people know what red looks like). (Score: 0.2745)\n",
      "Computer vision is the ability to analyze visual input. (Score: 0.2611)\n",
      "These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. (Score: 0.2563)\n",
      "Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. (Score: 0.2518)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.5003809332847595,\n",
       "  0.40379366278648376,\n",
       "  0.3924315571784973,\n",
       "  0.37294694781303406,\n",
       "  0.3710911273956299],\n",
       " [0.24858857691287994,\n",
       "  0.22353576123714447,\n",
       "  0.2152961790561676,\n",
       "  0.21045251190662384,\n",
       "  0.2089819461107254],\n",
       " [0.28912150859832764,\n",
       "  0.24686625599861145,\n",
       "  0.2447730451822281,\n",
       "  0.24456101655960083,\n",
       "  0.2386426478624344],\n",
       " [0.29747214913368225,\n",
       "  0.28097668290138245,\n",
       "  0.2725309431552887,\n",
       "  0.25569066405296326,\n",
       "  0.2396867573261261],\n",
       " [0.278329074382782,\n",
       "  0.24736516177654266,\n",
       "  0.24418598413467407,\n",
       "  0.2372487485408783,\n",
       "  0.2252652943134308],\n",
       " [0.3483860492706299,\n",
       "  0.30326569080352783,\n",
       "  0.2813758850097656,\n",
       "  0.27807050943374634,\n",
       "  0.2775396704673767],\n",
       " [0.3694527745246887,\n",
       "  0.25018545985221863,\n",
       "  0.2177867740392685,\n",
       "  0.21539166569709778,\n",
       "  0.21357858180999756],\n",
       " [0.25550955533981323,\n",
       "  0.23818449676036835,\n",
       "  0.22564257681369781,\n",
       "  0.22294126451015472,\n",
       "  0.21055921912193298],\n",
       " [0.3990171253681183,\n",
       "  0.3810650408267975,\n",
       "  0.35563212633132935,\n",
       "  0.3424082100391388,\n",
       "  0.3423215448856354],\n",
       " [0.3160785436630249,\n",
       "  0.2777494490146637,\n",
       "  0.2543160915374756,\n",
       "  0.23961623013019562,\n",
       "  0.23811611533164978],\n",
       " [0.24081891775131226,\n",
       "  0.23593740165233612,\n",
       "  0.23270705342292786,\n",
       "  0.22332881391048431,\n",
       "  0.2178398221731186],\n",
       " [0.3261649012565613,\n",
       "  0.3232416808605194,\n",
       "  0.30892983078956604,\n",
       "  0.29973727464675903,\n",
       "  0.2932327687740326],\n",
       " [0.3005898594856262,\n",
       "  0.2745436131954193,\n",
       "  0.2610831558704376,\n",
       "  0.2563379406929016,\n",
       "  0.251829594373703]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(5, ai_related_p, print_result = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Abstract on \"STRUCTURE OF SIGNAL PEPTIDE PEPTIDASE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The multiprotein complex y-secretase proteolytically cleaves the intramembrane region of amyloid precursorprotein (APP), which in turn forms the plaques found in Alzheimer's disease (AD) patients. The catalyticcomponent of Y-secretase is the intramembrane aspartyl protease (IAP) called presenilin. Mutations inpresenilin are directly linked to familial early-onset AD. Another known member of the IAP family is signalpeptide peptidase (SPP), which functions to further proteolyze remnant signal peptides after they have beencleaved by signal peptidase. Knowledge of the biochemistry and function of individual SPPs are onlybeginning to be elucidated, and homologues are found in all kingdoms of life. Presenilin and SPP exhibitsignificant sequence similarity, strongly suggesting they share structural and catalytic features. Thus, amolecular understanding of the more tractable SPP will likely impact drug design for presenilin and y-secretase. The goal of this proposal is to express, characterize, and solve the crystal structure of anextremophilic bacterial SPP ortholog by itself, with a transition-state analog inhibitor and with a substratemimic. In addition, drug candidates will be screened in silico. This first structure of an intramembraneprotease will provide critical insight into the biochemistry of intramembrane proteolysis and enable structure-based AD drug development and screening.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_text = abstracts[\"ABSTRACT\"][0]#get the text\n",
    "abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The multiprotein complex y-secretase proteolytically cleaves the intramembrane region of amyloid precursorprotein (APP), which in turn forms the plaques found in Alzheimer's disease (AD) patients.\", 'The catalyticcomponent of Y-secretase is the intramembrane aspartyl protease (IAP) called presenilin.', 'Mutations inpresenilin are directly linked to familial early-onset AD.', 'Another known member of the IAP family is signalpeptide peptidase (SPP), which functions to further proteolyze remnant signal peptides after they have beencleaved by signal peptidase.', 'Knowledge of the biochemistry and function of individual SPPs are onlybeginning to be elucidated, and homologues are found in all kingdoms of life.', 'Presenilin and SPP exhibitsignificant sequence similarity, strongly suggesting they share structural and catalytic features.', 'Thus, amolecular understanding of the more tractable SPP will likely impact drug design for presenilin and y-secretase.', 'The goal of this proposal is to express, characterize, and solve the crystal structure of anextremophilic bacterial SPP ortholog by itself, with a transition-state analog inhibitor and with a substratemimic.', 'In addition, drug candidates will be screened in silico.', 'This first structure of an intramembraneprotease will provide critical insight into the biochemistry of intramembrane proteolysis and enable structure-based AD drug development and screening.']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "get_score(5, abstract_text, print_result = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-4b74e7445b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-8db3f58d9034>\u001b[0m in \u001b[0;36mget_score\u001b[0;34m(queries, print_result)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#compare each sentence in the abstract to the ai corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# We use cosine-similarity and torch.topk to find the highest k scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'token_embeddings'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         )\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n\u001b[1;32m    567\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         )\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    174\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/crystal_bert/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2344\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m         )\n\u001b[0;32m-> 2346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_score(queries, print_result=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_Basilica = \"Pope Julius' scheme for the grandest building in Christendom was the subject of a competition for which a number of entries remain intact in the Uffizi Gallery, Florence. It was the design of Donato Bramante that was selected, and for which the foundation stone was laid in 1506. This plan was in the form of an enormous Greek Cross with a dome inspired by that of the huge circular Roman temple, the Pantheon.[7] The main difference between Bramante's design and that of the Pantheon is that where the dome of the Pantheon is supported by a continuous wall, that of the new basilica was to be supported only on four large piers. This feature was maintained in the ultimate design. Bramante's dome was to be surmounted by a lantern with its own small dome but otherwise very similar in form to the Early Renaissance lantern of Florence Cathedral designed for Brunelleschi's dome by Michelozzo. Bramante had envisioned that the central dome would be surrounded by four lower domes at the diagonal axes. The equal chancel, nave and transept arms were each to be of two bays ending in an apse. At each corner of the building was to stand a tower, so that the overall plan was square, with the apses projecting at the cardinal points. Each apse had two large radial buttresses, which squared off its semi-circular shape.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Pope Julius' scheme for the grandest building in Christendom was the subject of a competition for which a number of entries remain intact in the Uffizi Gallery, Florence.\", 'It was the design of Donato Bramante that was selected, and for which the foundation stone was laid in 1506.', 'This plan was in the form of an enormous Greek Cross with a dome inspired by that of the huge circular Roman temple, the Pantheon.', \"[7] The main difference between Bramante's design and that of the Pantheon is that where the dome of the Pantheon is supported by a continuous wall, that of the new basilica was to be supported only on four large piers.\", 'This feature was maintained in the ultimate design.', \"Bramante's dome was to be surmounted by a lantern with its own small dome but otherwise very similar in form to the Early Renaissance lantern of Florence Cathedral designed for Brunelleschi's dome by Michelozzo.\", 'Bramante had envisioned that the central dome would be surrounded by four lower domes at the diagonal axes.', 'The equal chancel, nave and transept arms were each to be of two bays ending in an apse.', 'At each corner of the building was to stand a tower, so that the overall plan was square, with the apses projecting at the cardinal points.', 'Each apse had two large radial buttresses, which squared off its semi-circular shape.']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "queries = tokenize.sent_tokenize(text_Basilica)\n",
    "print(queries[0:10])\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Pope Julius' scheme for the grandest building in Christendom was the subject of a competition for which a number of entries remain intact in the Uffizi Gallery, Florence.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2273, 0.2086, 0.2030, 0.2018]),\n",
      "indices=tensor([ 53, 369,  22, 211]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! (Score: 0.2273)\n",
      "Thought-capable artificial beings appeared as storytelling devices since antiquity, \n",
      "and have been a persistent theme in science fiction. (Score: 0.2086)\n",
      "These issues have been explored by myth, fiction and philosophy since antiquity. (Score: 0.2030)\n",
      "This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s. (Score: 0.2018)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: It was the design of Donato Bramante that was selected, and for which the foundation stone was laid in 1506.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.1747, 0.1650, 0.1568, 0.1529]),\n",
      "indices=tensor([370, 206,  28, 356]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. (Score: 0.1747)\n",
      "During the 1960s, symbolic approaches had achieved great success at simulating high-level \"thinking\" in small demonstration programs. (Score: 0.1650)\n",
      "The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. (Score: 0.1568)\n",
      "In January 2015, Musk donated $10 million to the Future of Life Institute to fund research on understanding AI decision making. (Score: 0.1529)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: This plan was in the form of an enormous Greek Cross with a dome inspired by that of the huge circular Roman temple, the Pantheon.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2661, 0.2561, 0.2545, 0.2447]),\n",
      "indices=tensor([211, 371,  22,  26]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s. (Score: 0.2661)\n",
      "This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). (Score: 0.2561)\n",
      "These issues have been explored by myth, fiction and philosophy since antiquity. (Score: 0.2545)\n",
      "Thought-capable artificial beings appeared as storytelling devices in antiquity,  and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. (Score: 0.2447)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: [7] The main difference between Bramante's design and that of the Pantheon is that where the dome of the Pantheon is supported by a continuous wall, that of the new basilica was to be supported only on four large piers.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2857, 0.2283, 0.1860, 0.1650]),\n",
      "indices=tensor([211, 357,  22, 114]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s. (Score: 0.2857)\n",
      "The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. (Score: 0.2283)\n",
      "These issues have been explored by myth, fiction and philosophy since antiquity. (Score: 0.1860)\n",
      "The cognitive capabilities of current architectures are very limited, using only a simplified version of what intelligence is really capable of. (Score: 0.1650)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: This feature was maintained in the ultimate design.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3845, 0.3560, 0.3249, 0.3157]),\n",
      "indices=tensor([225, 122, 228, 369]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "This includes embodied, situated, behavior-based, and nouvelle AI. (Score: 0.3845)\n",
      "These consist of particular traits or capabilities that researchers expect an intelligent system to display. (Score: 0.3560)\n",
      "This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence. (Score: 0.3249)\n",
      "Thought-capable artificial beings appeared as storytelling devices since antiquity, \n",
      "and have been a persistent theme in science fiction. (Score: 0.3157)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Bramante's dome was to be surmounted by a lantern with its own small dome but otherwise very similar in form to the Early Renaissance lantern of Florence Cathedral designed for Brunelleschi's dome by Michelozzo.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3039, 0.1499, 0.1390, 0.1377]),\n",
      "indices=tensor([211,  26, 369, 357]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s. (Score: 0.3039)\n",
      "Thought-capable artificial beings appeared as storytelling devices in antiquity,  and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. (Score: 0.1499)\n",
      "Thought-capable artificial beings appeared as storytelling devices since antiquity, \n",
      "and have been a persistent theme in science fiction. (Score: 0.1390)\n",
      "The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. (Score: 0.1377)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Bramante had envisioned that the central dome would be surrounded by four lower domes at the diagonal axes.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3363, 0.1833, 0.1792, 0.1731]),\n",
      "indices=tensor([211,  95,  96,  12]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s. (Score: 0.3363)\n",
      "A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the artificial neural network approach uses artificial \"neurons\" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to \"reinforce\" connections that seemed to be useful. (Score: 0.1833)\n",
      "These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. (Score: 0.1792)\n",
      "These sub-fields are based on technical considerations, such as particular goals (e.g. (Score: 0.1731)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: The equal chancel, nave and transept arms were each to be of two bays ending in an apse.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.1978, 0.1703, 0.1625, 0.1611]),\n",
      "indices=tensor([  1, 135, 211, 296]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "The distinction between the former and the latter categories is often revealed by the acronym chosen. (Score: 0.1978)\n",
      "The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge  by acting as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). (Score: 0.1703)\n",
      "This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s. (Score: 0.1625)\n",
      "\"[l] Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the \"mind\" might be. (Score: 0.1611)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: At each corner of the building was to stand a tower, so that the overall plan was square, with the apses projecting at the cardinal points.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.3349, 0.2684, 0.2607, 0.2133]),\n",
      "indices=tensor([211,  91, 165, 138]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s. (Score: 0.3349)\n",
      "For example, when viewing a map and looking for the shortest driving route from Denver to New York in the East, one can in most cases skip looking at any path through San Francisco or other areas far to the West; thus, an AI wielding a pathfinding algorithm like A* can avoid the combinatorial explosion that would ensue if every possible route had to be ponderously considered. (Score: 0.2684)\n",
      "Such input is usually ambiguous; a giant, fifty-meter-tall pedestrian far away may produce the same pixels as a nearby normal-sized pedestrian, requiring the AI to judge the relative likelihood and reasonableness of different interpretations, for example by using its \"object model\" to assess that fifty-meter pedestrians do not exist. (Score: 0.2607)\n",
      "They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make choices that maximize the utility (or \"value\") of available choices. (Score: 0.2133)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Each apse had two large radial buttresses, which squared off its semi-circular shape.\n",
      "Results: torch.return_types.topk(\n",
      "values=tensor([0.2252, 0.1974, 0.1924, 0.1786]),\n",
      "indices=tensor([211, 165, 117, 167]))\n",
      "\n",
      "Top k=5 most similar sentences in corpus:\n",
      "This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s. (Score: 0.2252)\n",
      "Such input is usually ambiguous; a giant, fifty-meter-tall pedestrian far away may produce the same pixels as a nearby normal-sized pedestrian, requiring the AI to judge the relative likelihood and reasonableness of different interpretations, for example by using its \"object model\" to assess that fifty-meter pedestrians do not exist. (Score: 0.1974)\n",
      "This gives rise to two classes of models: structuralist and functionalist. (Score: 0.1924)\n",
      "Advanced robotic arms and other industrial robots, widely used in modern factories, can learn from experience how to move efficiently despite the presence of friction and gear slippage. (Score: 0.1786)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.22725766897201538,\n",
       "  0.2086402177810669,\n",
       "  0.20304988324642181,\n",
       "  0.20184537768363953],\n",
       " [0.17468996345996857,\n",
       "  0.16498778760433197,\n",
       "  0.15678071975708008,\n",
       "  0.15291136503219604],\n",
       " [0.2660845220088959,\n",
       "  0.25608712434768677,\n",
       "  0.2545034885406494,\n",
       "  0.24469813704490662],\n",
       " [0.2857217490673065,\n",
       "  0.22828024625778198,\n",
       "  0.18599683046340942,\n",
       "  0.1650390774011612],\n",
       " [0.38453492522239685,\n",
       "  0.35602161288261414,\n",
       "  0.3248547911643982,\n",
       "  0.31567105650901794],\n",
       " [0.30394724011421204,\n",
       "  0.14987064898014069,\n",
       "  0.1389949917793274,\n",
       "  0.13767722249031067],\n",
       " [0.33625566959381104,\n",
       "  0.18328112363815308,\n",
       "  0.17916186153888702,\n",
       "  0.1731284260749817],\n",
       " [0.19776922464370728,\n",
       "  0.17027877271175385,\n",
       "  0.1624986231327057,\n",
       "  0.16105715930461884],\n",
       " [0.3348667323589325,\n",
       "  0.26839780807495117,\n",
       "  0.2606987953186035,\n",
       "  0.21333608031272888],\n",
       " [0.2252057045698166,\n",
       "  0.19740983843803406,\n",
       "  0.19238469004631042,\n",
       "  0.17855817079544067]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(4, queries, print_result = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "# Find the closest k sentences of the AI corpus for each query sentence (ML) based on cosine similarity\n",
    "top_k = min(k, len(ai_corpus))\n",
    "\n",
    "query = queries[0]\n",
    "\n",
    "query_embedding = embedder.encode(query, show_progress_bar=False) \n",
    "\n",
    "# We use cosine-similarity and torch.topk to find the highest k scores\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, ai_embeddings)[0]\n",
    "\n",
    "top_results = torch.topk(cos_scores, k=top_k)   #get the top k scores\n",
    "result.append(top_results.values.tolist()) #unlist the top result list\n",
    "\n",
    "print(\"\\n\\n======================\\n\\n\")\n",
    "print(\"Query:\", query)\n",
    "print(\"Results:\", top_results)\n",
    "print(\"\\nTop k=5 most similar sentences in corpus:\")\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    print(ai_corpus[idx], \"(Score: {:.4f})\".format(score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-crystal_bert]",
   "language": "python",
   "name": "conda-env-.conda-crystal_bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
