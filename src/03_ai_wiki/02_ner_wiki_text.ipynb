{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition on AI Wiki text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arthor: Crystal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major package used: Stanza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc331451b0474fa08ebfd2ffaa023800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-27 17:27:21 INFO: Downloading default packages for language: en (English)...\n",
      "2021-06-27 17:27:22 INFO: File exists: /home/zz3hs/stanza_resources/en/default.zip.\n",
      "2021-06-27 17:27:27 INFO: Finished downloading models and saved to /home/zz3hs/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "\n",
    "\n",
    "# web scrapping\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import bs4 as bs\n",
    "import urllib\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#name entity recognition\n",
    "import stanza\n",
    "stanza.download('en') # download English model\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import warnings\n",
    "#warnings.simplefilter('always')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in AI Wiki text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts = open(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/ai_wiki_text.txt\", \"r\")\n",
    "with open(\"/home/zz3hs/git/dspg21RnD/data/dspg21RnD/ai_wiki_text.txt\") as f:\n",
    "    contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First paragraph: oneNoneArtificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality. The distinction between the former and the latter categories is often revealed by the acronym chosen. 'Strong' AI is usually labelled as artificial general intelligence (AGI) while attempts to emulate 'natural' intelligence have been called artificial biological intelligence (ABI). Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of achieving its goals.  Colloquially, the term \"artificial intelligence\" is often used to describe machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\". \n",
      "As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenom\n",
      "Last paragraph: e familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity. \n",
      "Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune. In the 1980s, artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but alwa\n"
     ]
    }
   ],
   "source": [
    "len(contents)\n",
    "print(\"First paragraph:\",contents[1:1000])\n",
    "\n",
    "print(\"Last paragraph:\", contents[61100:61759])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-27 17:32:00 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-06-27 17:32:00 INFO: Use device: cpu\n",
      "2021-06-27 17:32:00 INFO: Loading: tokenize\n",
      "2021-06-27 17:32:00 INFO: Loading: ner\n",
      "2021-06-27 17:32:01 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner') # initialize English neural pipeline, tokenize and named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document will contain a list of Sentences, and the Sentences will contain lists of Tokens. \n",
    "doc = nlp(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract named entities\n",
    "entities = doc.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"text\": \"Tesler\",\n",
       "  \"type\": \"PERSON\",\n",
       "  \"start_char\": 1040,\n",
       "  \"end_char\": 1046\n",
       "}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for entity in entities:\n",
    "    df = df.append({\n",
    "        \"text\": entity.text,\n",
    "        'type': entity.type,\n",
    "        'start_char': entity.start_char,\n",
    "        'end_char': entity.end_char\n",
    "        }, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save name entity recognition on AI Wiki text as a panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(r'/home/zz3hs/git/dspg21RnD/data/dspg21RnD/ai_wiki_text_entity.csv', index = True) #export csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3416b071b1ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#de-duplicate text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_unique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_unique\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#de-duplicate text\n",
    "df_unique = df.drop_duplicates(subset=\"text\")\n",
    "len(df_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ORG            69\n",
       "DATE           58\n",
       "PERSON         58\n",
       "WORK_OF_ART    29\n",
       "CARDINAL       17\n",
       "GPE            13\n",
       "PRODUCT        10\n",
       "LOC             6\n",
       "ORDINAL         5\n",
       "PERCENT         5\n",
       "NORP            4\n",
       "MONEY           3\n",
       "TIME            2\n",
       "QUANTITY        1\n",
       "LANGUAGE        1\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique[\"type\"].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Named entities that we are not considering:\n",
    "- Cardinal, just numbers, which wer are not concerned about that could be linked to another Wiki page\n",
    "- Product, such as Skype, kINECT, Xobx 360\n",
    "- Percent\n",
    "- Ordinal\n",
    "- Money\n",
    "- Time\n",
    "- Quantity\n",
    "- Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-00df8ccd30fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morg_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_unique\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_unique\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ORG\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0morg_ls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_unique' is not defined"
     ]
    }
   ],
   "source": [
    "org_ls = df_unique[df_unique[\"type\"] == \"ORG\"].text.tolist()\n",
    "org_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1955',\n",
       " '2015',\n",
       " 'the twenty-first century',\n",
       " '1943',\n",
       " '1956',\n",
       " '1954',\n",
       " '1959',\n",
       " 'the middle of the 1960s',\n",
       " 'twenty years',\n",
       " '1974',\n",
       " 'next few years',\n",
       " 'the early 1980s',\n",
       " '1985',\n",
       " '1987',\n",
       " 'the 1980s',\n",
       " '1989',\n",
       " 'the late 1990s',\n",
       " 'early 21st century',\n",
       " '11 May 1997',\n",
       " '2011',\n",
       " '2012',\n",
       " 'March 2016',\n",
       " 'the 2017',\n",
       " 'two years',\n",
       " 'year',\n",
       " '2017',\n",
       " '2016',\n",
       " '2020',\n",
       " '10,000 days',\n",
       " 'the late 1980s',\n",
       " '1990s',\n",
       " '2019',\n",
       " '1988',\n",
       " 'millions of years',\n",
       " '1984',\n",
       " '1982â€“1992',\n",
       " '2010s',\n",
       " 'the 1940s and 1950s',\n",
       " '1960',\n",
       " '1980s',\n",
       " '1960s',\n",
       " 'the 1960s and the 1970s',\n",
       " '1970',\n",
       " '1950s',\n",
       " 'election year',\n",
       " '2005',\n",
       " 'the next few hundred years',\n",
       " '2010',\n",
       " 'the year 2029',\n",
       " '1863',\n",
       " '1998',\n",
       " 'February 2020',\n",
       " 'January 2015',\n",
       " '2001',\n",
       " '1968',\n",
       " '1999',\n",
       " '1951',\n",
       " '1986']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_ls = df_unique[df_unique[\"type\"] == \"DATE\"].text.tolist()\n",
    "date_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tesler',\n",
       " 'Alpha',\n",
       " 'Karel ÄŒapek',\n",
       " 'Turing',\n",
       " 'John McCarthy',\n",
       " 'Norbert Wiener',\n",
       " 'Marvin Minsky',\n",
       " 'James Lighthill',\n",
       " 'A. Mead',\n",
       " 'Mohammed Ismail',\n",
       " 'Moore',\n",
       " 'Garry Kasparov',\n",
       " 'Watson',\n",
       " 'Brad Rutter',\n",
       " 'Ken Jennings',\n",
       " 'Ke Jie',\n",
       " \"Deep Blue's\",\n",
       " 'Murray Campbell',\n",
       " 'Jack Clark',\n",
       " 'Clark',\n",
       " 'John Haugeland',\n",
       " 'Roger Schank',\n",
       " 'David Rumelhart',\n",
       " 'Markov',\n",
       " 'Joseph Weizenbaum',\n",
       " 'Weizenbaum',\n",
       " 'Wendell Wallach',\n",
       " 'Wallach',\n",
       " 'Charles T. Rubin',\n",
       " 'David Chalmers',\n",
       " 'Jerry Fodor',\n",
       " 'Hilary Putnam',\n",
       " 'John Searle',\n",
       " 'Searle',\n",
       " 'Vernor Vinge',\n",
       " 'Ray Kurzweil',\n",
       " 'Kevin Warwick',\n",
       " 'Robert Ettinger',\n",
       " \"Samuel Butler's\",\n",
       " 'George Dyson',\n",
       " 'Pricewaterhouse',\n",
       " 'Michael Osborne',\n",
       " 'Carl Benedikt Frey',\n",
       " 'Martin Ford',\n",
       " \"Andrew Yang's\",\n",
       " 'Irakli Beridze',\n",
       " 'Stephen Hawking',\n",
       " 'Bill Gates',\n",
       " 'Yuval Noah Harari',\n",
       " 'Elon Musk',\n",
       " 'Hawking',\n",
       " 'Mark Hurd',\n",
       " 'Mark Zuckerberg',\n",
       " 'Isaac Asimov',\n",
       " 'Asimov',\n",
       " 'George Lucas',\n",
       " 'Philip K. Dick',\n",
       " 'Dick']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_ls = df_unique[df_unique[\"type\"] == \"PERSON\"].text.tolist()\n",
    "person_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: WORK_OF_ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Mary Shelley's Frankenstein\",\n",
       " 'Turing-complete \"',\n",
       " 'Analog VLSI Implementation',\n",
       " 'Future of Go Summit',\n",
       " 'Moral Machines  For Wallach',\n",
       " 'part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as \"Does Humanity Want Computers Making Moral Decisions\"',\n",
       " 'Symposium on Machine Ethics',\n",
       " 'Ethics',\n",
       " 'the AAAI Fall 2005 Symposium on Machine Ethics',\n",
       " 'I think',\n",
       " 'Plug & Pray',\n",
       " 'Star Trek Next Generation, with the character of Commander Data',\n",
       " 'Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\"',\n",
       " 'Darwin among the Machines\"',\n",
       " 'book of the same name',\n",
       " 'Nick Bostrom',\n",
       " 'Human Compatible, AI researcher Stuart J. Russell',\n",
       " 'A Space Odyssey',\n",
       " 'The Terminator',\n",
       " 'The Matrix',\n",
       " 'Gort from The Day the Earth Stood Still',\n",
       " 'Bishop from Aliens',\n",
       " 'the Three Laws of Robotics',\n",
       " 'Ghost in the Shell',\n",
       " \"Hajime Sorayama's Sexy Robots\",\n",
       " 'Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids',\n",
       " \"Karel ÄŒapek's R.U.R.\",\n",
       " 'A.I. Artificial Intelligence and Ex Machina',\n",
       " 'well as the novel Do Androids Dream of Electric Sheep']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_of_art_ls = df_unique[df_unique[\"type\"] == \"WORK_OF_ART\"].text.tolist()\n",
    "work_of_art_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U.S.',\n",
       " 'US',\n",
       " 'Japan',\n",
       " 'U.S',\n",
       " 'China',\n",
       " 'Denver',\n",
       " 'England',\n",
       " 'Edinburgh',\n",
       " 'the United States',\n",
       " 'Russia',\n",
       " 'the United Kingdom',\n",
       " 'Republic',\n",
       " 'Bostrom']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpe_ls = df_unique[df_unique[\"type\"] == \"GPE\"].text.tolist()\n",
    "gpe_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['East', 'San Francisco', 'West', 'Europe', 'Rodney Brooks', 'Earth']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_ls = df_unique[df_unique[\"type\"] == \"LOC\"].text.tolist()\n",
    "loc_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: NORP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['British', 'Bayesian', 'Americans', 'Chinese']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norp_ls = df_unique[df_unique[\"type\"] == \"NORP\"].text.tolist()\n",
    "norp_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'org_ls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-679090e217a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morg_ls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdate_ls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperson_ls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwork_of_art_ls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgpe_ls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloc_ls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mner_ls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnorp_ls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'org_ls' is not defined"
     ]
    }
   ],
   "source": [
    "ls = org_ls + date_ls + person_ls + work_of_art_ls + gpe_ls + loc_ls + ner_ls + norp_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-crystal]",
   "language": "python",
   "name": "conda-env-.conda-crystal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
