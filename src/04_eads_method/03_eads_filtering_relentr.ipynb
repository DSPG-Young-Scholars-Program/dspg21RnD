{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eads et al Method, using NSF subsetted corpus to cfda = 47.070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import filter\n",
    "#from git/dspg21RnD/wheat_filtration/wheat_filtration import keywords\n",
    "#from git/dspg21RnD/wheat_filtration/wheat_filtration import filter\n",
    "#import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_topic_proportion(document_topics, relevant_topics):\n",
    "    \"\"\"Return sum of relevant topic proportions for a document.\n",
    "    Arguments:\n",
    "        document_topics (iterable of float): topic proportions for one document.\n",
    "        relevant topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user.\"\"\"\n",
    "    assert (len(relevant_topics) <= len(document_topics)\n",
    "            )  # TODO make this the right kind of error\n",
    "    return sum([document_topics[i] for i in relevant_topics])\n",
    "\n",
    "\n",
    "def keyword_proportion(document, keyword_list):\n",
    "    \"\"\"Return percentage of words in the given doc that are present in keyword_list.\"\"\"\n",
    "    doc_tokens = document.split()\n",
    "    num_keywords = sum(\n",
    "        [1 if word in keyword_list else 0 for word in doc_tokens])\n",
    "    return float(num_keywords)/len(doc_tokens)\n",
    "\n",
    "\n",
    "def superkeyword_presence(document, superkeywords):\n",
    "    \"\"\"Return 1 if document contains any superkeywords, 0 if not.\"\"\"\n",
    "    for word in superkeywords:\n",
    "        if word in document.split():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class FilterHelper():\n",
    "    \"\"\"Creates a filter object containing filter criteria such as keyword list,\n",
    "    superkeyword list, total topic proportion threshold, and keyword proportion\n",
    "    threshold.\n",
    "\n",
    "    Arguments:\n",
    "        topic_model (TopicModel): a TopicModel object instantiated with a corpus or\n",
    "            files from a Mallet topic model.\n",
    "        relevant_topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user. Note that the number\n",
    "            corresponding with the first topic is '0', the second topic is '1', etc.\n",
    "        n_keywords: number of keywords to include in keyword list. Default is 20.\n",
    "        superkeywords (iterable of str): a list of keywords which signify immediate relevance\n",
    "            of the document that contains them (better wording). Default is an empty list.\n",
    "        keyword_list: A list of keywords ordered by [the relevance they signify]. Default is\n",
    "            a keyword list generated using the relative entropy method.\n",
    "        total_topic_prop_threshold (float): the threshold of relevance for the total proportion\n",
    "            of relevant topics in a document. If a document surpases the threshold, it is considered relevant.\n",
    "        keyword_prop_threshold (float): the threshold of relevance for the proportion of words\n",
    "            on the keyword list that appear in a document. If a document surpases the threshold,\n",
    "            it is considered relevant.\n",
    "\n",
    "    Attributes:\n",
    "        topic_model (TopicModel): a TopicModel object instantiated with a corpus or\n",
    "            files from a Mallet topic model.\n",
    "        relevant_topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user.\n",
    "        superkeywords (iterable of str): a list of keywords which signify immediate relevance\n",
    "            of the document that contains them (better wording). Default is an empty list.\n",
    "        keyword_list: A list of keywords ordered by [the relevance they signify]. Default is\n",
    "            a keyword list generated using the relative entropy method.\n",
    "        total_topic_prop_threshold (float): the threshold of relevance for the total proportion\n",
    "            of relevant topics in a document. If a document surpases the threshold, \n",
    "            it is considered relevant. Default is 0.25.\n",
    "        keyword_prop_threshold (float): the threshold of relevance for the proportion of words\n",
    "            on the keyword list that appear in a document. If a document surpases the threshold,\n",
    "            it is considered relevant. Default is 0.15.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: if user enters both keyword list and n_keywords when using the\n",
    "        keyword_list setter method.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, topic_model, vectorizer, relevant_topics, keyword_list=None, n_keywords=100, superkeywords=[],\n",
    "                 term_words = [],\n",
    "                 total_topic_prop_threshold=0.25, keyword_prop_threshold=0.15):\n",
    "        self._relevant_topics = relevant_topics\n",
    "        if keyword_list is None:\n",
    "            keyword_list = keywords.rel_ent_key_list(\n",
    "                topic_model, n_keywords, relevant_topics)\n",
    "        self._keyword_list = keyword_list\n",
    "\n",
    "        lower_superkeys = [word.lower() for word in superkeywords]\n",
    "        # TODO: deal with this appropriately when making lowercasing optional\n",
    "        extended_superkeys = [\n",
    "            word for word in vectorizer.get_feature_names() if\n",
    "            word in lower_superkeys or\n",
    "            any([(chunk in lower_superkeys) for chunk in word.split('_')])\n",
    "        ]\n",
    "        self._superkeywords = extended_superkeys\n",
    "\n",
    "        self._total_topic_prop_threshold = total_topic_prop_threshold\n",
    "        self._keyword_prop_threshold = keyword_prop_threshold\n",
    "        self.term_words = term_words\n",
    "        self._topic_model = topic_model\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "    @property\n",
    "    def topic_model(self):\n",
    "        \"\"\"Get topic_model used to create filter\"\"\"\n",
    "        return self._topic_model\n",
    "\n",
    "    @property\n",
    "    def relevant_topics(self):\n",
    "        \"\"\"Get list of relevant topics\"\"\"\n",
    "        return self._relevant_topics\n",
    "\n",
    "    @property\n",
    "    def keyword_list(self):\n",
    "        \"\"\"Get or set keyword list. Input either a list of keywords, or input an integer n\n",
    "        to generate a keyword list containing n words.\"\"\"\n",
    "        return self._keyword_list\n",
    "\n",
    "    @keyword_list.setter\n",
    "    def keyword_list(self, keyword_list=None, n_keywords=None):\n",
    "        if keyword_list is not None:\n",
    "            self._keyword_list = keyword_list\n",
    "        elif n_keywords is not None:\n",
    "            self._keyword_list = keywords.rel_ent_key_list(\n",
    "                self.topic_model, n_keywords, self.relevant_topics)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Enter either a keyword list or an integer for number of keywords\")\n",
    "\n",
    "    @property\n",
    "    def superkeywords(self):\n",
    "        return self._superkeywords\n",
    "\n",
    "    @superkeywords.setter\n",
    "    def superkeywords(self, superkeywords):\n",
    "        self._superkeywords = superkeywords\n",
    "\n",
    "    @property\n",
    "    def total_topic_prop_threshold(self):\n",
    "        return self._total_topic_prop_threshold\n",
    "\n",
    "    @total_topic_prop_threshold.setter\n",
    "    def total_topic_prop_threshold(self, total_topic_prop_threshold):\n",
    "        self._total_topic_prop_threshold = total_topic_prop_threshold\n",
    "\n",
    "    @property\n",
    "    def keyword_prop_threshold(self):\n",
    "        return self._keyword_prop_threshold\n",
    "\n",
    "    @keyword_prop_threshold.setter\n",
    "    def keyword_prop_threshold(self, keyword_prop_threshold):\n",
    "        self._keyword_prop_threshold = keyword_prop_threshold\n",
    "\n",
    "\n",
    "def proportion_lists():\n",
    "    \"\"\"makes a matrix or list of ttp, superkeyword, and keyword proportion for the docs in corpus\n",
    "    and sets the respective topic model attributes\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def subset_quality(threshs, labeled_subset):  # also had args word_list_gen and scorefun\n",
    "    \"\"\"Calculate F1 score for the array of thresholds threshs\n",
    "    (max topic prop, total topic prop, vocab prop, and number of words\n",
    "    in vocabulary list) on labeled subset\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def subset_info(threshs):  # seems like a cool feature to include\n",
    "    \"\"\"Return set of false positives, true positives, false negatives, and true negatives, as\n",
    "    well as the sizes of the false neg and false pos sets, as well as the size of set\n",
    "    predicted as relevant, about the subset created by the given set of thresholds\n",
    "    (mtp, ttp, voc prop, and voc list length, in that order).\n",
    "    This function can be edited to output any kind of info about the subset, eg the filenames.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for creating a topic dictionary, viewing the topics in the topic model,\n",
    "#and selecting only the relevant topics based on a threshold and our keyword list.\n",
    "\n",
    "def topic_dictionary(lda_model, lda_vectorizer, top_n = 10):\n",
    "    topic_ls = {} #append keys, append the values\n",
    "\n",
    "    for idx, topic in enumerate(lda_model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "\n",
    "        print_list = [(lda_vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        topic_ls[idx] = print_list\n",
    "\n",
    "    return topic_ls\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)\n",
    "      \n",
    "def rel_ent_key_list(topic_model, vectorizer, n_top_keywords, relevant_topics):\n",
    "    \"\"\"Returns a list of the top n keywords based on relative entropy score\n",
    "     Arguments:\n",
    "       topic_model (TopicModel): a topic by vocabulary word matrix where each entry\n",
    "       is the total word count for that word in that topic\n",
    "       n_top_words (int): the number of keywords the method will return\n",
    "       relevant_topics (iterable of int)\n",
    "     Returns:\n",
    "       keyword_list (iterable of str): list of the top n keywords, sorted\n",
    "     \"\"\"\n",
    "    topic_word_matrix = topic_model.components_\n",
    "    lda_vectorizer = vectorizer\n",
    "    \n",
    "    # Log of probabilities of vocab words\n",
    "    #this works\n",
    "    vocab_logs = np.log(topic_word_matrix.sum(\n",
    "        axis=0) / topic_word_matrix.sum())\n",
    "\n",
    "    # Log of probabilities of vocab words given they were in each relevant topic\n",
    "    #this is being built to calculate p(w)*log[p(w)/q(w)]\n",
    "    #this works\n",
    "    topic_logs = np.log(topic_word_matrix[relevant_topics, :].sum(\n",
    "        axis=0) / topic_word_matrix[relevant_topics, :].sum())\n",
    "\n",
    "    # relative entropy proportions, unsorted\n",
    "    #log rules: log[p(w)/q(w)] = log(p(w)) - log(q(w))\n",
    "    unsorted_props = np.asarray(topic_word_matrix.sum(axis=0) /\n",
    "                                topic_word_matrix.sum()) * np.asarray(topic_logs - vocab_logs)\n",
    "\n",
    "    unsorted_props = np.matrix.flatten(unsorted_props)\n",
    "\n",
    "    sorted_props_and_voc = sorted([(unsorted_props[i], lda_vectorizer.get_feature_names()[i]) for i in list(\n",
    "        np.argpartition(unsorted_props, len(lda_vectorizer.get_feature_names()) - n_top_keywords))[-n_top_keywords:]], reverse=True)\n",
    "    ordered_vocab = []\n",
    "    for (_, voc) in sorted_props_and_voc:\n",
    "        ordered_vocab.append(voc)\n",
    "    return ordered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a filter_corpus function (copied from wheat_filtration package)\n",
    "def total_topic_proportion(document_topics, relevant_topics, doc_number = 0):\n",
    "    \"\"\"Return sum of relevant topic proportions for a document.\n",
    "    Arguments:\n",
    "        document_topics (iterable of float): topic proportions for one document.\n",
    "        relevant topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user.\"\"\"\n",
    "    assert (len(relevant_topics) <= len(document_topics)\n",
    "            )  # TODO make this the right kind of error\n",
    "    document = document_topics[doc_number]\n",
    "    topic_prop = 0\n",
    "    for i in relevant_topics:\n",
    "        topic_prop += document[i]    \n",
    "    return topic_prop\n",
    "\n",
    "def keyword_proportion(document, keyword_list):\n",
    "    \"\"\"Return percentage of words in the given doc that are present in keyword_list.\"\"\"\n",
    "    doc_tokens = document\n",
    "    num_keywords = sum(\n",
    "        [1 if word in keyword_list else 0 for word in doc_tokens])\n",
    "    return float(num_keywords)/len(doc_tokens)\n",
    "\n",
    "def superkeyword_presence(document, superkeywords):\n",
    "    \"\"\"Return 1 if document contains any superkeywords, 0 if not.\"\"\"\n",
    "    for word in superkeywords:\n",
    "        if word in document:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def in_ai_phrases(abstract, ai_phrases):\n",
    "    text = \" \".join(abstract)\n",
    "    for phrase in ai_phrases:\n",
    "        if phrase in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_relevant(doc, doc_topics, filter_helper, doc_number = 0, ai_phrases = [\"machine learn\", \"deep learn\", \"deep learning\", \"artificial intelligence\", \"natural language processing\"]):\n",
    "    \"\"\"Returns a boolean for relevance of given document. A document is considered\n",
    "    relevant if: it contains any superkeywords(filter_helper.superkeywords), passes\n",
    "    the total topic proportion threshold(filter_helper.total_topic_prop_threshold),\n",
    "    or passes the keyword proportion threshold(filter_helper.keyword_prop_threshold).\n",
    "    Arguments:\n",
    "        doc (string): preprocessed document from the corpus\n",
    "        doc_topics (iterable of float): proportion of each topic present in the given document\n",
    "        filter_helper (FilterHelper): an object containing the necessary information\n",
    "            to label the relevance of the given document\n",
    "    Returns:\n",
    "        (bool): Representing whether or not the given document is relevant according\n",
    "        to the information in filter_helper\"\"\"\n",
    "\n",
    "    has_superkeyword = superkeyword_presence(\n",
    "        doc, filter_helper.superkeywords)\n",
    "    \n",
    "    in_phrases = in_ai_phrases(doc, ai_phrases)\n",
    "    \n",
    "    passes_total_topic_thresh = total_topic_proportion(\n",
    "        doc_topics, filter_helper.relevant_topics, doc_number) > (filter_helper.total_topic_prop_threshold)\n",
    "    \n",
    "    passes_keyword_thresh = keyword_proportion(\n",
    "        doc, filter_helper.keyword_list) > filter_helper.keyword_prop_threshold\n",
    "\n",
    "    return has_superkeyword or passes_total_topic_thresh or passes_keyword_thresh or in_phrases\n",
    "\n",
    "\n",
    "def filter_corpus(abstract_column, doc_topics, filter_helper, ai_phrases = [\"machine learn\", \"deep learn\", \"deep learning\", \"artificial intelligence\", \"natural language processing\"]):\n",
    "    subcorpus_id = []\n",
    "    superkey = 0\n",
    "    topic_thresh = 0\n",
    "    keyword_thresh = 0\n",
    "    phrases = 0\n",
    "    for i, abstract in enumerate(abstract_column):\n",
    "        doc = abstract \n",
    "        if is_relevant(doc, doc_topics, filter_helper, doc_number = i):\n",
    "            if superkeyword_presence(doc, filter_helper.superkeywords):\n",
    "                superkey += 1\n",
    "            if total_topic_proportion(doc_topics, filter_helper.relevant_topics, doc_number = i) > (filter_helper.total_topic_prop_threshold):\n",
    "                topic_thresh += 1\n",
    "            if keyword_proportion(doc, filter_helper.keyword_list) > filter_helper.keyword_prop_threshold :\n",
    "                keyword_thresh += 1\n",
    "            if in_ai_phrases(doc, ai_phrases):\n",
    "                phrases += 1\n",
    "            subcorpus_id.append(i)\n",
    "    print(\"Superkeyword presence: \", superkey, \"\\nTotal Topic Proportion: \", topic_thresh, \"\\nKeyword Threshold: \",\n",
    "          keyword_thresh, \"\\nPhrase words matched: \", phrases, \"\\nTotal docs: \", len(subcorpus_id))\n",
    "    return subcorpus_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the core terms from the OECD paper\n",
    "core_terms = [\"adaboost\",\"artificial intelligence\",\"artificial neural network\",\"back propagation\"\n",
    ",\"back propagation neural network\",\"computational intelligence\",\"computer vision\"\n",
    ",\"convolutional neural network\",\"deep belief network\",\"deep convolutional neural network\"\n",
    ",\"deep learn\",\"deep neural network\",\"elman network\",\"elman neural network\"\n",
    ",\"expert system\",\"fee forward neural network\",\"inference engine\",\"machine intelligence\"\n",
    ",\"machine learn\",\"machine translation\",\"machine vision\",\"multilayer neural network\"\n",
    ",\"natural language process\",\"perceptron\",\"random forest\",\"rbf neural network\",\"recurrent neural network\"\n",
    ",\"self organize map\",\"spike neural network\",\"supervise learn\",\"support vector machine\"\n",
    ",\"svm classifier\",\"unsupervised learn\",\"artificial_intelligence\",\"artificial_neural_network\",\"back_propagation\"\n",
    ",\"back_propagation_neural_network\",\"computational_intelligence\",\"computer_vision\"\n",
    ",\"convolutional_neural_network\",\"deep_belief_network\",\"deep_convolutional_neural_network\"\n",
    ",\"deep_learn\",\"deep_neural_network\",\"elman_network\",\"elman_neural_network\"\n",
    ",\"expert_system\",\"fee_forward_neural_network\",\"inference_engine\",\"machine_intelligence\"\n",
    ",\"machine_learn\",\"machine_translation\",\"machine_vision\",\"multilayer_neural_network\"\n",
    ",\"natural_language_process\",\"random_forest\",\"rbf_neural_network\",\"recurrent_neural_network\"\n",
    ",\"self_organize_map\",\"spike_neural_network\",\"supervise_learn\",\"support_vector_machine\"\n",
    ",\"svm_classifier\",\"unsupervised_learn\", \"machine_learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_topics(topic_dictionary, keyword_list, threshold = 1):\n",
    "    \"\"\"returns a list of the topics which contain a threshold % of the\n",
    "    relevant words in the keyword list\"\"\"\n",
    "    relevant_topic = []\n",
    "    for key in topic_dictionary:\n",
    "        relevant_words = 0\n",
    "        for i in range(len(topic_dictionary[key])):\n",
    "            if topic_dictionary[key][i][0] in keyword_list:\n",
    "                relevant_words += 1\n",
    "            else: relevant_words += 0\n",
    "        if (relevant_words) >= threshold :#/ len(topic_dictionary[key]) >= threshold :\n",
    "            relevant_topic.append(key)\n",
    "    return relevant_topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../data/dspg21RnD/smaller-final-dataset.pkl\")\n",
    "nsf = df[df[\"AGENCY\"] == \"NSF\"]\n",
    "# filter where cfda = 47.070\n",
    "\n",
    "nsf_csci = nsf[nsf[\"CFDA_CODE\"] == \"47.070\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nsf_csci[\"final_frqwds_removed\"]\n",
    "\n",
    "text = [] # text will contain the processed tokens in string form (1 string per abstract)\n",
    "\n",
    "\n",
    "for abstract in tokens:\n",
    "    text.append(\" \".join(abstract))\n",
    "    \n",
    "text = pd.Series(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vectorizer = CountVectorizer(max_df=0.6, min_df=20)\n",
    "\n",
    "lda_dtm = lda_vectorizer.fit_transform(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 100\n",
    "lda_model_100 = LatentDirichletAllocation(n_components=num_topics, doc_topic_prior = 1/num_topics, \n",
    "                                      topic_word_prior=0.1, n_jobs=39, random_state = 0)\n",
    "\n",
    "doc_top_dist_100 = lda_model_100.fit_transform(lda_dtm)\n",
    "top_term_dist_100 = lda_model_100.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsfcs_dic100 = topic_dictionary(lda_model_100, lda_vectorizer, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 23, 28, 87, 97]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics(nsfcs_dic100, core_terms, 0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we get 5 topics when we do 1 word out of 50 ahhaha.  We only get topic 97 when we use 2 words out of 50.  I will look through these topics and add to the topics i picked out myself and decide the relevant topics, then pick out the relative entropy keyword list before making a superkeyword list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"AI\" is the 20th term.  and there are only 34 times it comes up in this topic?  Not gonna include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will keep 27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT = [27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it on my own, I picked out 19, 52, 54, 76, 79, 86, 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 87, 97]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't know about this, since it is just robot and not the other AI terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 87, 97, 19, 52, 79, 86]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics_HT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so with my judgement plus some that the relevant_topics function picked out, we have 7 topics that should be roughly about AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the relative entropy keyword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ent_top200 = rel_ent_key_list(lda_model_100, lda_vectorizer, 200, relevant_topics_HT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the superkeyword list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"To create the super keyword list, we examine an expanded list -- the top 1000 words -- of high-relative-entropy-constribution words from the last step and select those words that are unambiguously related to the concept of interest, i.e. likely to be used when referring to the concept of interest and no other concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the filter helper to see if we can start trying to filter the corpus to get some sort of sense the abstracts that are about AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_HT_KL = ['machine_learning',  'artificial_intelligence', 'artificial_intelligence_ai',\n",
    "                'convolutional_neural_network', 'recognition_asr',  'artificial_intelligence_machine_learning']\n",
    "\n",
    "phrase = ['learning', 'learn', 'processing',  'natural', 'deep', 'intelligence', 'artificial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = [\"machine learn\", \"deep learn\", \"deep learning\", \"artificial intelligence\", \"natural language processing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter_helper = FilterHelper(topic_model = lda_model_100,\n",
    "                                vectorizer = lda_vectorizer,\n",
    "                               relevant_topics = relevant_topics_HT,\n",
    "                               superkeywords = ai_HT_KL,\n",
    "                               keyword_list = rel_ent_top200,\n",
    "                               total_topic_prop_threshold = 0.25,\n",
    "                               keyword_prop_threshold = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16k rows because one for each document.  100 columns because 1 for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new document-topic-distribution with the full corpus\n",
    "tokens2 = df[\"final_frqwds_removed\"]\n",
    "\n",
    "fullcorpus = [] # text will contain the processed tokens in string form (1 string per abstract)\n",
    "\n",
    "\n",
    "for abstract in tokens2:\n",
    "    fullcorpus.append(\" \".join(abstract))\n",
    "    \n",
    "fullcorpus = pd.Series(fullcorpus)\n",
    "\n",
    "newdocs = fullcorpus\n",
    "new_doc_term_matrix = lda_vectorizer.transform(newdocs) \n",
    "new_doc_term_dist = lda_model_100.transform(new_doc_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.054090</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.080156</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069471</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.027441</td>\n",
       "      <td>0.046532</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.025483</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.111705</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690809</th>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.033541</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690810</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.095025</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.031832</td>\n",
       "      <td>0.058164</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690811</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690812</th>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025708</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690813</th>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.052894</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.113811</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690814 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "0       0.000238  0.000238  0.000238  0.000238  0.000238  0.000238  0.000238   \n",
       "1       0.000130  0.000130  0.000130  0.000130  0.000130  0.000130  0.000130   \n",
       "2       0.069471  0.000164  0.000164  0.000164  0.000164  0.000164  0.000164   \n",
       "3       0.000192  0.000192  0.000192  0.000192  0.000192  0.000192  0.000192   \n",
       "4       0.000476  0.000476  0.111705  0.000476  0.000476  0.000476  0.000476   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "690809  0.000115  0.000115  0.000115  0.000115  0.000115  0.000115  0.000115   \n",
       "690810  0.000092  0.000092  0.000092  0.000092  0.000092  0.000092  0.000092   \n",
       "690811  0.000090  0.000090  0.000090  0.000090  0.000090  0.000090  0.000090   \n",
       "690812  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096   \n",
       "690813  0.000116  0.000116  0.000116  0.000116  0.000116  0.000116  0.000116   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "0       0.000238  0.000238  0.000238  ...  0.000238  0.000238  0.000238   \n",
       "1       0.000130  0.000130  0.000130  ...  0.011638  0.000130  0.000130   \n",
       "2       0.000164  0.000164  0.000164  ...  0.000164  0.000164  0.000164   \n",
       "3       0.000192  0.000192  0.000192  ...  0.000192  0.000192  0.027441   \n",
       "4       0.000476  0.000476  0.000476  ...  0.000476  0.000476  0.000476   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "690809  0.000115  0.000115  0.000115  ...  0.000115  0.000115  0.000115   \n",
       "690810  0.000092  0.000092  0.000092  ...  0.000092  0.000092  0.000092   \n",
       "690811  0.000090  0.000090  0.000090  ...  0.000090  0.000090  0.000090   \n",
       "690812  0.000096  0.000096  0.000096  ...  0.025708  0.000096  0.000096   \n",
       "690813  0.000116  0.000116  0.000116  ...  0.000116  0.000116  0.000116   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "0       0.054090  0.000238  0.000238  0.000238  0.000238  0.000238  0.000238  \n",
       "1       0.080156  0.000130  0.000130  0.000130  0.000130  0.000130  0.000130  \n",
       "2       0.000164  0.000164  0.000164  0.000164  0.020310  0.000164  0.000164  \n",
       "3       0.046532  0.000192  0.025483  0.000192  0.000192  0.043263  0.000192  \n",
       "4       0.000476  0.000476  0.000476  0.000476  0.000476  0.000476  0.000476  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "690809  0.033541  0.000115  0.000115  0.000115  0.000115  0.000115  0.000115  \n",
       "690810  0.095025  0.000092  0.031832  0.058164  0.000092  0.000092  0.000092  \n",
       "690811  0.000090  0.000090  0.000090  0.000090  0.000090  0.000090  0.000090  \n",
       "690812  0.072165  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  \n",
       "690813  0.052894  0.000116  0.113811  0.000116  0.000116  0.000116  0.000116  \n",
       "\n",
       "[690814 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(new_doc_term_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superkeyword presence:  1748 \n",
      "Total Topic Proportion:  1870 \n",
      "Keyword Threshold:  6383 \n",
      "Phrase words matched:  868 \n",
      "Total docs:  6930\n"
     ]
    }
   ],
   "source": [
    "my_subcorpus = filter_corpus(nsf_csci[\"final_frqwds_removed\"], doc_top_dist_100, my_filter_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6930"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_subcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 10, 11, 15, 18, 19, 21, 23, 42, 46]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_subcorpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-d0d36e778813>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nsf_csci[\"index\"] = range(len(nsf_csci))\n"
     ]
    }
   ],
   "source": [
    "# nsf_csci dataframe of NSF \n",
    "# my_subcorpus = list of indices \n",
    "nsf_csci[\"index\"] = range(len(nsf_csci))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcorpus_df = pd.DataFrame(my_subcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcorpus_df[\"is_ai_eads\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcorpus_df = subcorpus_df.rename(columns = {0: \"index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>is_ai_eads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6925</th>\n",
       "      <td>16405</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6926</th>\n",
       "      <td>16406</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6927</th>\n",
       "      <td>16408</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6928</th>\n",
       "      <td>16413</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6929</th>\n",
       "      <td>16415</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6930 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  is_ai_eads\n",
       "0         8        True\n",
       "1        10        True\n",
       "2        11        True\n",
       "3        15        True\n",
       "4        18        True\n",
       "...     ...         ...\n",
       "6925  16405        True\n",
       "6926  16406        True\n",
       "6927  16408        True\n",
       "6928  16413        True\n",
       "6929  16415        True\n",
       "\n",
       "[6930 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subcorpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_test = pd.merge(nsf_csci, subcorpus_df, on=\"index\", how = \"right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original index</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>FY</th>\n",
       "      <th>ORG_COUNT</th>\n",
       "      <th>PI_COUNT</th>\n",
       "      <th>nchar</th>\n",
       "      <th>final_frqwds_removed</th>\n",
       "      <th>PROJECT_TERMS</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>...</th>\n",
       "      <th>ORGANIZATION_ZIP</th>\n",
       "      <th>ORGANIZATION_COUNTRY</th>\n",
       "      <th>BUDGET_START_DATE</th>\n",
       "      <th>BUDGET_END_DATE</th>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <th>FY.y</th>\n",
       "      <th>FY_TOTAL_COST</th>\n",
       "      <th>FY_TOTAL_COST_SUB_PROJECTS</th>\n",
       "      <th>index</th>\n",
       "      <th>is_ai_eads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10449</td>\n",
       "      <td>99165</td>\n",
       "      <td>An effective document representation is a cruc...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1851</td>\n",
       "      <td>[document, representation, crucial, text, proc...</td>\n",
       "      <td>Classification; Computer Assisted; Computers;...</td>\n",
       "      <td>CAREER: MULTIRESOLUTION REPRESENTATIONS OF DOC...</td>\n",
       "      <td>...</td>\n",
       "      <td>47907-2114</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11551</td>\n",
       "      <td>101540</td>\n",
       "      <td>This project involves designing approximation ...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1143</td>\n",
       "      <td>[approximation, algorithm, forfundamental, ari...</td>\n",
       "      <td>Algorithms; Communication; computer network; ...</td>\n",
       "      <td>APPROXIMATING BICRITERIA NETWORK-DESIGN PROBLEMS</td>\n",
       "      <td>...</td>\n",
       "      <td>08102-1400</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2008</td>\n",
       "      <td>55904.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3330</td>\n",
       "      <td>93291</td>\n",
       "      <td>Cyber-Physical Systems (CPS) is a new class of...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1671</td>\n",
       "      <td>[cyber_physical, systems, cps, class, embed, c...</td>\n",
       "      <td>Assisted Living Facilities; Collaborations; C...</td>\n",
       "      <td>SPECIAL SESSION ON ROBOTICS AND CYBER-PHYSICAL...</td>\n",
       "      <td>...</td>\n",
       "      <td>12180-3590</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2008</td>\n",
       "      <td>17645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5210</td>\n",
       "      <td>94539</td>\n",
       "      <td>This SGER award is investigating the feasibili...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2037</td>\n",
       "      <td>[sger, award, feasibility, execute, scale, age...</td>\n",
       "      <td>Address; Architecture; Award; base; Behavior;...</td>\n",
       "      <td>SGER: EXPLORING DATA-PARALLEL TECHNIQUES FOR M...</td>\n",
       "      <td>...</td>\n",
       "      <td>49931-1295</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2008</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199282</td>\n",
       "      <td>121128</td>\n",
       "      <td>Time series of gene expression of gene, protei...</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2205</td>\n",
       "      <td>[series, gene, gene, protein, metabolite, conc...</td>\n",
       "      <td>Algorithms; base; Base Sequence; Biochemical;...</td>\n",
       "      <td>SEI: UNRAVELING THE STRUCTURE AND KINETICS OF ...</td>\n",
       "      <td>...</td>\n",
       "      <td>48109-1271</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2009</td>\n",
       "      <td>146898.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6925</th>\n",
       "      <td>1059840</td>\n",
       "      <td>1086737</td>\n",
       "      <td>Artificial intelligence, fueled by recent adva...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2801</td>\n",
       "      <td>[artificial_intelligence, fuel, recent, advanc...</td>\n",
       "      <td>Address; Algorithmic Analysis; Alzheimer's Di...</td>\n",
       "      <td>CAREER: NEW LEARNING-BASED ALGORITHMS FOR THE ...</td>\n",
       "      <td>...</td>\n",
       "      <td>14850-2820</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2018</td>\n",
       "      <td>581438.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16405</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6926</th>\n",
       "      <td>1063000</td>\n",
       "      <td>1089879</td>\n",
       "      <td>This frontier project establishes the Center f...</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2293</td>\n",
       "      <td>[frontier, trustworthy, machine_learning, ctml...</td>\n",
       "      <td>Algorithms; Award; base; broadening participa...</td>\n",
       "      <td>SATC: CORE: FRONTIER: COLLABORATIVE: END-TO-EN...</td>\n",
       "      <td>...</td>\n",
       "      <td>22904-4195</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2018</td>\n",
       "      <td>182666.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16406</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6927</th>\n",
       "      <td>1068780</td>\n",
       "      <td>1095678</td>\n",
       "      <td>Software-defined networking (SDN) is a key ena...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1711</td>\n",
       "      <td>[software, define_networking_sdn, enable, tech...</td>\n",
       "      <td>Address; Advocate; Algorithms; Award; Communi...</td>\n",
       "      <td>CAREER: BUG TOLERANT NETWORKING: ENABLING HIGH...</td>\n",
       "      <td>...</td>\n",
       "      <td>02912-9002</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2018</td>\n",
       "      <td>93129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16408</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6928</th>\n",
       "      <td>1059261</td>\n",
       "      <td>1086153</td>\n",
       "      <td>The Future of Work at the Human-Technology Fro...</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3376</td>\n",
       "      <td>[future, human, technology, frontier_fw_htf, b...</td>\n",
       "      <td>Adverse event; Aircraft; Algorithms; Artifici...</td>\n",
       "      <td>FW-HTF: COLLABORATIVE RESEARCH: AUGMENTING AND...</td>\n",
       "      <td>...</td>\n",
       "      <td>99164-1060</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2018</td>\n",
       "      <td>1378337.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16413</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6929</th>\n",
       "      <td>1062845</td>\n",
       "      <td>1089729</td>\n",
       "      <td>This project provides seamless connections amo...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2064</td>\n",
       "      <td>[seamless, connection, platform, scientific, s...</td>\n",
       "      <td>Address; Adoption; application programming in...</td>\n",
       "      <td>FRAMEWORK: DATA: HDR: EXTENSIBLE GEOSPATIAL DA...</td>\n",
       "      <td>...</td>\n",
       "      <td>47907-2114</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2018</td>\n",
       "      <td>4571811.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16415</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6930 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      original index PROJECT_ID  \\\n",
       "0              10449      99165   \n",
       "1              11551     101540   \n",
       "2               3330      93291   \n",
       "3               5210      94539   \n",
       "4             199282     121128   \n",
       "...              ...        ...   \n",
       "6925         1059840    1086737   \n",
       "6926         1063000    1089879   \n",
       "6927         1068780    1095678   \n",
       "6928         1059261    1086153   \n",
       "6929         1062845    1089729   \n",
       "\n",
       "                                               ABSTRACT    FY  ORG_COUNT  \\\n",
       "0     An effective document representation is a cruc...  2008          1   \n",
       "1     This project involves designing approximation ...  2008          1   \n",
       "2     Cyber-Physical Systems (CPS) is a new class of...  2008          1   \n",
       "3     This SGER award is investigating the feasibili...  2008          1   \n",
       "4     Time series of gene expression of gene, protei...  2009          1   \n",
       "...                                                 ...   ...        ...   \n",
       "6925  Artificial intelligence, fueled by recent adva...  2018          1   \n",
       "6926  This frontier project establishes the Center f...  2018          6   \n",
       "6927  Software-defined networking (SDN) is a key ena...  2018          1   \n",
       "6928  The Future of Work at the Human-Technology Fro...  2018          3   \n",
       "6929  This project provides seamless connections amo...  2018          1   \n",
       "\n",
       "      PI_COUNT  nchar                               final_frqwds_removed  \\\n",
       "0            1   1851  [document, representation, crucial, text, proc...   \n",
       "1            1   1143  [approximation, algorithm, forfundamental, ari...   \n",
       "2            1   1671  [cyber_physical, systems, cps, class, embed, c...   \n",
       "3            1   2037  [sger, award, feasibility, execute, scale, age...   \n",
       "4            1   2205  [series, gene, gene, protein, metabolite, conc...   \n",
       "...        ...    ...                                                ...   \n",
       "6925         1   2801  [artificial_intelligence, fuel, recent, advanc...   \n",
       "6926         6   2293  [frontier, trustworthy, machine_learning, ctml...   \n",
       "6927         1   1711  [software, define_networking_sdn, enable, tech...   \n",
       "6928         3   3376  [future, human, technology, frontier_fw_htf, b...   \n",
       "6929         1   2064  [seamless, connection, platform, scientific, s...   \n",
       "\n",
       "                                          PROJECT_TERMS  \\\n",
       "0      Classification; Computer Assisted; Computers;...   \n",
       "1      Algorithms; Communication; computer network; ...   \n",
       "2      Assisted Living Facilities; Collaborations; C...   \n",
       "3      Address; Architecture; Award; base; Behavior;...   \n",
       "4      Algorithms; base; Base Sequence; Biochemical;...   \n",
       "...                                                 ...   \n",
       "6925   Address; Algorithmic Analysis; Alzheimer's Di...   \n",
       "6926   Algorithms; Award; base; broadening participa...   \n",
       "6927   Address; Advocate; Algorithms; Award; Communi...   \n",
       "6928   Adverse event; Aircraft; Algorithms; Artifici...   \n",
       "6929   Address; Adoption; application programming in...   \n",
       "\n",
       "                                          PROJECT_TITLE  ... ORGANIZATION_ZIP  \\\n",
       "0     CAREER: MULTIRESOLUTION REPRESENTATIONS OF DOC...  ...       47907-2114   \n",
       "1      APPROXIMATING BICRITERIA NETWORK-DESIGN PROBLEMS  ...       08102-1400   \n",
       "2     SPECIAL SESSION ON ROBOTICS AND CYBER-PHYSICAL...  ...       12180-3590   \n",
       "3     SGER: EXPLORING DATA-PARALLEL TECHNIQUES FOR M...  ...       49931-1295   \n",
       "4     SEI: UNRAVELING THE STRUCTURE AND KINETICS OF ...  ...       48109-1271   \n",
       "...                                                 ...  ...              ...   \n",
       "6925  CAREER: NEW LEARNING-BASED ALGORITHMS FOR THE ...  ...       14850-2820   \n",
       "6926  SATC: CORE: FRONTIER: COLLABORATIVE: END-TO-EN...  ...       22904-4195   \n",
       "6927  CAREER: BUG TOLERANT NETWORKING: ENABLING HIGH...  ...       02912-9002   \n",
       "6928  FW-HTF: COLLABORATIVE RESEARCH: AUGMENTING AND...  ...       99164-1060   \n",
       "6929  FRAMEWORK: DATA: HDR: EXTENSIBLE GEOSPATIAL DA...  ...       47907-2114   \n",
       "\n",
       "     ORGANIZATION_COUNTRY BUDGET_START_DATE BUDGET_END_DATE CFDA_CODE  FY.y  \\\n",
       "0           UNITED STATES               NaN             NaN    47.070  2008   \n",
       "1           UNITED STATES               NaN             NaN    47.070  2008   \n",
       "2           UNITED STATES               NaN             NaN    47.070  2008   \n",
       "3           UNITED STATES               NaN             NaN    47.070  2008   \n",
       "4           UNITED STATES               NaN             NaN    47.070  2009   \n",
       "...                   ...               ...             ...       ...   ...   \n",
       "6925        UNITED STATES               NaN             NaN    47.070  2018   \n",
       "6926        UNITED STATES               NaN             NaN    47.070  2018   \n",
       "6927        UNITED STATES               NaN             NaN    47.070  2018   \n",
       "6928        UNITED STATES               NaN             NaN    47.070  2018   \n",
       "6929        UNITED STATES               NaN             NaN    47.070  2018   \n",
       "\n",
       "     FY_TOTAL_COST FY_TOTAL_COST_SUB_PROJECTS  index is_ai_eads  \n",
       "0              NaN                        NaN      8       True  \n",
       "1          55904.0                        NaN     10       True  \n",
       "2          17645.0                        NaN     11       True  \n",
       "3         100000.0                        NaN     15       True  \n",
       "4         146898.0                        NaN     18       True  \n",
       "...            ...                        ...    ...        ...  \n",
       "6925      581438.0                        NaN  16405       True  \n",
       "6926      182666.0                        NaN  16406       True  \n",
       "6927       93129.0                        NaN  16408       True  \n",
       "6928     1378337.0                        NaN  16413       True  \n",
       "6929     4571811.0                        NaN  16415       True  \n",
       "\n",
       "[6930 rows x 33 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_test.to_csv(\"../../data/dspg21RnD/Eads_AI_abstracts.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yesterday, when I just used the top 100 or so relative entropy, I had 685,677 out of 690,814 as the subcorpus.  Today, when picking out just a couple superkeywords, I got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a pretty complicated method that we had to adapt to our problem, so there are a lot of things we are going to have to work out such as deciding what to do about lemmatization (if we want to fuzzy match), work on the sensitivity of including a corpus,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "210907 with a shorter list on the full corpus.  11,219 within NSF CSCI dataset with the shorter list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subcorpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsf_csci[\"ABSTRACT\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subcorpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsf_csci[\"ABSTRACT\"].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subcorpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsf_csci[\"ABSTRACT\"].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-HT_Bert]",
   "language": "python",
   "name": "conda-env-.conda-HT_Bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
