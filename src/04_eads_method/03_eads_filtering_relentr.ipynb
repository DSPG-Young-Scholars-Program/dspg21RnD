{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eads et al Method, using NSF subsetted corpus to cfda = 47.070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import filter\n",
    "#from git/dspg21RnD/wheat_filtration/wheat_filtration import keywords\n",
    "#from git/dspg21RnD/wheat_filtration/wheat_filtration import filter\n",
    "#import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_topic_proportion(document_topics, relevant_topics):\n",
    "    \"\"\"Return sum of relevant topic proportions for a document.\n",
    "    Arguments:\n",
    "        document_topics (iterable of float): topic proportions for one document.\n",
    "        relevant topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user.\"\"\"\n",
    "    assert (len(relevant_topics) <= len(document_topics)\n",
    "            )  # TODO make this the right kind of error\n",
    "    return sum([document_topics[i] for i in relevant_topics])\n",
    "\n",
    "\n",
    "def keyword_proportion(document, keyword_list):\n",
    "    \"\"\"Return percentage of words in the given doc that are present in keyword_list.\"\"\"\n",
    "    doc_tokens = document.split()\n",
    "    num_keywords = sum(\n",
    "        [1 if word in keyword_list else 0 for word in doc_tokens])\n",
    "    return float(num_keywords)/len(doc_tokens)\n",
    "\n",
    "\n",
    "def superkeyword_presence(document, superkeywords):\n",
    "    \"\"\"Return 1 if document contains any superkeywords, 0 if not.\"\"\"\n",
    "    for word in superkeywords:\n",
    "        if word in document.split():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class FilterHelper():\n",
    "    \"\"\"Creates a filter object containing filter criteria such as keyword list,\n",
    "    superkeyword list, total topic proportion threshold, and keyword proportion\n",
    "    threshold.\n",
    "\n",
    "    Arguments:\n",
    "        topic_model (TopicModel): a TopicModel object instantiated with a corpus or\n",
    "            files from a Mallet topic model.\n",
    "        relevant_topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user. Note that the number\n",
    "            corresponding with the first topic is '0', the second topic is '1', etc.\n",
    "        n_keywords: number of keywords to include in keyword list. Default is 20.\n",
    "        superkeywords (iterable of str): a list of keywords which signify immediate relevance\n",
    "            of the document that contains them (better wording). Default is an empty list.\n",
    "        keyword_list: A list of keywords ordered by [the relevance they signify]. Default is\n",
    "            a keyword list generated using the relative entropy method.\n",
    "        total_topic_prop_threshold (float): the threshold of relevance for the total proportion\n",
    "            of relevant topics in a document. If a document surpases the threshold, it is considered relevant.\n",
    "        keyword_prop_threshold (float): the threshold of relevance for the proportion of words\n",
    "            on the keyword list that appear in a document. If a document surpases the threshold,\n",
    "            it is considered relevant.\n",
    "\n",
    "    Attributes:\n",
    "        topic_model (TopicModel): a TopicModel object instantiated with a corpus or\n",
    "            files from a Mallet topic model.\n",
    "        relevant_topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user.\n",
    "        superkeywords (iterable of str): a list of keywords which signify immediate relevance\n",
    "            of the document that contains them (better wording). Default is an empty list.\n",
    "        keyword_list: A list of keywords ordered by [the relevance they signify]. Default is\n",
    "            a keyword list generated using the relative entropy method.\n",
    "        total_topic_prop_threshold (float): the threshold of relevance for the total proportion\n",
    "            of relevant topics in a document. If a document surpases the threshold, \n",
    "            it is considered relevant. Default is 0.25.\n",
    "        keyword_prop_threshold (float): the threshold of relevance for the proportion of words\n",
    "            on the keyword list that appear in a document. If a document surpases the threshold,\n",
    "            it is considered relevant. Default is 0.15.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: if user enters both keyword list and n_keywords when using the\n",
    "        keyword_list setter method.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, topic_model, vectorizer, relevant_topics, keyword_list=None, n_keywords=100, superkeywords=[],\n",
    "                 total_topic_prop_threshold=0.25, keyword_prop_threshold=0.15):\n",
    "        self._relevant_topics = relevant_topics\n",
    "        if keyword_list is None:\n",
    "            keyword_list = keywords.rel_ent_key_list(\n",
    "                topic_model, n_keywords, relevant_topics)\n",
    "        self._keyword_list = keyword_list\n",
    "\n",
    "        lower_superkeys = [word.lower() for word in superkeywords]\n",
    "        # TODO: deal with this appropriately when making lowercasing optional\n",
    "        extended_superkeys = [\n",
    "            word for word in vectorizer.get_feature_names() if\n",
    "            word in lower_superkeys or\n",
    "            any([(chunk in lower_superkeys) for chunk in word.split('_')])\n",
    "        ]\n",
    "        self._superkeywords = extended_superkeys\n",
    "\n",
    "        self._total_topic_prop_threshold = total_topic_prop_threshold\n",
    "        self._keyword_prop_threshold = keyword_prop_threshold\n",
    "        self._topic_model = topic_model\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "    @property\n",
    "    def topic_model(self):\n",
    "        \"\"\"Get topic_model used to create filter\"\"\"\n",
    "        return self._topic_model\n",
    "\n",
    "    @property\n",
    "    def relevant_topics(self):\n",
    "        \"\"\"Get list of relevant topics\"\"\"\n",
    "        return self._relevant_topics\n",
    "\n",
    "    @property\n",
    "    def keyword_list(self):\n",
    "        \"\"\"Get or set keyword list. Input either a list of keywords, or input an integer n\n",
    "        to generate a keyword list containing n words.\"\"\"\n",
    "        return self._keyword_list\n",
    "\n",
    "    @keyword_list.setter\n",
    "    def keyword_list(self, keyword_list=None, n_keywords=None):\n",
    "        if keyword_list is not None:\n",
    "            self._keyword_list = keyword_list\n",
    "        elif n_keywords is not None:\n",
    "            self._keyword_list = keywords.rel_ent_key_list(\n",
    "                self.topic_model, n_keywords, self.relevant_topics)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Enter either a keyword list or an integer for number of keywords\")\n",
    "\n",
    "    @property\n",
    "    def superkeywords(self):\n",
    "        return self._superkeywords\n",
    "\n",
    "    @superkeywords.setter\n",
    "    def superkeywords(self, superkeywords):\n",
    "        self._superkeywords = superkeywords\n",
    "\n",
    "    @property\n",
    "    def total_topic_prop_threshold(self):\n",
    "        return self._total_topic_prop_threshold\n",
    "\n",
    "    @total_topic_prop_threshold.setter\n",
    "    def total_topic_prop_threshold(self, total_topic_prop_threshold):\n",
    "        self._total_topic_prop_threshold = total_topic_prop_threshold\n",
    "\n",
    "    @property\n",
    "    def keyword_prop_threshold(self):\n",
    "        return self._keyword_prop_threshold\n",
    "\n",
    "    @keyword_prop_threshold.setter\n",
    "    def keyword_prop_threshold(self, keyword_prop_threshold):\n",
    "        self._keyword_prop_threshold = keyword_prop_threshold\n",
    "\n",
    "\n",
    "def proportion_lists():\n",
    "    \"\"\"makes a matrix or list of ttp, superkeyword, and keyword proportion for the docs in corpus\n",
    "    and sets the respective topic model attributes\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def subset_quality(threshs, labeled_subset):  # also had args word_list_gen and scorefun\n",
    "    \"\"\"Calculate F1 score for the array of thresholds threshs\n",
    "    (max topic prop, total topic prop, vocab prop, and number of words\n",
    "    in vocabulary list) on labeled subset\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def subset_info(threshs):  # seems like a cool feature to include\n",
    "    \"\"\"Return set of false positives, true positives, false negatives, and true negatives, as\n",
    "    well as the sizes of the false neg and false pos sets, as well as the size of set\n",
    "    predicted as relevant, about the subset created by the given set of thresholds\n",
    "    (mtp, ttp, voc prop, and voc list length, in that order).\n",
    "    This function can be edited to output any kind of info about the subset, eg the filenames.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for creating a topic dictionary, viewing the topics in the topic model,\n",
    "#and selecting only the relevant topics based on a threshold and our keyword list.\n",
    "\n",
    "def topic_dictionary(lda_model, lda_vectorizer, top_n = 10):\n",
    "    topic_ls = {} #append keys, append the values\n",
    "\n",
    "    for idx, topic in enumerate(lda_model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "\n",
    "        print_list = [(lda_vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        topic_ls[idx] = print_list\n",
    "\n",
    "    return topic_ls\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)\n",
    "      \n",
    "def relevant_topics(topic_dictionary, keyword_list, threshold = 0.15):\n",
    "    \"\"\"returns a list of the topics which contain a threshold % of the\n",
    "    relevant words in the keyword list\"\"\"\n",
    "    relevant_topic = []\n",
    "    for key in topic_dictionary:\n",
    "        relevant_words = 0\n",
    "        for i in range(len(topic_dictionary[key])):\n",
    "            if topic_dictionary[key][i][0] in keyword_list:\n",
    "                relevant_words += 1\n",
    "        if (relevant_words) / len(topic_dictionary[key]) >= threshold :\n",
    "            relevant_topic.append(key)\n",
    "    return relevant_topic  \n",
    "\n",
    "def rel_ent_key_list(topic_model, vectorizer, n_top_keywords, relevant_topics):\n",
    "    \"\"\"Returns a list of the top n keywords based on relative entropy score\n",
    "     Arguments:\n",
    "       topic_model (TopicModel): a topic by vocabulary word matrix where each entry\n",
    "       is the total word count for that word in that topic\n",
    "       n_top_words (int): the number of keywords the method will return\n",
    "       relevant_topics (iterable of int)\n",
    "     Returns:\n",
    "       keyword_list (iterable of str): list of the top n keywords, sorted\n",
    "     \"\"\"\n",
    "    topic_word_matrix = topic_model.components_\n",
    "    lda_vectorizer = vectorizer\n",
    "    \n",
    "    # Log of probabilities of vocab words\n",
    "    #this works\n",
    "    vocab_logs = np.log(topic_word_matrix.sum(\n",
    "        axis=0) / topic_word_matrix.sum())\n",
    "\n",
    "    # Log of probabilities of vocab words given they were in each relevant topic\n",
    "    #this is being built to calculate p(w)*log[p(w)/q(w)]\n",
    "    #this works\n",
    "    topic_logs = np.log(topic_word_matrix[relevant_topics, :].sum(\n",
    "        axis=0) / topic_word_matrix[relevant_topics, :].sum())\n",
    "\n",
    "    # relative entropy proportions, unsorted\n",
    "    #log rules: log[p(w)/q(w)] = log(p(w)) - log(q(w))\n",
    "    unsorted_props = np.asarray(topic_word_matrix.sum(axis=0) /\n",
    "                                topic_word_matrix.sum()) * np.asarray(topic_logs - vocab_logs)\n",
    "\n",
    "    unsorted_props = np.matrix.flatten(unsorted_props)\n",
    "\n",
    "    sorted_props_and_voc = sorted([(unsorted_props[i], lda_vectorizer.get_feature_names()[i]) for i in list(\n",
    "        np.argpartition(unsorted_props, len(lda_vectorizer.get_feature_names()) - n_top_keywords))[-n_top_keywords:]], reverse=True)\n",
    "    ordered_vocab = []\n",
    "    for (_, voc) in sorted_props_and_voc:\n",
    "        ordered_vocab.append(voc)\n",
    "    return ordered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the core terms from the OECD paper\n",
    "core_terms = [\"adaboost\",\"artificial intelligence\",\"artificial neural network\",\"back propagation\"\n",
    ",\"back propagation neural network\",\"computational intelligence\",\"computer vision\"\n",
    ",\"convolutional neural network\",\"deep belief network\",\"deep convolutional neural network\"\n",
    ",\"deep learn\",\"deep neural network\",\"elman network\",\"elman neural network\"\n",
    ",\"expert system\",\"fee forward neural network\",\"inference engine\",\"machine intelligence\"\n",
    ",\"machine learn\",\"machine translation\",\"machine vision\",\"multilayer neural network\"\n",
    ",\"natural language process\",\"perceptron\",\"random forest\",\"rbf neural network\",\"recurrent neural network\"\n",
    ",\"self organize map\",\"spike neural network\",\"supervise learn\",\"support vector machine\"\n",
    ",\"svm classifier\",\"unsupervised learn\",\"artificial_intelligence\",\"artificial_neural_network\",\"back_propagation\"\n",
    ",\"back_propagation_neural_network\",\"computational_intelligence\",\"computer_vision\"\n",
    ",\"convolutional_neural_network\",\"deep_belief_network\",\"deep_convolutional_neural_network\"\n",
    ",\"deep_learn\",\"deep_neural_network\",\"elman_network\",\"elman_neural_network\"\n",
    ",\"expert_system\",\"fee_forward_neural_network\",\"inference_engine\",\"machine_intelligence\"\n",
    ",\"machine_learn\",\"machine_translation\",\"machine_vision\",\"multilayer_neural_network\"\n",
    ",\"natural_language_process\",\"random_forest\",\"rbf_neural_network\",\"recurrent_neural_network\"\n",
    ",\"self_organize_map\",\"spike_neural_network\",\"supervise_learn\",\"support_vector_machine\"\n",
    ",\"svm_classifier\",\"unsupervised_learn\", \"machine_learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../data/dspg21RnD/smaller-final-dataset.pkl\")\n",
    "nsf = df[df[\"AGENCY\"] == \"NSF\"]\n",
    "# filter where cfda = 47.070\n",
    "\n",
    "nsf_csci = nsf[nsf[\"CFDA_CODE\"] == \"47.070\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nsf_csci[\"final_frqwds_removed\"]\n",
    "\n",
    "text = [] # text will contain the processed tokens in string form (1 string per abstract)\n",
    "\n",
    "\n",
    "for abstract in tokens:\n",
    "    text.append(\" \".join(abstract))\n",
    "    \n",
    "text = pd.Series(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vectorizer = CountVectorizer(max_df=0.6, min_df=20)\n",
    "\n",
    "lda_dtm = lda_vectorizer.fit_transform(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 100\n",
    "lda_model_100 = LatentDirichletAllocation(n_components=num_topics, doc_topic_prior = 1/num_topics, \n",
    "                                      topic_word_prior=0.1, n_jobs=39, random_state = 0)\n",
    "\n",
    "doc_top_dist_100 = lda_model_100.fit_transform(lda_dtm)\n",
    "top_term_dist_100 = lda_model_100.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsfcs_dic100 = topic_dictionary(lda_model_100, lda_vectorizer, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics(nsfcs_dic100, core_terms, 0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we get 5 topics when we do 1 word out of 50 ahhaha.  We only get topic 97 when we use 2 words out of 50.  I will look through these topics and add to the topics i picked out myself and decide the relevant topics, then pick out the relative entropy keyword list before making a superkeyword list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"AI\" is the 20th term.  and there are only 34 times it comes up in this topic?  Not gonna include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will keep 27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT = [27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it on my own, I picked out 19, 52, 54, 76, 79, 86, 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 87, 97]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't know about this, since it is just robot and not the other AI terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 87, 97, 19, 52, 79, 86]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics_HT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so with my judgement plus some that the relevant_topics function picked out, we have 7 topics that should be roughly about AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the relative entropy keyword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ent = rel_ent_key_list(lda_model_100, lda_vectorizer, 1000, relevant_topics_HT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the superkeyword list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"To create the super keyword list, we examine an expanded list -- the top 1000 words -- of high-relative-entropy-constribution words from the last step and select those words that are unambiguously related to the concept of interest, i.e. likely to be used when referring to the concept of interest and no other concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "superkeyword_HT = ['algorithm', 'learning', 'task', 'learn','robot','machine_learning', 'natural', 'deep', 'speech', 'robotics', 'mining', 'recognition', 'autonomous', 'language', 'train', 'ai']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, expanding this to 1000 and I select those that should be unambigiously about AI\n",
    "#rel_ent_superkeyword = rel_ent_key_list(lda_model_100, lda_vectorizer, 1000, relevant_topics_HT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the way I will do this is go through it in sets of 25 and add to \"superkeyword\" all but those that I don't think are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the filter helper to see if we can start trying to filter the corpus to get some sort of sense the abstracts that are about AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter_helper = FilterHelper(topic_model = lda_model_100,\n",
    "                                vectorizer = lda_vectorizer,\n",
    "                               relevant_topics = relevant_topics_HT,\n",
    "                               superkeywords = superkeyword_HT,\n",
    "                               keyword_list = core_terms,\n",
    "                               total_topic_prop_threshold = 0.25,\n",
    "                               keyword_prop_threshold = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16k rows because one for each document.  100 columns because 1 for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a filter_corpus function (copied from wheat_filtration package)\n",
    "def total_topic_proportion(document_topics, relevant_topics, doc_number = 0):\n",
    "    \"\"\"Return sum of relevant topic proportions for a document.\n",
    "    Arguments:\n",
    "        document_topics (iterable of float): topic proportions for one document.\n",
    "        relevant topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user.\"\"\"\n",
    "    assert (len(relevant_topics) <= len(document_topics)\n",
    "            )  # TODO make this the right kind of error\n",
    "    document = document_topics[doc_number]\n",
    "    topic_prop = 0\n",
    "    for i in relevant_topics:\n",
    "        topic_prop += document[i]    \n",
    "    return topic_prop\n",
    "\n",
    "def keyword_proportion(document, keyword_list):\n",
    "    \"\"\"Return percentage of words in the given doc that are present in keyword_list.\"\"\"\n",
    "    doc_tokens = document\n",
    "    num_keywords = sum(\n",
    "        [1 if word in keyword_list else 0 for word in doc_tokens])\n",
    "    return float(num_keywords)/len(doc_tokens)\n",
    "\n",
    "def superkeyword_presence(document, superkeywords):\n",
    "    \"\"\"Return 1 if document contains any superkeywords, 0 if not.\"\"\"\n",
    "    for word in superkeywords:\n",
    "        if word in document:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_relevant(doc, doc_topics, filter_helper, doc_number = 0):\n",
    "    \"\"\"Returns a boolean for relevance of given document. A document is considered\n",
    "    relevant if: it contains any superkeywords(filter_helper.superkeywords), passes\n",
    "    the total topic proportion threshold(filter_helper.total_topic_prop_threshold),\n",
    "    or passes the keyword proportion threshold(filter_helper.keyword_prop_threshold).\n",
    "    Arguments:\n",
    "        doc (string): preprocessed document from the corpus\n",
    "        doc_topics (iterable of float): proportion of each topic present in the given document\n",
    "        filter_helper (FilterHelper): an object containing the necessary information\n",
    "            to label the relevance of the given document\n",
    "    Returns:\n",
    "        (bool): Representing whether or not the given document is relevant according\n",
    "        to the information in filter_helper\"\"\"\n",
    "\n",
    "    has_superkeyword = superkeyword_presence(\n",
    "        doc, filter_helper.superkeywords)\n",
    "    \n",
    "    passes_total_topic_thresh = total_topic_proportion(\n",
    "        doc_topics, filter_helper.relevant_topics, doc_number) > (filter_helper.total_topic_prop_threshold)\n",
    "    \n",
    "    passes_keyword_thresh = keyword_proportion(\n",
    "        doc, filter_helper.keyword_list) > filter_helper.keyword_prop_threshold\n",
    "\n",
    "    return has_superkeyword or passes_total_topic_thresh or passes_keyword_thresh\n",
    "\n",
    "\n",
    "def filter_corpus(abstract_column, doc_topics, filter_helper):\n",
    "    subcorpus_id = []\n",
    "    for i, abstract in enumerate(abstract_column):\n",
    "        doc = abstract \n",
    "        if is_relevant(doc, doc_topics, filter_helper, doc_number = i):\n",
    "            subcorpus_id.append(i)\n",
    "    return subcorpus_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690814, 31)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scb8kw/.conda/envs/HT_Bert/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#creating a new document-topic-distribution with the full corpus\n",
    "tokens2 = df[\"final_frqwds_removed\"]\n",
    "\n",
    "fullcorpus = [] # text will contain the processed tokens in string form (1 string per abstract)\n",
    "\n",
    "\n",
    "for abstract in tokens2:\n",
    "    fullcorpus.append(\" \".join(abstract))\n",
    "    \n",
    "fullcorpus = pd.Series(fullcorpus)\n",
    "\n",
    "newdocs = fullcorpus\n",
    "new_doc_term_matrix = lda_vectorizer.transform(newdocs) \n",
    "new_doc_term_dist = lda_model_100.transform(new_doc_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.054090</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.080156</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069471</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.027441</td>\n",
       "      <td>0.046532</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.025483</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.111705</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690809</th>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.033541</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690810</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.095025</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.031832</td>\n",
       "      <td>0.058164</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690811</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690812</th>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025708</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690813</th>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.052894</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.113811</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690814 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "0       0.000238  0.000238  0.000238  0.000238  0.000238  0.000238  0.000238   \n",
       "1       0.000130  0.000130  0.000130  0.000130  0.000130  0.000130  0.000130   \n",
       "2       0.069471  0.000164  0.000164  0.000164  0.000164  0.000164  0.000164   \n",
       "3       0.000192  0.000192  0.000192  0.000192  0.000192  0.000192  0.000192   \n",
       "4       0.000476  0.000476  0.111705  0.000476  0.000476  0.000476  0.000476   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "690809  0.000115  0.000115  0.000115  0.000115  0.000115  0.000115  0.000115   \n",
       "690810  0.000092  0.000092  0.000092  0.000092  0.000092  0.000092  0.000092   \n",
       "690811  0.000090  0.000090  0.000090  0.000090  0.000090  0.000090  0.000090   \n",
       "690812  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096   \n",
       "690813  0.000116  0.000116  0.000116  0.000116  0.000116  0.000116  0.000116   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "0       0.000238  0.000238  0.000238  ...  0.000238  0.000238  0.000238   \n",
       "1       0.000130  0.000130  0.000130  ...  0.011638  0.000130  0.000130   \n",
       "2       0.000164  0.000164  0.000164  ...  0.000164  0.000164  0.000164   \n",
       "3       0.000192  0.000192  0.000192  ...  0.000192  0.000192  0.027441   \n",
       "4       0.000476  0.000476  0.000476  ...  0.000476  0.000476  0.000476   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "690809  0.000115  0.000115  0.000115  ...  0.000115  0.000115  0.000115   \n",
       "690810  0.000092  0.000092  0.000092  ...  0.000092  0.000092  0.000092   \n",
       "690811  0.000090  0.000090  0.000090  ...  0.000090  0.000090  0.000090   \n",
       "690812  0.000096  0.000096  0.000096  ...  0.025708  0.000096  0.000096   \n",
       "690813  0.000116  0.000116  0.000116  ...  0.000116  0.000116  0.000116   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "0       0.054090  0.000238  0.000238  0.000238  0.000238  0.000238  0.000238  \n",
       "1       0.080156  0.000130  0.000130  0.000130  0.000130  0.000130  0.000130  \n",
       "2       0.000164  0.000164  0.000164  0.000164  0.020310  0.000164  0.000164  \n",
       "3       0.046532  0.000192  0.025483  0.000192  0.000192  0.043263  0.000192  \n",
       "4       0.000476  0.000476  0.000476  0.000476  0.000476  0.000476  0.000476  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "690809  0.033541  0.000115  0.000115  0.000115  0.000115  0.000115  0.000115  \n",
       "690810  0.095025  0.000092  0.031832  0.058164  0.000092  0.000092  0.000092  \n",
       "690811  0.000090  0.000090  0.000090  0.000090  0.000090  0.000090  0.000090  \n",
       "690812  0.072165  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  \n",
       "690813  0.052894  0.000116  0.113811  0.000116  0.000116  0.000116  0.000116  \n",
       "\n",
       "[690814 rows x 100 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(new_doc_term_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subcorpus = filter_corpus(df[\"final_frqwds_removed\"], new_doc_term_dist, my_filter_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210907"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_subcorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yesterday, when I just used the top 100 or so relative entropy, I had 685,677 out of 690,814 as the subcorpus.  Today, when picking out just a couple superkeywords, I got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This was a pretty complicated method that we had to adapt to our problem, so there are a lot of things we are going to have to work out such as deciding what to do about lemmatization (if we want to fuzzy match), work on the sensitivity of including a corpus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "210907"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-HT_Bert]",
   "language": "python",
   "name": "conda-env-.conda-HT_Bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
