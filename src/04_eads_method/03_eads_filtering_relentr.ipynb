{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eads et al Method, using NSF subsetted corpus to cfda = 47.070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import filter\n",
    "#from git/dspg21RnD/wheat_filtration/wheat_filtration import keywords\n",
    "#from git/dspg21RnD/wheat_filtration/wheat_filtration import filter\n",
    "#import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_topic_proportion(document_topics, relevant_topics):\n",
    "    \"\"\"Return sum of relevant topic proportions for a document.\n",
    "    Arguments:\n",
    "        document_topics (iterable of float): topic proportions for one document.\n",
    "        relevant topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user.\"\"\"\n",
    "    assert (len(relevant_topics) <= len(document_topics)\n",
    "            )  # TODO make this the right kind of error\n",
    "    return sum([document_topics[i] for i in relevant_topics])\n",
    "\n",
    "\n",
    "def keyword_proportion(document, keyword_list):\n",
    "    \"\"\"Return percentage of words in the given doc that are present in keyword_list.\"\"\"\n",
    "    doc_tokens = document.split()\n",
    "    num_keywords = sum(\n",
    "        [1 if word in keyword_list else 0 for word in doc_tokens])\n",
    "    return float(num_keywords)/len(doc_tokens)\n",
    "\n",
    "\n",
    "def superkeyword_presence(document, superkeywords):\n",
    "    \"\"\"Return 1 if document contains any superkeywords, 0 if not.\"\"\"\n",
    "    for word in superkeywords:\n",
    "        if word in document.split():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class FilterHelper():\n",
    "    \"\"\"Creates a filter object containing filter criteria such as keyword list,\n",
    "    superkeyword list, total topic proportion threshold, and keyword proportion\n",
    "    threshold.\n",
    "\n",
    "    Arguments:\n",
    "        topic_model (TopicModel): a TopicModel object instantiated with a corpus or\n",
    "            files from a Mallet topic model.\n",
    "        relevant_topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user. Note that the number\n",
    "            corresponding with the first topic is '0', the second topic is '1', etc.\n",
    "        n_keywords: number of keywords to include in keyword list. Default is 20.\n",
    "        superkeywords (iterable of str): a list of keywords which signify immediate relevance\n",
    "            of the document that contains them (better wording). Default is an empty list.\n",
    "        keyword_list: A list of keywords ordered by [the relevance they signify]. Default is\n",
    "            a keyword list generated using the relative entropy method.\n",
    "        total_topic_prop_threshold (float): the threshold of relevance for the total proportion\n",
    "            of relevant topics in a document. If a document surpases the threshold, it is considered relevant.\n",
    "        keyword_prop_threshold (float): the threshold of relevance for the proportion of words\n",
    "            on the keyword list that appear in a document. If a document surpases the threshold,\n",
    "            it is considered relevant.\n",
    "\n",
    "    Attributes:\n",
    "        topic_model (TopicModel): a TopicModel object instantiated with a corpus or\n",
    "            files from a Mallet topic model.\n",
    "        relevant_topics (iterable of int): a list of the numbers corresponding\n",
    "            with the topics considered relevant by the user.\n",
    "        superkeywords (iterable of str): a list of keywords which signify immediate relevance\n",
    "            of the document that contains them (better wording). Default is an empty list.\n",
    "        keyword_list: A list of keywords ordered by [the relevance they signify]. Default is\n",
    "            a keyword list generated using the relative entropy method.\n",
    "        total_topic_prop_threshold (float): the threshold of relevance for the total proportion\n",
    "            of relevant topics in a document. If a document surpases the threshold, \n",
    "            it is considered relevant. Default is 0.25.\n",
    "        keyword_prop_threshold (float): the threshold of relevance for the proportion of words\n",
    "            on the keyword list that appear in a document. If a document surpases the threshold,\n",
    "            it is considered relevant. Default is 0.15.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: if user enters both keyword list and n_keywords when using the\n",
    "        keyword_list setter method.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, topic_model, vectorizer, relevant_topics, keyword_list=None, n_keywords=100, superkeywords=[],\n",
    "                 total_topic_prop_threshold=0.25, keyword_prop_threshold=0.15):\n",
    "        self._relevant_topics = relevant_topics\n",
    "        if keyword_list is None:\n",
    "            keyword_list = keywords.rel_ent_key_list(\n",
    "                topic_model, n_keywords, relevant_topics)\n",
    "        self._keyword_list = keyword_list\n",
    "\n",
    "        lower_superkeys = [word.lower() for word in superkeywords]\n",
    "        # TODO: deal with this appropriately when making lowercasing optional\n",
    "        extended_superkeys = [\n",
    "            word for word in vectorizer.get_feature_names() if\n",
    "            word in lower_superkeys or\n",
    "            any([(chunk in lower_superkeys) for chunk in word.split('_')])\n",
    "        ]\n",
    "        self._superkeywords = extended_superkeys\n",
    "\n",
    "        self._total_topic_prop_threshold = total_topic_prop_threshold\n",
    "        self._keyword_prop_threshold = keyword_prop_threshold\n",
    "        self._topic_model = topic_model\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "    @property\n",
    "    def topic_model(self):\n",
    "        \"\"\"Get topic_model used to create filter\"\"\"\n",
    "        return self._topic_model\n",
    "\n",
    "    @property\n",
    "    def relevant_topics(self):\n",
    "        \"\"\"Get list of relevant topics\"\"\"\n",
    "        return self._relevant_topics\n",
    "\n",
    "    @property\n",
    "    def keyword_list(self):\n",
    "        \"\"\"Get or set keyword list. Input either a list of keywords, or input an integer n\n",
    "        to generate a keyword list containing n words.\"\"\"\n",
    "        return self._keyword_list\n",
    "\n",
    "    @keyword_list.setter\n",
    "    def keyword_list(self, keyword_list=None, n_keywords=None):\n",
    "        if keyword_list is not None:\n",
    "            self._keyword_list = keyword_list\n",
    "        elif n_keywords is not None:\n",
    "            self._keyword_list = keywords.rel_ent_key_list(\n",
    "                self.topic_model, n_keywords, self.relevant_topics)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Enter either a keyword list or an integer for number of keywords\")\n",
    "\n",
    "    @property\n",
    "    def superkeywords(self):\n",
    "        return self._superkeywords\n",
    "\n",
    "    @superkeywords.setter\n",
    "    def superkeywords(self, superkeywords):\n",
    "        self._superkeywords = superkeywords\n",
    "\n",
    "    @property\n",
    "    def total_topic_prop_threshold(self):\n",
    "        return self._total_topic_prop_threshold\n",
    "\n",
    "    @total_topic_prop_threshold.setter\n",
    "    def total_topic_prop_threshold(self, total_topic_prop_threshold):\n",
    "        self._total_topic_prop_threshold = total_topic_prop_threshold\n",
    "\n",
    "    @property\n",
    "    def keyword_prop_threshold(self):\n",
    "        return self._keyword_prop_threshold\n",
    "\n",
    "    @keyword_prop_threshold.setter\n",
    "    def keyword_prop_threshold(self, keyword_prop_threshold):\n",
    "        self._keyword_prop_threshold = keyword_prop_threshold\n",
    "\n",
    "\n",
    "def is_relevant(doc, doc_topics, filter_helper):\n",
    "    \"\"\"Returns a boolean for relevance of given document. A document is considered\n",
    "    relevant if: it contains any superkeywords(filter_helper.superkeywords), passes\n",
    "    the total topic proportion threshold(filter_helper.total_topic_prop_threshold),\n",
    "    or passes the keyword proportion threshold(filter_helper.keyword_prop_threshold).\n",
    "    Arguments:\n",
    "        doc (string): preprocessed document from the corpus\n",
    "        doc_topics (iterable of float): proportion of each topic present in the given document\n",
    "        filter_helper (FilterHelper): an object containing the necessary information\n",
    "            to label the relevance of the given document\n",
    "    Returns:\n",
    "        (bool): Representing whether or not the given document is relevant according\n",
    "        to the information in filter_helper\"\"\"\n",
    "\n",
    "    has_superkeyword = superkeyword_presence(\n",
    "        doc, filter_helper.superkeywords)\n",
    "    passes_total_topic_thresh = total_topic_proportion(\n",
    "        doc_topics, filter_helper.relevant_topics) > filter_helper.total_topic_prop_threshold\n",
    "    passes_keyword_thresh = keyword_proportion(\n",
    "        doc, filter_helper.keyword_list) > filter_helper.keyword_prop_threshold\n",
    "\n",
    "    return has_superkeyword or passes_total_topic_thresh or passes_keyword_thresh\n",
    "\n",
    "\n",
    "def filter_corpus(topic_model, filter_helper):\n",
    "    \"\"\"Filters corpus used to make topic_model according to criteria entered in filter_helper.\n",
    "    Arguments:\n",
    "        topic_model (TopicModel): a TopicModel object instantiated with a corpus or\n",
    "        files from a Mallet topic model.\n",
    "        filter_helper (FilterHelper): a FilterHelper object instantiated with filter\n",
    "        properties.\n",
    "    Returns:\n",
    "        subcorpus (dict): a dictionary containing the subset of the corpus that passed\n",
    "        the relevance filter. keys are the unique document ids and values are the (unprocessed)\n",
    "        document text\"\"\"\n",
    "    subcorpus = {}\n",
    "    for i, doc_id in enumerate(topic_model.docs):\n",
    "        doc = topic_model.docs[doc_id]\n",
    "        doc_topics = topic_model.doc_topic_proportions[i, :]\n",
    "        if is_relevant(doc, doc_topics, filter_helper):\n",
    "            # add full document to subcorpus as <doc_id>: <doc_body>\n",
    "            subcorpus[doc_id] = topic_model.full_docs[doc_id]\n",
    "    return subcorpus\n",
    "\n",
    "#####################################################\n",
    "######### under this line are things it would be nice to add later #############\n",
    "# TODO (faunam|6/20/19): implement\n",
    "\n",
    "\n",
    "def proportion_lists():\n",
    "    \"\"\"makes a matrix or list of ttp, superkeyword, and keyword proportion for the docs in corpus\n",
    "    and sets the respective topic model attributes\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def subset_quality(threshs, labeled_subset):  # also had args word_list_gen and scorefun\n",
    "    \"\"\"Calculate F1 score for the array of thresholds threshs\n",
    "    (max topic prop, total topic prop, vocab prop, and number of words\n",
    "    in vocabulary list) on labeled subset\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def subset_info(threshs):  # seems like a cool feature to include\n",
    "    \"\"\"Return set of false positives, true positives, false negatives, and true negatives, as\n",
    "    well as the sizes of the false neg and false pos sets, as well as the size of set\n",
    "    predicted as relevant, about the subset created by the given set of thresholds\n",
    "    (mtp, ttp, voc prop, and voc list length, in that order).\n",
    "    This function can be edited to output any kind of info about the subset, eg the filenames.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for creating a topic dictionary, viewing the topics in the topic model,\n",
    "#and selecting only the relevant topics based on a threshold and our keyword list.\n",
    "\n",
    "\n",
    "def topic_dictionary(lda_model, lda_vectorizer, top_n = 10):\n",
    "    topic_ls = {} #append keys, append the values\n",
    "    \n",
    "    \n",
    "    for idx, topic in enumerate(lda_model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "\n",
    "        print_list = [(lda_vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        topic_ls[idx] = print_list\n",
    "\n",
    "    return topic_ls\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)\n",
    "            \n",
    "            \n",
    "def relevant_topics(topic_dictionary, keyword_list, threshold = 0.15):\n",
    "    \"\"\"returns a list of the topics which contain a threshold % of the\n",
    "    relevant words in the keyword list\"\"\"\n",
    "    relevant_topic = []\n",
    "    for key in topic_dictionary:\n",
    "        relevant_words = 0\n",
    "        for i in range(len(topic_dictionary[key])):\n",
    "            if topic_dictionary[key][i][0] in keyword_list:\n",
    "                relevant_words += 1\n",
    "        if (relevant_words) / len(topic_dictionary[key]) >= threshold :\n",
    "            relevant_topic.append(key)\n",
    "    return relevant_topic  \n",
    "\n",
    "def rel_ent_key_list(topic_model, vectorizer, n_top_keywords, relevant_topics):\n",
    "    \"\"\"Returns a list of the top n keywords based on relative entropy score\n",
    "     Arguments:\n",
    "       topic_model (TopicModel): a topic by vocabulary word matrix where each entry\n",
    "       is the total word count for that word in that topic\n",
    "       n_top_words (int): the number of keywords the method will return\n",
    "       relevant_topics (iterable of int)\n",
    "     Returns:\n",
    "       keyword_list (iterable of str): list of the top n keywords, sorted\n",
    "     \"\"\"\n",
    "    topic_word_matrix = topic_model.components_\n",
    "    lda_vectorizer = vectorizer\n",
    "    \n",
    "    # Log of probabilities of vocab words\n",
    "    #this works\n",
    "    vocab_logs = np.log(topic_word_matrix.sum(\n",
    "        axis=0) / topic_word_matrix.sum())\n",
    "\n",
    "    # Log of probabilities of vocab words given they were in each relevant topic\n",
    "    #this is being built to calculate p(w)*log[p(w)/q(w)]\n",
    "    #this works\n",
    "    topic_logs = np.log(topic_word_matrix[relevant_topics, :].sum(\n",
    "        axis=0) / topic_word_matrix[relevant_topics, :].sum())\n",
    "\n",
    "    # relative entropy proportions, unsorted\n",
    "    #log rules: log[p(w)/q(w)] = log(p(w)) - log(q(w))\n",
    "    unsorted_props = np.asarray(topic_word_matrix.sum(axis=0) /\n",
    "                                topic_word_matrix.sum()) * np.asarray(topic_logs - vocab_logs)\n",
    "\n",
    "    unsorted_props = np.matrix.flatten(unsorted_props)\n",
    "\n",
    "    sorted_props_and_voc = sorted([(unsorted_props[i], lda_vectorizer.get_feature_names()[i]) for i in list(\n",
    "        np.argpartition(unsorted_props, len(lda_vectorizer.get_feature_names()) - n_top_keywords))[-n_top_keywords:]], reverse=True)\n",
    "    ordered_vocab = []\n",
    "    for (_, voc) in sorted_props_and_voc:\n",
    "        ordered_vocab.append(voc)\n",
    "    return ordered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the core terms from the OECD paper\n",
    "core_terms = [\"adaboost\",\"artificial intelligence\",\"artificial neural network\",\"back propagation\"\n",
    ",\"back propagation neural network\",\"computational intelligence\",\"computer vision\"\n",
    ",\"convolutional neural network\",\"deep belief network\",\"deep convolutional neural network\"\n",
    ",\"deep learn\",\"deep neural network\",\"elman network\",\"elman neural network\"\n",
    ",\"expert system\",\"fee forward neural network\",\"inference engine\",\"machine intelligence\"\n",
    ",\"machine learn\",\"machine translation\",\"machine vision\",\"multilayer neural network\"\n",
    ",\"natural language process\",\"perceptron\",\"random forest\",\"rbf neural network\",\"recurrent neural network\"\n",
    ",\"self organize map\",\"spike neural network\",\"supervise learn\",\"support vector machine\"\n",
    ",\"svm classifier\",\"unsupervised learn\",\"artificial_intelligence\",\"artificial_neural_network\",\"back_propagation\"\n",
    ",\"back_propagation_neural_network\",\"computational_intelligence\",\"computer_vision\"\n",
    ",\"convolutional_neural_network\",\"deep_belief_network\",\"deep_convolutional_neural_network\"\n",
    ",\"deep_learn\",\"deep_neural_network\",\"elman_network\",\"elman_neural_network\"\n",
    ",\"expert_system\",\"fee_forward_neural_network\",\"inference_engine\",\"machine_intelligence\"\n",
    ",\"machine_learn\",\"machine_translation\",\"machine_vision\",\"multilayer_neural_network\"\n",
    ",\"natural_language_process\",\"random_forest\",\"rbf_neural_network\",\"recurrent_neural_network\"\n",
    ",\"self_organize_map\",\"spike_neural_network\",\"supervise_learn\",\"support_vector_machine\"\n",
    ",\"svm_classifier\",\"unsupervised_learn\", \"machine_learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../data/dspg21RnD/smaller-final-dataset.pkl\")\n",
    "nsf = df[df[\"AGENCY\"] == \"NSF\"]\n",
    "# filter where cfda = 47.070\n",
    "\n",
    "nsf_csci = nsf[nsf[\"CFDA_CODE\"] == \"47.070\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original index</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>FY</th>\n",
       "      <th>ORG_COUNT</th>\n",
       "      <th>PI_COUNT</th>\n",
       "      <th>nchar</th>\n",
       "      <th>final_frqwds_removed</th>\n",
       "      <th>PROJECT_TERMS</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>...</th>\n",
       "      <th>ORGANIZATION_CITY</th>\n",
       "      <th>ORGANIZATION_STATE</th>\n",
       "      <th>ORGANIZATION_ZIP</th>\n",
       "      <th>ORGANIZATION_COUNTRY</th>\n",
       "      <th>BUDGET_START_DATE</th>\n",
       "      <th>BUDGET_END_DATE</th>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <th>FY.y</th>\n",
       "      <th>FY_TOTAL_COST</th>\n",
       "      <th>FY_TOTAL_COST_SUB_PROJECTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>11849</td>\n",
       "      <td>101844</td>\n",
       "      <td>This symposium is a premiere forum for researc...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>697</td>\n",
       "      <td>[symposium, premiere, forum, interaction, comp...</td>\n",
       "      <td>Arts; Award; Collaborations; Communities; Com...</td>\n",
       "      <td>SUPPORT FOR THE THIRTEENTH INTERNATIONAL CONFE...</td>\n",
       "      <td>...</td>\n",
       "      <td>CHAMPAIGN</td>\n",
       "      <td>IL</td>\n",
       "      <td>61820-7406</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2008</td>\n",
       "      <td>4651.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>11747</td>\n",
       "      <td>101739</td>\n",
       "      <td>This award is to the Computer Research Associa...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2638</td>\n",
       "      <td>[award, computer, association, cra, coordinate...</td>\n",
       "      <td>Address; Architecture; Award; base; Collabora...</td>\n",
       "      <td>THE 2007 FIND PIS MEETING</td>\n",
       "      <td>...</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>DC</td>\n",
       "      <td>20036-0000</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2008</td>\n",
       "      <td>29940.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>12250</td>\n",
       "      <td>102249</td>\n",
       "      <td>IIS-0808994PI: Jonathan FurnerUniversity of Ca...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1682</td>\n",
       "      <td>[iis, pi, furneruniversity, california_los, an...</td>\n",
       "      <td>Arts; Award; California; Development; Dimensi...</td>\n",
       "      <td>WORKSHOP: I-CONFERENCE DOCTORAL RESEARCH COLLO...</td>\n",
       "      <td>...</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>90095-1406</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2008</td>\n",
       "      <td>25859.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>12405</td>\n",
       "      <td>102404</td>\n",
       "      <td>ABSTRACT0812795Vijay K. VaishnaviGa State U Re...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>574</td>\n",
       "      <td>[vaishnaviga, res, fdnthis, seek, funding, enc...</td>\n",
       "      <td>computer science; design; Discipline; Funding...</td>\n",
       "      <td>STUDENT PARTICIPATION IN 3RD INTERNATIONAL CON...</td>\n",
       "      <td>...</td>\n",
       "      <td>ATLANTA</td>\n",
       "      <td>GA</td>\n",
       "      <td>30303-3999</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2008</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>12451</td>\n",
       "      <td>102448</td>\n",
       "      <td>The 3rd International Conference on emerging N...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3025</td>\n",
       "      <td>[3rd, international, conference, emerge, netwo...</td>\n",
       "      <td>Award; base; career; Commit; Communication; C...</td>\n",
       "      <td>STUDENT TRAVEL SUPPORT FOR CONEXT 2007 CONFERENCE</td>\n",
       "      <td>...</td>\n",
       "      <td>MADISON</td>\n",
       "      <td>WI</td>\n",
       "      <td>53715-1218</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.070</td>\n",
       "      <td>2008</td>\n",
       "      <td>10875.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      original index PROJECT_ID  \\\n",
       "1996           11849     101844   \n",
       "2067           11747     101739   \n",
       "2187           12250     102249   \n",
       "2234           12405     102404   \n",
       "2235           12451     102448   \n",
       "\n",
       "                                               ABSTRACT    FY  ORG_COUNT  \\\n",
       "1996  This symposium is a premiere forum for researc...  2008          1   \n",
       "2067  This award is to the Computer Research Associa...  2008          1   \n",
       "2187  IIS-0808994PI: Jonathan FurnerUniversity of Ca...  2008          1   \n",
       "2234  ABSTRACT0812795Vijay K. VaishnaviGa State U Re...  2008          1   \n",
       "2235  The 3rd International Conference on emerging N...  2008          1   \n",
       "\n",
       "      PI_COUNT  nchar                               final_frqwds_removed  \\\n",
       "1996         1    697  [symposium, premiere, forum, interaction, comp...   \n",
       "2067         1   2638  [award, computer, association, cra, coordinate...   \n",
       "2187         1   1682  [iis, pi, furneruniversity, california_los, an...   \n",
       "2234         1    574  [vaishnaviga, res, fdnthis, seek, funding, enc...   \n",
       "2235         1   3025  [3rd, international, conference, emerge, netwo...   \n",
       "\n",
       "                                          PROJECT_TERMS  \\\n",
       "1996   Arts; Award; Collaborations; Communities; Com...   \n",
       "2067   Address; Architecture; Award; base; Collabora...   \n",
       "2187   Arts; Award; California; Development; Dimensi...   \n",
       "2234   computer science; design; Discipline; Funding...   \n",
       "2235   Award; base; career; Commit; Communication; C...   \n",
       "\n",
       "                                          PROJECT_TITLE  ...  \\\n",
       "1996  SUPPORT FOR THE THIRTEENTH INTERNATIONAL CONFE...  ...   \n",
       "2067                          THE 2007 FIND PIS MEETING  ...   \n",
       "2187  WORKSHOP: I-CONFERENCE DOCTORAL RESEARCH COLLO...  ...   \n",
       "2234  STUDENT PARTICIPATION IN 3RD INTERNATIONAL CON...  ...   \n",
       "2235  STUDENT TRAVEL SUPPORT FOR CONEXT 2007 CONFERENCE  ...   \n",
       "\n",
       "     ORGANIZATION_CITY ORGANIZATION_STATE ORGANIZATION_ZIP  \\\n",
       "1996         CHAMPAIGN                 IL       61820-7406   \n",
       "2067        WASHINGTON                 DC       20036-0000   \n",
       "2187       LOS ANGELES                 CA       90095-1406   \n",
       "2234           ATLANTA                 GA       30303-3999   \n",
       "2235           MADISON                 WI       53715-1218   \n",
       "\n",
       "     ORGANIZATION_COUNTRY BUDGET_START_DATE BUDGET_END_DATE CFDA_CODE  FY.y  \\\n",
       "1996        UNITED STATES               NaN             NaN    47.070  2008   \n",
       "2067        UNITED STATES               NaN             NaN    47.070  2008   \n",
       "2187        UNITED STATES               NaN             NaN    47.070  2008   \n",
       "2234        UNITED STATES               NaN             NaN    47.070  2008   \n",
       "2235        UNITED STATES               NaN             NaN    47.070  2008   \n",
       "\n",
       "     FY_TOTAL_COST FY_TOTAL_COST_SUB_PROJECTS  \n",
       "1996        4651.0                        NaN  \n",
       "2067       29940.0                        NaN  \n",
       "2187       25859.0                        NaN  \n",
       "2234        9000.0                        NaN  \n",
       "2235       10875.0                        NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsf_csci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nsf_csci[\"final_frqwds_removed\"]\n",
    "\n",
    "text = [] # text will contain the processed tokens in string form (1 string per abstract)\n",
    "\n",
    "\n",
    "for abstract in tokens:\n",
    "    text.append(\" \".join(abstract))\n",
    "    \n",
    "text = pd.Series(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY TOPIC MODELING WITH LDA\n",
    "\n",
    "lda_vectorizer = CountVectorizer(max_df=0.6, min_df=20)\n",
    "#this is our way to filter out words that don't appear enough, and those that appear way too often (we want the middle set of terms)\n",
    "#^this filters the size of our matrix\n",
    "\n",
    "lda_dtm = lda_vectorizer.fit_transform(text)\n",
    "#text = our abstract text, right in the dataframe\n",
    "#fits our doc-term matrix to our specific text \n",
    "#this is the standard for scikit-learn\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 100\n",
    "lda_model_100 = LatentDirichletAllocation(n_components=num_topics, doc_topic_prior = 1/num_topics, \n",
    "                                      topic_word_prior=0.1, n_jobs=39, random_state = 0)\n",
    "\n",
    "doc_top_dist_100 = lda_model_100.fit_transform(lda_dtm)\n",
    "top_term_dist_100 = lda_model_100.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsfcs_dic100 = topic_dictionary(lda_model_100, lda_vectorizer, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics(nsfcs_dic100, core_terms, 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we get 5 topics when we do 1 word out of 50 ahhaha.  We only get topic 97 when we use 2 words out of 50.  I will look through these topics and add to the topics i picked out myself and decide the relevant topics, then pick out the relative entropy keyword list before making a superkeyword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('health', 2116.0966517857764),\n",
       " ('patient', 1244.7450881968607),\n",
       " ('care', 819.0339929744473),\n",
       " ('medical', 676.7615454630422),\n",
       " ('healthcare', 623.2943716504553),\n",
       " ('clinical', 556.9735215587403),\n",
       " ('technology', 223.57570780885894),\n",
       " ('individual', 210.91677664812553),\n",
       " ('cost', 207.34347137206652),\n",
       " ('population', 190.37128252654986),\n",
       " ('hospital', 184.70507724109828),\n",
       " ('medicine', 181.99000710864803),\n",
       " ('record', 177.60667439014955),\n",
       " ('personalize', 171.30428620903686),\n",
       " ('family', 170.9999364264955),\n",
       " ('challenge', 165.7542634300328),\n",
       " ('need', 160.33314127369772),\n",
       " ('behavioral', 150.11844464162309),\n",
       " ('electronic', 131.96048414061747),\n",
       " ('monitoring', 131.77603124067357)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[15][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('search', 1368.653014681624),\n",
       " ('feature', 588.7102878122361),\n",
       " ('extraction', 153.1498066330481),\n",
       " ('heuristic', 123.9245757396409),\n",
       " ('space', 75.89387486559578),\n",
       " ('chinese', 72.84054190082773),\n",
       " ('selection', 70.46146991527411),\n",
       " ('proxy', 51.84053060678479),\n",
       " ('classification', 50.7655025843738),\n",
       " ('patent', 47.71687693737752),\n",
       " ('china', 46.84252098056582),\n",
       " ('relevant', 45.849468757338485),\n",
       " ('narrative', 44.00343995713097),\n",
       " ('combinatorial', 42.86278148086518),\n",
       " ('case', 40.83760602745175),\n",
       " ('exploration', 39.40466537566612),\n",
       " ('select', 39.33212718844669),\n",
       " ('probe', 36.31510692372034),\n",
       " ('aaai', 34.8912867766287),\n",
       " ('ai', 34.75250071433617)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[23][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"AI\" is the 20th term.  and there are only 34 times it comes up in this topic?  Not gonna include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decision', 1211.7633387852838),\n",
       " ('domain', 324.96557996466885),\n",
       " ('label', 260.52331656824236),\n",
       " ('example', 221.35194514993535),\n",
       " ('machine_learning', 217.52270449024778),\n",
       " ('learn', 201.98552761463682),\n",
       " ('uncertainty', 198.61946907103538),\n",
       " ('classifier', 189.4280778482446),\n",
       " ('ml', 174.54927586033412),\n",
       " ('reasoning', 161.5635472118136),\n",
       " ('algorithm', 141.7574753819408),\n",
       " ('real', 128.6539416936688),\n",
       " ('task', 123.45169384502242),\n",
       " ('explanation', 104.5974839087037),\n",
       " ('automated', 103.00937330998413),\n",
       " ('framework', 99.95478220465216),\n",
       " ('robust', 99.6318830974613),\n",
       " ('human', 99.45204627261225),\n",
       " ('source', 94.48550058915484),\n",
       " ('uncertain', 93.46741756910629)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[28][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will keep 27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT = [27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('algorithm', 1819.06996660863),\n",
       " ('machine_learning', 1059.3157087702514),\n",
       " ('learn', 866.006523246125),\n",
       " ('inference', 809.5514827848334),\n",
       " ('statistical', 733.4181344556185),\n",
       " ('learning', 732.3256051256889),\n",
       " ('scale', 566.1333020691231),\n",
       " ('dataset', 564.3152416367084),\n",
       " ('dimensional', 562.5206095408722),\n",
       " ('set', 557.742088611085),\n",
       " ('framework', 482.4322917225055),\n",
       " ('big', 417.2567183957516),\n",
       " ('clustering', 323.7412237170666),\n",
       " ('sparse', 316.4792781225677),\n",
       " ('efficient', 313.0336721213797),\n",
       " ('massive', 311.97348253810674),\n",
       " ('challenge', 311.1389008875552),\n",
       " ('estimation', 280.519515354007),\n",
       " ('representation', 278.6236354818512),\n",
       " ('machine', 263.5076627080198)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[87][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning', 1753.425587928131),\n",
       " ('deep', 1206.3295947990032),\n",
       " ('learn', 977.393948584993),\n",
       " ('machine_learning', 449.65095379314283),\n",
       " ('neural', 411.89155081104036),\n",
       " ('ai', 273.8011831510943),\n",
       " ('machine', 266.3093513909305),\n",
       " ('intelligence', 210.80992532602272),\n",
       " ('intelligent', 199.38004638266955),\n",
       " ('network', 196.9212463435522),\n",
       " ('algorithm', 192.03032816490338),\n",
       " ('artificial_intelligence', 167.46230118906087),\n",
       " ('reinforcement', 167.4450670234159),\n",
       " ('architecture', 166.82888205136672),\n",
       " ('advance', 118.04581497835483),\n",
       " ('vision', 107.12164855307428),\n",
       " ('world', 106.54101575482942),\n",
       " ('domain', 102.06689086441504),\n",
       " ('train', 99.33526221413456),\n",
       " ('real', 95.65366884119737)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[97][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it on my own, I picked out 19, 52, 54, 76, 79, 86, 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 87, 97]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('graph', 2462.2972841025025),\n",
       " ('mining', 660.3066246687682),\n",
       " ('algorithm', 559.8030180085358),\n",
       " ('pattern', 289.113236307129),\n",
       " ('edge', 227.46269296479247),\n",
       " ('real', 197.20648487656408),\n",
       " ('domain', 137.2843781269611),\n",
       " ('anomaly', 134.39008641464014),\n",
       " ('social', 134.07527965190465),\n",
       " ('analytics', 130.93391842624442),\n",
       " ('network', 129.21605419748968),\n",
       " ('irregular', 127.48866737124281),\n",
       " ('node', 119.53130373091832),\n",
       " ('world', 108.44226220914099),\n",
       " ('processing', 105.90993203052052),\n",
       " ('represent', 105.57140975731879),\n",
       " ('efficient', 105.02798903145188),\n",
       " ('scale', 103.15140867694929),\n",
       " ('dynamic', 102.46176242573956),\n",
       " ('scalable', 91.12291265802436)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[19][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('task', 1667.5998674114032),\n",
       " ('robot', 1205.2733479871963),\n",
       " ('human', 667.5030619334059),\n",
       " ('planning', 651.9088761238436),\n",
       " ('motion', 547.4441514047675),\n",
       " ('action', 545.034947422538),\n",
       " ('autonomous', 524.3019424622092),\n",
       " ('manipulation', 508.5319744841163),\n",
       " ('environment', 507.2456930949452),\n",
       " ('robotic', 475.71316628732137),\n",
       " ('algorithm', 413.94966228240946),\n",
       " ('enable', 288.71012963912256),\n",
       " ('robotics', 283.509516724759),\n",
       " ('agent', 279.35572225212815),\n",
       " ('learn', 228.39561697061572),\n",
       " ('level', 190.82268774181435),\n",
       " ('uncertainty', 189.2610487214873),\n",
       " ('capability', 171.53116752407882),\n",
       " ('framework', 159.42105370299586),\n",
       " ('domain', 156.37989153139853)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[52][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('robot', 400.84901342166),\n",
       " ('motor', 286.6005205219227),\n",
       " ('hand', 285.2745892318118),\n",
       " ('human', 223.16141139120617),\n",
       " ('robotic', 218.12729661842502),\n",
       " ('movement', 212.42065577045534),\n",
       " ('force', 210.76716086016995),\n",
       " ('arm', 173.53051174420514),\n",
       " ('environment', 145.77342117895245),\n",
       " ('robotics', 118.73722282530731),\n",
       " ('assistive', 116.94376450869532),\n",
       " ('pis', 115.4334127330146),\n",
       " ('object', 114.63801914889936),\n",
       " ('walk', 110.31475543395928),\n",
       " ('gait', 107.15655344429868),\n",
       " ('contact', 106.36771297027417),\n",
       " ('motion', 105.5398986508488),\n",
       " ('task', 102.95519664082595),\n",
       " ('exoskeleton', 102.08365972980206),\n",
       " ('prosthesis', 101.07030785125205)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[54][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't know about this, since it is just robot and not the other AI terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('algorithm', 1240.1942831168114),\n",
       " ('computational', 747.3377107663017),\n",
       " ('solution', 602.5737568888037),\n",
       " ('numerical', 435.97774869387007),\n",
       " ('optimization', 415.7614482832579),\n",
       " ('solving', 403.99019929766405),\n",
       " ('linear', 384.42221155603283),\n",
       " ('computer', 333.66799399344507),\n",
       " ('solve', 333.16176886312076),\n",
       " ('science', 333.07163175650385),\n",
       " ('mathematical', 280.08281954069486),\n",
       " ('engineering', 266.6018952432416),\n",
       " ('solver', 251.9626318867719),\n",
       " ('matrix', 243.3638556521591),\n",
       " ('equation', 234.6849094327548),\n",
       " ('mathematics', 231.58240590608483),\n",
       " ('uas', 211.099695064392),\n",
       " ('apply', 198.6032935523939),\n",
       " ('pis', 173.8920765561421),\n",
       " ('real', 173.7369329425362)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[76][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', 2144.887478365342),\n",
       " ('speech', 1131.1117051254016),\n",
       " ('natural', 599.6745213872944),\n",
       " ('processing', 479.1224783524312),\n",
       " ('linguistic', 447.9412757359252),\n",
       " ('text', 444.0013130334293),\n",
       " ('recognition', 415.10750001393774),\n",
       " ('annotation', 323.56805380072103),\n",
       " ('translation', 299.85525376117687),\n",
       " ('human', 271.64202926108186),\n",
       " ('automatic', 261.98584160629355),\n",
       " ('word', 260.42863801789616),\n",
       " ('speaker', 236.27280804289842),\n",
       " ('audio', 227.33951009200655),\n",
       " ('technology', 208.12115178435536),\n",
       " ('nlp', 203.06825159780698),\n",
       " ('machine', 200.47687588575002),\n",
       " ('corpus', 186.28160800955476),\n",
       " ('computational', 171.36350838667278),\n",
       " ('english', 169.86888666804097)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[79][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('human', 3485.8322547179414),\n",
       " ('robot', 1777.589081373325),\n",
       " ('interaction', 699.6492100943879),\n",
       " ('robotics', 657.2793637927633),\n",
       " ('environment', 393.39388202687496),\n",
       " ('team', 382.00487124254914),\n",
       " ('people', 218.92423623762568),\n",
       " ('machine', 207.63928741334777),\n",
       " ('task', 203.30837221711272),\n",
       " ('cognitive', 189.08865713300628),\n",
       " ('operator', 176.57447472975485),\n",
       " ('interact', 165.88041193385916),\n",
       " ('intelligent', 151.45301088124222),\n",
       " ('robotic', 149.51433987674383),\n",
       " ('collaborative', 140.59228383853102),\n",
       " ('hri', 126.5535598340643),\n",
       " ('worker', 114.59998404856047),\n",
       " ('physical', 113.6439584131836),\n",
       " ('real', 108.76366521289732),\n",
       " ('integrate', 106.22562654370202)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsfcs_dic100[86][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_topics_HT.append(86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 87, 97, 19, 52, 79, 86]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics_HT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so with my judgement plus some that the relevant_topics function picked out, we have 7 topics that should be roughly about AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the relative entropy keyword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ent = rel_ent_key_list(lda_model_100, lda_vectorizer, 1000, relevant_topics_HT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stimulation',\n",
       " 'imply',\n",
       " 'difficult_impossible',\n",
       " 'schema',\n",
       " 'jointly',\n",
       " 'constituent',\n",
       " 'overlap',\n",
       " 'conform',\n",
       " 'planetary',\n",
       " 'sorting',\n",
       " 'agriculture',\n",
       " 'daunting_task',\n",
       " 'stop',\n",
       " 'aged',\n",
       " 'upcoming',\n",
       " 'reflective',\n",
       " 'university_texas_dallas',\n",
       " 'retain',\n",
       " 'restriction',\n",
       " 'heavily',\n",
       " 'sufficiently',\n",
       " 'anchor',\n",
       " 'available',\n",
       " 'opinion',\n",
       " 'leap',\n",
       " 'heighten',\n",
       " 'fraud',\n",
       " 'rapidly',\n",
       " 'essentially',\n",
       " 'credible',\n",
       " 'impression',\n",
       " 'personality',\n",
       " 'advantage',\n",
       " 'publicly',\n",
       " 'face',\n",
       " 'subject',\n",
       " 'queueing',\n",
       " 'complicated',\n",
       " 'precursor',\n",
       " 'milestone',\n",
       " 'difference',\n",
       " 'instantiate',\n",
       " 'escape',\n",
       " 'retail',\n",
       " 'inter_related',\n",
       " 'season',\n",
       " 'glean',\n",
       " 'giant',\n",
       " 'remote',\n",
       " 'fortunately']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#relative entropy keywork list\n",
    "rel_ent[950:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the superkeyword list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"To create the super keyword list, we examine an expanded list -- the top 1000 words -- of high-relative-entropy-constribution words from the last step and select those words that are unambiguously related to the concept of interest, i.e. likely to be used when referring to the concept of interest and no other concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, expanding this to 1000 and I select those that should be unambigiously about AI\n",
    "rel_ent_superkeyword = rel_ent_key_list(lda_model_100, lda_vectorizer, 1000, relevant_topics_HT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the way I will do this is go through it in sets of 25 and add to \"superkeyword\" all but those that I don't think are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "superkeyword = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 human\n",
      "1 algorithm\n",
      "2 learning\n",
      "3 task\n",
      "4 learn\n",
      "5 robot\n",
      "6 language\n",
      "7 graph\n",
      "8 machine_learning\n",
      "9 processing\n",
      "10 environment\n",
      "11 machine\n",
      "12 domain\n",
      "13 interaction\n",
      "14 set\n",
      "15 statistical\n",
      "16 natural\n",
      "17 framework\n",
      "18 deep\n",
      "19 real\n",
      "20 speech\n",
      "21 robotics\n",
      "22 scale\n",
      "23 dataset\n",
      "24 mining\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(rel_ent_superkeyword[0:25])):\n",
    "    print(index, rel_ent_superkeyword[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 inference\n",
      "26 pattern\n",
      "27 world\n",
      "28 planning\n",
      "29 representation\n",
      "30 big\n",
      "31 neural\n",
      "32 action\n",
      "33 dimensional\n",
      "34 recognition\n",
      "35 motion\n",
      "36 robotic\n",
      "37 team\n",
      "38 autonomous\n",
      "39 text\n",
      "40 advance\n",
      "41 intelligent\n",
      "42 manipulation\n",
      "43 eg\n",
      "44 feature\n",
      "45 agent\n",
      "46 efficient\n",
      "47 apply\n",
      "48 massive\n",
      "49 train\n"
     ]
    }
   ],
   "source": [
    "for index in range(25):\n",
    "    print(index + 25, rel_ent_superkeyword[index + 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 scalable\n",
      "51 art\n",
      "52 robust\n",
      "53 represent\n",
      "54 extract\n",
      "55 linguistic\n",
      "56 class\n",
      "57 combine\n",
      "58 sparse\n",
      "59 recent\n",
      "60 estimation\n",
      "61 prediction\n",
      "62 accuracy\n",
      "63 automatic\n",
      "64 classification\n",
      "65 analyze\n",
      "66 efficiently\n",
      "67 challenge\n",
      "68 sound\n",
      "69 incorporate\n",
      "70 operator\n",
      "71 annotation\n",
      "72 input\n",
      "73 vision\n",
      "74 intelligence\n"
     ]
    }
   ],
   "source": [
    "for index in range(25):\n",
    "    print(index + 50, rel_ent_superkeyword[index + 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 cognitive\n",
      "76 clustering\n",
      "77 example\n",
      "78 generate\n",
      "79 edge\n",
      "80 exploit\n",
      "81 capability\n",
      "82 estimate\n",
      "83 word\n",
      "84 translation\n",
      "85 probabilistic\n",
      "86 general\n",
      "87 size\n",
      "88 people\n",
      "89 order\n",
      "90 automatically\n",
      "91 accurate\n",
      "92 relationship\n",
      "93 tree\n",
      "94 statistic\n",
      "95 parameter\n",
      "96 ai\n",
      "97 analytics\n",
      "98 handle\n",
      "99 neuron\n"
     ]
    }
   ],
   "source": [
    "for index in range(25):\n",
    "    print(index + 75, rel_ent_superkeyword[index + 75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 uncertainty\n",
      "101 dimension\n",
      "102 variable\n",
      "103 recognize\n",
      "104 able\n",
      "105 interact\n",
      "106 discover\n",
      "107 variety\n",
      "108 situation\n",
      "109 modern\n",
      "110 manufacturing\n",
      "111 perception\n",
      "112 observe\n",
      "113 animal\n",
      "114 audio\n",
      "115 capable\n",
      "116 sampling\n",
      "117 operate\n",
      "118 assist\n",
      "119 artificial_intelligence\n",
      "120 speaker\n",
      "121 highly\n",
      "122 demonstration\n",
      "123 typically\n",
      "124 constraint\n"
     ]
    }
   ],
   "source": [
    "for index in range(25):\n",
    "    print(index + 100, rel_ent_superkeyword[index + 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 thrust\n",
      "126 success\n",
      "127 challenging\n",
      "128 flexible\n",
      "129 adapt\n",
      "130 brain\n",
      "131 insight\n",
      "132 series\n",
      "133 rank\n",
      "134 graphical\n",
      "135 significantly\n",
      "136 single\n",
      "137 contribution\n",
      "138 space\n",
      "139 noisy\n",
      "140 unified\n",
      "141 setting\n",
      "142 exploratory\n",
      "143 auditory\n",
      "144 effectively\n",
      "145 inspire\n",
      "146 exhibit\n",
      "147 anomaly\n",
      "148 voice\n",
      "149 distribution\n"
     ]
    }
   ],
   "source": [
    "for index in range(25):\n",
    "    print(index + 125, rel_ent_superkeyword[index + 125])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the filter helper to see if we can start trying to filter the corpus to get some sort of sense the abstracts that are about AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter_helper = FilterHelper(topic_model = lda_model_100,\n",
    "                                vectorizer = lda_vectorizer,\n",
    "                               relevant_topics = relevant_topics_HT,\n",
    "                               superkeywords = rel_ent_superkeyword[0:500],\n",
    "                               keyword_list = core_terms,\n",
    "                               total_topic_prop_threshold = 0.25,\n",
    "                               keyword_prop_threshold = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.FilterHelper at 0x7f6eb1c47b20>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filter_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LatentDirichletAllocation' object has no attribute 'docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-33a4d608a75b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_corpus_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model_100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_filter_helper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d5c29bb8bf2a>\u001b[0m in \u001b[0;36mfilter_corpus\u001b[0;34m(topic_model, filter_helper)\u001b[0m\n\u001b[1;32m    179\u001b[0m         document text\"\"\"\n\u001b[1;32m    180\u001b[0m     \u001b[0msubcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mdoc_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_topic_proportions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'docs'"
     ]
    }
   ],
   "source": [
    "filtered_corpus_text = filter_corpus(lda_model_100, my_filter_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-HT_Bert]",
   "language": "python",
   "name": "conda-env-.conda-HT_Bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
